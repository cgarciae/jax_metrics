{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"JAX Metrics A Metrics library for the JAX ecosystem Main Features Standard metrics that can be used in any JAX project. Pytree abstractions that can natively integrate with all JAX APIs and pytree-supporting frameworks (flax.struct, equinox, treex, etc). Distributed-friendly APIs that make it super easy to synchronize metrics across devices. Automatic accumulation over epochs. JAX Metrics is implemented on top of Treeo . What is included? The Keras-like Loss and Metric abstractions. A metrics module containing popular metrics. The losses and regularizers modules containing popular losses. The Metrics , Losses , and LossesAndMetrics combinators. Installation Install using pip: pip install jax_metrics Status Metrics on this library are usually tested against their Keras or Torchmetrics counterparts for numerical equivalence. This code base comes from Treex and Elegy so it's already in use. Getting Started Metric The Metric API consists of 3 basic methods: reset : Used to both initialize and reset a metric. update : Takes in new data and updates the metric state. compute : Returns the current value of the metric. Simple usage looks like this: import jax_metrics as jm metric = jm . metrics . Accuracy () # Initialize the metric metric = metric . init () # Update the metric with a batch of predictions and labels metric = metric . update ( target = y , preds = logits ) # Get the current value of the metric acc = metric . compute () # 0.95 # alternatively, produce a logs dict logs = metric . compute_logs () # {'accuracy': 0.95} # Reset the metric metric = metric . reset () Note that update enforces the use of keyword arguments. Also the Metric.name property is used as the key in the returned dict, by default this is the name of the class in lowercase but can be overridden in the constructor via the name argument. Tipical Training Setup Because Metrics are pytrees they can be used with jit , pmap , etc. On a more realistic scenario you will proably want to use them inside some of your JAX functions in a setup similar to this: import jax_metrics as jm metric = jm . metrics . Accuracy () @jax . jit def init_step ( metric : jm . Metric ) -> jm . Metric : return metric . init () def loss_fn ( params , metric , x , y ): ... metric = metric . update ( target = y , preds = logits ) ... return loss , metric @jax . jit def train_step ( params , metric , x , y ): grads , metric = jax . grad ( loss_fn , has_aux = True )( params , metric , x , y ) ... return params , metric @jax . jit def reset_step ( metric : jm . Metric ) -> jm . Metric : return metric . reset () Since the loss function usually has access to the predictions and labels, its usually where you would call metric.update , and the new metric state can be returned as an auxiliary output. Distributed Training JAX Metrics has a distributed friendly API via the batch_updates and aggregate methods. A simple example of a loss function inside a data parallel setup could look like this: def loss_fn ( params , metric , x , y ): ... # compuate batch update batch_updates = metric . batch_updates ( target = y , preds = logits ) # gather over all devices and aggregate batch_updates = jax . lax . all_gather ( batch_updates , \"device\" ) . aggregate () # update metric metric = metric . merge ( batch_updates ) ... The batch_updates method behaves similar to update but returns a new metric state with only information about that batch, jax.lax.all_gather \"gathers\" the metric state over all devices plus adds a new axis to the metric state, and aggregate reduces the metric state over all devices (first axis). Finally, merge combines the accumulated metric state over the previous batches with the batch updates. Loss The Loss API just consists of a __call__ method. Simple usage looks like this: import jax_metrics as jm crossentropy = jm . losses . Crossentropy () # get reduced loss value loss = crossentropy ( target = y , preds = logits ) # 0.23 Note that losses are not pytrees so they should be marked as static. Similar to Keras, all losses have a reduction strategy that can be specified in the constructor and (usually) makes sure that the output is a scalar. Why have losses in a metrics library? There are a few reasons for having losses in a metrics library: 1. Most code from this library was originally written for and will still be consumed by Elegy. Since Elegy needs support for calculating cumulative losses, as you will see later, a Metric abstraction called `Losses` was created for this. 2. A couple of API design decisions are shared between the `Loss` and `Metric` APIs. This includes: * `__call__` and `update` both accept any number keyword only arguments. This is used to facilitate composition (see [Combinators](#combinators) section). * Both classes have the `index_into` and `map_arg` methods that allow them to modify how arguments are consumed. * Argument names are standardized to be consistent when ever possible, e.g. both `metrics.Accuracy` and `losses.Crossentropy` use the `target` and `preds` arguments. This is super convenient for the `LossesAndMetrics` combinator. Combinators Combinators enable you to group together multiple metrics while also being instances of Metric and thus maintaining the same API. Metrics The Metrics combinator lets you combine multiple metrics into a single Metric object. metrics = jm . Metrics ([ jm . metrics . Accuracy (), jm . metrics . F1 (), # not yet implemented \ud83d\ude05, coming soon? ]) # same API metrics = metrics . init () # same API metrics = metrics . update ( target = y , preds = logits ) # compute now returns a dict metrics . compute () # {'accuracy': 0.95, 'f1': 0.87} # same as compute_logs in the case metrics . compute_logs () # {'accuracy': 0.95, 'f1': 0.87} # Reset the metrics metrics = metrics . reset () As you can see the Metrics.update method accepts and forwards all the arguments required by the individual metrics. In this example they use the same arguments, but in practice they may consume different subsets of the arguments. Also, if names are repeated then unique names are generated for each metric by appending a number to the metric name. If a dictionary is used instead of a list, the keys are used instead of the name property of the metrics to determine the key in the returned dict. metrics = jm . Metrics ({ \"acc\" : jm . metrics . Accuracy (), \"f_one\" : jm . metrics . F1 (), # not yet implemented \ud83d\ude05, coming soon? }) # same API metrics = metrics . init () # same API metrics = metrics . update ( target = y , preds = logits ) # compute new returns a dict metrics . compute () # {'acc': 0.95, 'f_one': 0.87} # same as compute_logs in the case metrics . compute_logs () # {'acc': 0.95, 'f_one': 0.87} # Reset the metrics metrics = metrics . reset () You can use nested structures of dicts and lists to group metrics, the keys of the dicts are used to determine group names. Group names and metrics names are concatenated using \"/\" e.g. \"{group_name}/{metric_name}\" . Losses Losses is a Metric combinator that behaves very similarly to Metrics but contains Loss instances. Losses calculates the cumulative mean value of each loss over the batches. losses = jm . Losses ([ jm . losses . Crossentropy (), jm . regularizers . L2 ( 1e-4 ), ]) # same API losses = losses . init () # same API losses = losses . update ( target = y , preds = logits , parameters = params ) # compute new returns a dict losses . compute () # {'crossentropy': 0.23, 'l2': 0.005} # same as compute_logs in the case losses . compute_logs () # {'crossentropy': 0.23, 'l2': 0.005} # you can also compute the total loss loss = losses . total_loss () # 0.235 # Reset the losses losses = losses . reset () As with Metrics , the update method accepts and forwards all the arguments required by the individual losses. In this example target and preds are used by the Crossentropy , while parameters is used by the L2 . The total_loss method returns the sum of all values returned by compute . If a dictionary is used instead of a list, the keys are used instead of the name property of the losses to determine the key in the returned dict. losses = jm . Losses ({ \"xent\" : jm . losses . Crossentropy (), \"l_two\" : jm . regularizers . L2 ( 1e-4 ), }) # same API losses = losses . init () # same API losses = losses . update ( target = y , preds = logits , parameters = params ) # compute new returns a dict losses . compute () # {'xent': 0.23, 'l_two': 0.005} # same as compute_logs in the case losses . compute_logs () # {'xent': 0.23, 'l_two': 0.005} # you can also compute the total loss loss = losses . total_loss () # 0.235 # Reset the losses losses = losses . reset () If you want to use Losses to calculate the loss of a model, you should use batch_updates followed by total_loss to get the correct batch loss. For example, a loss function could be written as: def loss_fn ( ... , losses ): ... batch_updates = losses . batch_updates ( target = y , preds = logits , parameters = params ) loss = batch_updates . total_loss () losses = losses . merge ( batch_updates ) ... return loss , losses For convenience, the previous pattern can be simplified to a single line using the loss_and_update method: def loss_fn ( ... ): ... loss , lossses = losses . loss_and_update ( target = y , preds = logits , parameters = params ) ... return loss , losses LossesAndMetrics The LossesAndMetrics combinator is a Metric that combines the Lossses and Metrics combinators. Its main utility instead of using these independently is that it can computes a single logs dictionary while making sure that names/keys remain unique in case of collisions. losses_and_metrics = jm . LossesAndMetrics ( metrics = [ jm . metrics . Accuracy (), jm . metrics . F1 (), # not yet implemented \ud83d\ude05, coming soon? ], losses = [ jm . losses . Crossentropy (), jm . regularizers . L2 ( 1e-4 ), ], ) # same API losses_and_metrics = losses_and_metrics . init () # same API losses_and_metrics = losses_and_metrics . update ( target = y , preds = logits , parameters = params ) # compute new returns a dict losses_and_metrics . compute () # {'loss': 0.235, 'accuracy': 0.95, 'f1': 0.87, 'crossentropy': 0.23, 'l2': 0.005} # same as compute_logs in the case losses_and_metrics . compute_logs () # {'loss': 0.235, 'accuracy': 0.95, 'f1': 0.87, 'crossentropy': 0.23, 'l2': 0.005} # you can also compute the total loss loss = losses_and_metrics . total_loss () # 0.235 # Reset metrics losses_and_metrics = losses_and_metrics . reset () Thanks to consistent naming, Accuracy , F1 and Crossentropy all consume the same target and preds arguments, while L2 consumes parameters . For convenience a \"loss\" key is added to the returned logs dictionary. If you want to use LossesAndMetrics to calculate the loss of a model, you should use batch_updates followed by total_loss to get the correct batch loss. For example, a loss function could be written as: def loss_fn ( ... ): ... batch_updates = losses_and_metrics . batch_updates ( target = y , preds = logits , parameters = params ) loss = batch_updates . total_loss () losses_and_metrics = losses_and_metrics . merge ( batch_updates ) ... return loss , losses_and_metrics For convenience, the previous pattern can be simplified to a single line using the loss_and_update method: def loss_fn ( ... ): ... loss , losses_and_metrics = losses_and_metrics . loss_and_update ( target = y , preds = logits , parameters = params ) ... return loss , losses_and_metrics If the loss function is running in a distributed context (e.g. pmap ) you can calculate the device-local loss and synchronize the metric state across devices like this: def loss_fn ( ... ): ... batch_updates = losses_and_metrics . batch_updates ( target = y , preds = logits , parameters = params ) loss = batch_updates . total_loss () batch_updates = jax . lax . all_gather ( batch_updates , \"device\" ) . aggregate () losses_and_metrics = losses_and_metrics . merge ( batch_updates ) ... return loss , losses_and_metrics","title":"JAX Metrics"},{"location":"#jax-metrics","text":"A Metrics library for the JAX ecosystem","title":"JAX Metrics"},{"location":"#main-features","text":"Standard metrics that can be used in any JAX project. Pytree abstractions that can natively integrate with all JAX APIs and pytree-supporting frameworks (flax.struct, equinox, treex, etc). Distributed-friendly APIs that make it super easy to synchronize metrics across devices. Automatic accumulation over epochs. JAX Metrics is implemented on top of Treeo .","title":"Main Features"},{"location":"#what-is-included","text":"The Keras-like Loss and Metric abstractions. A metrics module containing popular metrics. The losses and regularizers modules containing popular losses. The Metrics , Losses , and LossesAndMetrics combinators.","title":"What is included?"},{"location":"#installation","text":"Install using pip: pip install jax_metrics","title":"Installation"},{"location":"#status","text":"Metrics on this library are usually tested against their Keras or Torchmetrics counterparts for numerical equivalence. This code base comes from Treex and Elegy so it's already in use.","title":"Status"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#metric","text":"The Metric API consists of 3 basic methods: reset : Used to both initialize and reset a metric. update : Takes in new data and updates the metric state. compute : Returns the current value of the metric. Simple usage looks like this: import jax_metrics as jm metric = jm . metrics . Accuracy () # Initialize the metric metric = metric . init () # Update the metric with a batch of predictions and labels metric = metric . update ( target = y , preds = logits ) # Get the current value of the metric acc = metric . compute () # 0.95 # alternatively, produce a logs dict logs = metric . compute_logs () # {'accuracy': 0.95} # Reset the metric metric = metric . reset () Note that update enforces the use of keyword arguments. Also the Metric.name property is used as the key in the returned dict, by default this is the name of the class in lowercase but can be overridden in the constructor via the name argument.","title":"Metric"},{"location":"#tipical-training-setup","text":"Because Metrics are pytrees they can be used with jit , pmap , etc. On a more realistic scenario you will proably want to use them inside some of your JAX functions in a setup similar to this: import jax_metrics as jm metric = jm . metrics . Accuracy () @jax . jit def init_step ( metric : jm . Metric ) -> jm . Metric : return metric . init () def loss_fn ( params , metric , x , y ): ... metric = metric . update ( target = y , preds = logits ) ... return loss , metric @jax . jit def train_step ( params , metric , x , y ): grads , metric = jax . grad ( loss_fn , has_aux = True )( params , metric , x , y ) ... return params , metric @jax . jit def reset_step ( metric : jm . Metric ) -> jm . Metric : return metric . reset () Since the loss function usually has access to the predictions and labels, its usually where you would call metric.update , and the new metric state can be returned as an auxiliary output.","title":"Tipical Training Setup"},{"location":"#distributed-training","text":"JAX Metrics has a distributed friendly API via the batch_updates and aggregate methods. A simple example of a loss function inside a data parallel setup could look like this: def loss_fn ( params , metric , x , y ): ... # compuate batch update batch_updates = metric . batch_updates ( target = y , preds = logits ) # gather over all devices and aggregate batch_updates = jax . lax . all_gather ( batch_updates , \"device\" ) . aggregate () # update metric metric = metric . merge ( batch_updates ) ... The batch_updates method behaves similar to update but returns a new metric state with only information about that batch, jax.lax.all_gather \"gathers\" the metric state over all devices plus adds a new axis to the metric state, and aggregate reduces the metric state over all devices (first axis). Finally, merge combines the accumulated metric state over the previous batches with the batch updates.","title":"Distributed Training"},{"location":"#loss","text":"The Loss API just consists of a __call__ method. Simple usage looks like this: import jax_metrics as jm crossentropy = jm . losses . Crossentropy () # get reduced loss value loss = crossentropy ( target = y , preds = logits ) # 0.23 Note that losses are not pytrees so they should be marked as static. Similar to Keras, all losses have a reduction strategy that can be specified in the constructor and (usually) makes sure that the output is a scalar. Why have losses in a metrics library? There are a few reasons for having losses in a metrics library: 1. Most code from this library was originally written for and will still be consumed by Elegy. Since Elegy needs support for calculating cumulative losses, as you will see later, a Metric abstraction called `Losses` was created for this. 2. A couple of API design decisions are shared between the `Loss` and `Metric` APIs. This includes: * `__call__` and `update` both accept any number keyword only arguments. This is used to facilitate composition (see [Combinators](#combinators) section). * Both classes have the `index_into` and `map_arg` methods that allow them to modify how arguments are consumed. * Argument names are standardized to be consistent when ever possible, e.g. both `metrics.Accuracy` and `losses.Crossentropy` use the `target` and `preds` arguments. This is super convenient for the `LossesAndMetrics` combinator.","title":"Loss"},{"location":"#combinators","text":"Combinators enable you to group together multiple metrics while also being instances of Metric and thus maintaining the same API.","title":"Combinators"},{"location":"#metrics","text":"The Metrics combinator lets you combine multiple metrics into a single Metric object. metrics = jm . Metrics ([ jm . metrics . Accuracy (), jm . metrics . F1 (), # not yet implemented \ud83d\ude05, coming soon? ]) # same API metrics = metrics . init () # same API metrics = metrics . update ( target = y , preds = logits ) # compute now returns a dict metrics . compute () # {'accuracy': 0.95, 'f1': 0.87} # same as compute_logs in the case metrics . compute_logs () # {'accuracy': 0.95, 'f1': 0.87} # Reset the metrics metrics = metrics . reset () As you can see the Metrics.update method accepts and forwards all the arguments required by the individual metrics. In this example they use the same arguments, but in practice they may consume different subsets of the arguments. Also, if names are repeated then unique names are generated for each metric by appending a number to the metric name. If a dictionary is used instead of a list, the keys are used instead of the name property of the metrics to determine the key in the returned dict. metrics = jm . Metrics ({ \"acc\" : jm . metrics . Accuracy (), \"f_one\" : jm . metrics . F1 (), # not yet implemented \ud83d\ude05, coming soon? }) # same API metrics = metrics . init () # same API metrics = metrics . update ( target = y , preds = logits ) # compute new returns a dict metrics . compute () # {'acc': 0.95, 'f_one': 0.87} # same as compute_logs in the case metrics . compute_logs () # {'acc': 0.95, 'f_one': 0.87} # Reset the metrics metrics = metrics . reset () You can use nested structures of dicts and lists to group metrics, the keys of the dicts are used to determine group names. Group names and metrics names are concatenated using \"/\" e.g. \"{group_name}/{metric_name}\" .","title":"Metrics"},{"location":"#losses","text":"Losses is a Metric combinator that behaves very similarly to Metrics but contains Loss instances. Losses calculates the cumulative mean value of each loss over the batches. losses = jm . Losses ([ jm . losses . Crossentropy (), jm . regularizers . L2 ( 1e-4 ), ]) # same API losses = losses . init () # same API losses = losses . update ( target = y , preds = logits , parameters = params ) # compute new returns a dict losses . compute () # {'crossentropy': 0.23, 'l2': 0.005} # same as compute_logs in the case losses . compute_logs () # {'crossentropy': 0.23, 'l2': 0.005} # you can also compute the total loss loss = losses . total_loss () # 0.235 # Reset the losses losses = losses . reset () As with Metrics , the update method accepts and forwards all the arguments required by the individual losses. In this example target and preds are used by the Crossentropy , while parameters is used by the L2 . The total_loss method returns the sum of all values returned by compute . If a dictionary is used instead of a list, the keys are used instead of the name property of the losses to determine the key in the returned dict. losses = jm . Losses ({ \"xent\" : jm . losses . Crossentropy (), \"l_two\" : jm . regularizers . L2 ( 1e-4 ), }) # same API losses = losses . init () # same API losses = losses . update ( target = y , preds = logits , parameters = params ) # compute new returns a dict losses . compute () # {'xent': 0.23, 'l_two': 0.005} # same as compute_logs in the case losses . compute_logs () # {'xent': 0.23, 'l_two': 0.005} # you can also compute the total loss loss = losses . total_loss () # 0.235 # Reset the losses losses = losses . reset () If you want to use Losses to calculate the loss of a model, you should use batch_updates followed by total_loss to get the correct batch loss. For example, a loss function could be written as: def loss_fn ( ... , losses ): ... batch_updates = losses . batch_updates ( target = y , preds = logits , parameters = params ) loss = batch_updates . total_loss () losses = losses . merge ( batch_updates ) ... return loss , losses For convenience, the previous pattern can be simplified to a single line using the loss_and_update method: def loss_fn ( ... ): ... loss , lossses = losses . loss_and_update ( target = y , preds = logits , parameters = params ) ... return loss , losses","title":"Losses"},{"location":"#lossesandmetrics","text":"The LossesAndMetrics combinator is a Metric that combines the Lossses and Metrics combinators. Its main utility instead of using these independently is that it can computes a single logs dictionary while making sure that names/keys remain unique in case of collisions. losses_and_metrics = jm . LossesAndMetrics ( metrics = [ jm . metrics . Accuracy (), jm . metrics . F1 (), # not yet implemented \ud83d\ude05, coming soon? ], losses = [ jm . losses . Crossentropy (), jm . regularizers . L2 ( 1e-4 ), ], ) # same API losses_and_metrics = losses_and_metrics . init () # same API losses_and_metrics = losses_and_metrics . update ( target = y , preds = logits , parameters = params ) # compute new returns a dict losses_and_metrics . compute () # {'loss': 0.235, 'accuracy': 0.95, 'f1': 0.87, 'crossentropy': 0.23, 'l2': 0.005} # same as compute_logs in the case losses_and_metrics . compute_logs () # {'loss': 0.235, 'accuracy': 0.95, 'f1': 0.87, 'crossentropy': 0.23, 'l2': 0.005} # you can also compute the total loss loss = losses_and_metrics . total_loss () # 0.235 # Reset metrics losses_and_metrics = losses_and_metrics . reset () Thanks to consistent naming, Accuracy , F1 and Crossentropy all consume the same target and preds arguments, while L2 consumes parameters . For convenience a \"loss\" key is added to the returned logs dictionary. If you want to use LossesAndMetrics to calculate the loss of a model, you should use batch_updates followed by total_loss to get the correct batch loss. For example, a loss function could be written as: def loss_fn ( ... ): ... batch_updates = losses_and_metrics . batch_updates ( target = y , preds = logits , parameters = params ) loss = batch_updates . total_loss () losses_and_metrics = losses_and_metrics . merge ( batch_updates ) ... return loss , losses_and_metrics For convenience, the previous pattern can be simplified to a single line using the loss_and_update method: def loss_fn ( ... ): ... loss , losses_and_metrics = losses_and_metrics . loss_and_update ( target = y , preds = logits , parameters = params ) ... return loss , losses_and_metrics If the loss function is running in a distributed context (e.g. pmap ) you can calculate the device-local loss and synchronize the metric state across devices like this: def loss_fn ( ... ): ... batch_updates = losses_and_metrics . batch_updates ( target = y , preds = logits , parameters = params ) loss = batch_updates . total_loss () batch_updates = jax . lax . all_gather ( batch_updates , \"device\" ) . aggregate () losses_and_metrics = losses_and_metrics . merge ( batch_updates ) ... return loss , losses_and_metrics","title":"LossesAndMetrics"},{"location":"api/Loss/","text":"jax_metrics.Loss Loss base class. To be implemented by subclasses: call() : Contains the logic for loss calculation. Example subclass implementation: class MeanSquaredError ( Loss ): def call ( self , target , preds ): return jnp . mean ( jnp . square ( preds - target ), axis =- 1 ) Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this. Source code in jax_metrics/losses/loss.py class Loss ( ABC ): \"\"\" Loss base class. To be implemented by subclasses: * `call()`: Contains the logic for loss calculation. Example subclass implementation: ```python class MeanSquaredError(Loss): def call(self, target, preds): return jnp.mean(jnp.square(preds - target), axis=-1) ``` Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this. \"\"\" def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ types . ScalarLike ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. name: Optional name for the instance, if not provided lower snake_case version of the name of the class is used instead. \"\"\" self . name = name if name is not None else utils . _get_name ( self ) self . weight = ( jnp . asarray ( weight , dtype = jnp . float32 ) if weight is not None else jnp . array ( 1.0 , dtype = jnp . float32 ) ) self . reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) def __call__ ( self , ** kwargs , ) -> jnp . ndarray : sample_weight : tp . Optional [ jnp . ndarray ] = kwargs . pop ( \"sample_weight\" , None ) values = self . call ( ** kwargs ) return reduce_loss ( values , sample_weight , self . weight , self . reduction ) @abstractmethod def call ( self , ** kwargs ) -> jnp . ndarray : ... def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedLoss\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python losses = jm.Losses([ jm.losses.Crossentropy().index_into(target=[\"a\"]), jm.losses.Crossentropy().index_into(target=[\"b\"]), ]).reset() losses = losses.update( target={ \"a\": y0, \"b\": y1, }, preds=logits, ) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedLoss instance \"\"\" return IndexedLoss ( self , kwargs ) def map_arg ( self , ** kwargs : str ) -> \"MapArgsLoss\" : \"\"\" Returns a loss that renames the keyword arguments expected by `__call__`. Example: ```python crossentropy_loss = jm.losses.Crossentropy().map_arg(target=\"y_true\", preds=\"y_pred\") ... loss = crossentropy_loss(y_true=y, y_pred=logits) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A MapArgsLoss instance \"\"\" return MapArgsLoss ( self , kwargs ) __init__ ( self , reduction = None , weight = None , name = None ) special Initializes Loss class. Parameters: Name Type Description Default reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Union[float, numpy.ndarray, jax._src.numpy.lax_numpy.ndarray] Optional weight contribution for the total loss. Defaults to 1 . None name Optional[str] Optional name for the instance, if not provided lower snake_case version of the name of the class is used instead. None Source code in jax_metrics/losses/loss.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ types . ScalarLike ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. name: Optional name for the instance, if not provided lower snake_case version of the name of the class is used instead. \"\"\" self . name = name if name is not None else utils . _get_name ( self ) self . weight = ( jnp . asarray ( weight , dtype = jnp . float32 ) if weight is not None else jnp . array ( 1.0 , dtype = jnp . float32 ) ) self . reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) index_into ( self , ** kwargs ) Returns a metric that \"indexes\" the specified keyword arguments expected by .update() . You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing ( __getitem__ ). Examples: losses = jm . Losses ([ jm . losses . Crossentropy () . index_into ( target = [ \"a\" ]), jm . losses . Crossentropy () . index_into ( target = [ \"b\" ]), ]) . reset () losses = losses . update ( target = { \"a\" : y0 , \"b\" : y1 , }, preds = logits , ) Here values is set to a dict of arrays, but thanks to .index_into() each loss can index into its correspoding array. Parameters: Name Type Description Default **kwargs Union[str, int, Sequence[Union[str, int]]] keyword arguments to be indexed {} Returns: Type Description IndexedLoss A IndexedLoss instance Source code in jax_metrics/losses/loss.py def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedLoss\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python losses = jm.Losses([ jm.losses.Crossentropy().index_into(target=[\"a\"]), jm.losses.Crossentropy().index_into(target=[\"b\"]), ]).reset() losses = losses.update( target={ \"a\": y0, \"b\": y1, }, preds=logits, ) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedLoss instance \"\"\" return IndexedLoss ( self , kwargs ) map_arg ( self , ** kwargs ) Returns a loss that renames the keyword arguments expected by __call__ . Examples: crossentropy_loss = jm . losses . Crossentropy () . map_arg ( target = \"y_true\" , preds = \"y_pred\" ) ... loss = crossentropy_loss ( y_true = y , y_pred = logits ) Parameters: Name Type Description Default **kwargs str keyword arguments to be renamed {} Returns: Type Description MapArgsLoss A MapArgsLoss instance Source code in jax_metrics/losses/loss.py def map_arg ( self , ** kwargs : str ) -> \"MapArgsLoss\" : \"\"\" Returns a loss that renames the keyword arguments expected by `__call__`. Example: ```python crossentropy_loss = jm.losses.Crossentropy().map_arg(target=\"y_true\", preds=\"y_pred\") ... loss = crossentropy_loss(y_true=y, y_pred=logits) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A MapArgsLoss instance \"\"\" return MapArgsLoss ( self , kwargs )","title":"Loss"},{"location":"api/Loss/#jax_metricsloss","text":"Loss base class. To be implemented by subclasses: call() : Contains the logic for loss calculation. Example subclass implementation: class MeanSquaredError ( Loss ): def call ( self , target , preds ): return jnp . mean ( jnp . square ( preds - target ), axis =- 1 ) Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this. Source code in jax_metrics/losses/loss.py class Loss ( ABC ): \"\"\" Loss base class. To be implemented by subclasses: * `call()`: Contains the logic for loss calculation. Example subclass implementation: ```python class MeanSquaredError(Loss): def call(self, target, preds): return jnp.mean(jnp.square(preds - target), axis=-1) ``` Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this. \"\"\" def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ types . ScalarLike ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. name: Optional name for the instance, if not provided lower snake_case version of the name of the class is used instead. \"\"\" self . name = name if name is not None else utils . _get_name ( self ) self . weight = ( jnp . asarray ( weight , dtype = jnp . float32 ) if weight is not None else jnp . array ( 1.0 , dtype = jnp . float32 ) ) self . reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) def __call__ ( self , ** kwargs , ) -> jnp . ndarray : sample_weight : tp . Optional [ jnp . ndarray ] = kwargs . pop ( \"sample_weight\" , None ) values = self . call ( ** kwargs ) return reduce_loss ( values , sample_weight , self . weight , self . reduction ) @abstractmethod def call ( self , ** kwargs ) -> jnp . ndarray : ... def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedLoss\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python losses = jm.Losses([ jm.losses.Crossentropy().index_into(target=[\"a\"]), jm.losses.Crossentropy().index_into(target=[\"b\"]), ]).reset() losses = losses.update( target={ \"a\": y0, \"b\": y1, }, preds=logits, ) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedLoss instance \"\"\" return IndexedLoss ( self , kwargs ) def map_arg ( self , ** kwargs : str ) -> \"MapArgsLoss\" : \"\"\" Returns a loss that renames the keyword arguments expected by `__call__`. Example: ```python crossentropy_loss = jm.losses.Crossentropy().map_arg(target=\"y_true\", preds=\"y_pred\") ... loss = crossentropy_loss(y_true=y, y_pred=logits) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A MapArgsLoss instance \"\"\" return MapArgsLoss ( self , kwargs )","title":"jax_metrics.Loss"},{"location":"api/Loss/#jax_metrics.losses.loss.Loss.__init__","text":"Initializes Loss class. Parameters: Name Type Description Default reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Union[float, numpy.ndarray, jax._src.numpy.lax_numpy.ndarray] Optional weight contribution for the total loss. Defaults to 1 . None name Optional[str] Optional name for the instance, if not provided lower snake_case version of the name of the class is used instead. None Source code in jax_metrics/losses/loss.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ types . ScalarLike ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. name: Optional name for the instance, if not provided lower snake_case version of the name of the class is used instead. \"\"\" self . name = name if name is not None else utils . _get_name ( self ) self . weight = ( jnp . asarray ( weight , dtype = jnp . float32 ) if weight is not None else jnp . array ( 1.0 , dtype = jnp . float32 ) ) self . reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE )","title":"__init__()"},{"location":"api/Loss/#jax_metrics.losses.loss.Loss.index_into","text":"Returns a metric that \"indexes\" the specified keyword arguments expected by .update() . You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing ( __getitem__ ). Examples: losses = jm . Losses ([ jm . losses . Crossentropy () . index_into ( target = [ \"a\" ]), jm . losses . Crossentropy () . index_into ( target = [ \"b\" ]), ]) . reset () losses = losses . update ( target = { \"a\" : y0 , \"b\" : y1 , }, preds = logits , ) Here values is set to a dict of arrays, but thanks to .index_into() each loss can index into its correspoding array. Parameters: Name Type Description Default **kwargs Union[str, int, Sequence[Union[str, int]]] keyword arguments to be indexed {} Returns: Type Description IndexedLoss A IndexedLoss instance Source code in jax_metrics/losses/loss.py def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedLoss\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python losses = jm.Losses([ jm.losses.Crossentropy().index_into(target=[\"a\"]), jm.losses.Crossentropy().index_into(target=[\"b\"]), ]).reset() losses = losses.update( target={ \"a\": y0, \"b\": y1, }, preds=logits, ) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedLoss instance \"\"\" return IndexedLoss ( self , kwargs )","title":"index_into()"},{"location":"api/Loss/#jax_metrics.losses.loss.Loss.map_arg","text":"Returns a loss that renames the keyword arguments expected by __call__ . Examples: crossentropy_loss = jm . losses . Crossentropy () . map_arg ( target = \"y_true\" , preds = \"y_pred\" ) ... loss = crossentropy_loss ( y_true = y , y_pred = logits ) Parameters: Name Type Description Default **kwargs str keyword arguments to be renamed {} Returns: Type Description MapArgsLoss A MapArgsLoss instance Source code in jax_metrics/losses/loss.py def map_arg ( self , ** kwargs : str ) -> \"MapArgsLoss\" : \"\"\" Returns a loss that renames the keyword arguments expected by `__call__`. Example: ```python crossentropy_loss = jm.losses.Crossentropy().map_arg(target=\"y_true\", preds=\"y_pred\") ... loss = crossentropy_loss(y_true=y, y_pred=logits) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A MapArgsLoss instance \"\"\" return MapArgsLoss ( self , kwargs )","title":"map_arg()"},{"location":"api/Losses/","text":"jax_metrics.Losses Source code in jax_metrics/metrics/losses.py class Losses ( Metric ): losses : tp . Dict [ str , Loss ] = to . static () totals : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = to . node () counts : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = to . node () def __init__ ( self , losses : tp . Any , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): super () . __init__ ( name = name , dtype = dtype ) names : tp . Set [ str ] = set () def get_name ( path , metric , parent_iterable ): name = utils . _get_name ( metric ) if path : if parent_iterable : return f \" { path } / { name } \" else : return path else : return name self . losses = { utils . _unique_name ( names , get_name ( path , loss , parent_iterable )): loss for path , loss , parent_iterable in utils . _flatten_names ( losses ) } self . totals = None self . counts = None def reset ( self : M ) -> M : totals = { name : jnp . array ( 0.0 , dtype = jnp . float32 ) for name in self . losses } counts = { name : jnp . array ( 0 , dtype = jnp . uint32 ) for name in self . losses } return self . replace ( totals = totals , counts = counts ) def update ( self : M , ** kwargs ) -> M : if self . totals is None or self . counts is None : raise ValueError ( \"Losses not initialized, call 'reset()' first\" ) totals = { name : self . totals [ name ] + loss ( ** kwargs ) for name , loss in self . losses . items () } counts = { name : self . counts [ name ] + 1 for name in self . losses . keys ()} return self . replace ( totals = totals , counts = counts ) def compute ( self ) -> tp . Dict [ str , jnp . ndarray ]: if self . totals is None or self . counts is None : raise ValueError ( \"Losses not initialized, call 'reset()' first\" ) outputs = {} names = set () for name in self . losses . keys (): value = self . totals [ name ] / self . counts [ name ] for path , value , parent_iterable in utils . _flatten_names ( value ): name = utils . _unique_name ( names , name ) if path : if parent_iterable : name = f \" { path } / { name } \" else : name = path outputs [ name ] = value return outputs def compute_logs ( self ) -> tp . Dict [ str , jnp . ndarray ]: return self . compute () def __call__ ( self : M , ** kwargs ) -> tp . Tuple [ tp . Dict [ str , jnp . ndarray ], M ]: return super () . __call__ ( ** kwargs ) def total_loss ( self ) -> jnp . ndarray : return sum ( self . compute () . values (), jnp . array ( 0.0 )) def slice ( self , ** kwargs : types . IndexLike ) -> \"Losses\" : losses = { name : loss . index_into ( ** kwargs ) for name , loss in self . losses . items ()} return self . replace ( losses = losses ) def loss_and_update ( self : M , ** kwargs ) -> tp . Tuple [ jnp . ndarray , M ]: batch_updates = self . batch_updates ( ** kwargs ) loss = batch_updates . total_loss () metrics = self . merge ( batch_updates ) return loss , metrics compute ( self ) Compute the current metric value. Source code in jax_metrics/metrics/losses.py def compute ( self ) -> tp . Dict [ str , jnp . ndarray ]: if self . totals is None or self . counts is None : raise ValueError ( \"Losses not initialized, call 'reset()' first\" ) outputs = {} names = set () for name in self . losses . keys (): value = self . totals [ name ] / self . counts [ name ] for path , value , parent_iterable in utils . _flatten_names ( value ): name = utils . _unique_name ( names , name ) if path : if parent_iterable : name = f \" { path } / { name } \" else : name = path outputs [ name ] = value return outputs compute_logs ( self ) Compute the current metric value(s) and returns it/them in a {metric_name: metric_value} dictionary. Returns: Type Description Dict[str, jax._src.numpy.lax_numpy.ndarray] A dictionary of metric values Source code in jax_metrics/metrics/losses.py def compute_logs ( self ) -> tp . Dict [ str , jnp . ndarray ]: return self . compute () reset ( self ) Resets the metric state. Returns: Type Description ~M Metric with the initial state Source code in jax_metrics/metrics/losses.py def reset ( self : M ) -> M : totals = { name : jnp . array ( 0.0 , dtype = jnp . float32 ) for name in self . losses } counts = { name : jnp . array ( 0 , dtype = jnp . uint32 ) for name in self . losses } return self . replace ( totals = totals , counts = counts ) update ( self , ** kwargs ) Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining **kwargs . Parameters: Name Type Description Default **kwargs data to update the metric with {} Returns: Type Description ~M Metric with updated state Source code in jax_metrics/metrics/losses.py def update ( self : M , ** kwargs ) -> M : if self . totals is None or self . counts is None : raise ValueError ( \"Losses not initialized, call 'reset()' first\" ) totals = { name : self . totals [ name ] + loss ( ** kwargs ) for name , loss in self . losses . items () } counts = { name : self . counts [ name ] + 1 for name in self . losses . keys ()} return self . replace ( totals = totals , counts = counts )","title":"Losses"},{"location":"api/Losses/#jax_metricslosses","text":"Source code in jax_metrics/metrics/losses.py class Losses ( Metric ): losses : tp . Dict [ str , Loss ] = to . static () totals : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = to . node () counts : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = to . node () def __init__ ( self , losses : tp . Any , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): super () . __init__ ( name = name , dtype = dtype ) names : tp . Set [ str ] = set () def get_name ( path , metric , parent_iterable ): name = utils . _get_name ( metric ) if path : if parent_iterable : return f \" { path } / { name } \" else : return path else : return name self . losses = { utils . _unique_name ( names , get_name ( path , loss , parent_iterable )): loss for path , loss , parent_iterable in utils . _flatten_names ( losses ) } self . totals = None self . counts = None def reset ( self : M ) -> M : totals = { name : jnp . array ( 0.0 , dtype = jnp . float32 ) for name in self . losses } counts = { name : jnp . array ( 0 , dtype = jnp . uint32 ) for name in self . losses } return self . replace ( totals = totals , counts = counts ) def update ( self : M , ** kwargs ) -> M : if self . totals is None or self . counts is None : raise ValueError ( \"Losses not initialized, call 'reset()' first\" ) totals = { name : self . totals [ name ] + loss ( ** kwargs ) for name , loss in self . losses . items () } counts = { name : self . counts [ name ] + 1 for name in self . losses . keys ()} return self . replace ( totals = totals , counts = counts ) def compute ( self ) -> tp . Dict [ str , jnp . ndarray ]: if self . totals is None or self . counts is None : raise ValueError ( \"Losses not initialized, call 'reset()' first\" ) outputs = {} names = set () for name in self . losses . keys (): value = self . totals [ name ] / self . counts [ name ] for path , value , parent_iterable in utils . _flatten_names ( value ): name = utils . _unique_name ( names , name ) if path : if parent_iterable : name = f \" { path } / { name } \" else : name = path outputs [ name ] = value return outputs def compute_logs ( self ) -> tp . Dict [ str , jnp . ndarray ]: return self . compute () def __call__ ( self : M , ** kwargs ) -> tp . Tuple [ tp . Dict [ str , jnp . ndarray ], M ]: return super () . __call__ ( ** kwargs ) def total_loss ( self ) -> jnp . ndarray : return sum ( self . compute () . values (), jnp . array ( 0.0 )) def slice ( self , ** kwargs : types . IndexLike ) -> \"Losses\" : losses = { name : loss . index_into ( ** kwargs ) for name , loss in self . losses . items ()} return self . replace ( losses = losses ) def loss_and_update ( self : M , ** kwargs ) -> tp . Tuple [ jnp . ndarray , M ]: batch_updates = self . batch_updates ( ** kwargs ) loss = batch_updates . total_loss () metrics = self . merge ( batch_updates ) return loss , metrics","title":"jax_metrics.Losses"},{"location":"api/Losses/#jax_metrics.metrics.losses.Losses.compute","text":"Compute the current metric value. Source code in jax_metrics/metrics/losses.py def compute ( self ) -> tp . Dict [ str , jnp . ndarray ]: if self . totals is None or self . counts is None : raise ValueError ( \"Losses not initialized, call 'reset()' first\" ) outputs = {} names = set () for name in self . losses . keys (): value = self . totals [ name ] / self . counts [ name ] for path , value , parent_iterable in utils . _flatten_names ( value ): name = utils . _unique_name ( names , name ) if path : if parent_iterable : name = f \" { path } / { name } \" else : name = path outputs [ name ] = value return outputs","title":"compute()"},{"location":"api/Losses/#jax_metrics.metrics.losses.Losses.compute_logs","text":"Compute the current metric value(s) and returns it/them in a {metric_name: metric_value} dictionary. Returns: Type Description Dict[str, jax._src.numpy.lax_numpy.ndarray] A dictionary of metric values Source code in jax_metrics/metrics/losses.py def compute_logs ( self ) -> tp . Dict [ str , jnp . ndarray ]: return self . compute ()","title":"compute_logs()"},{"location":"api/Losses/#jax_metrics.metrics.losses.Losses.reset","text":"Resets the metric state. Returns: Type Description ~M Metric with the initial state Source code in jax_metrics/metrics/losses.py def reset ( self : M ) -> M : totals = { name : jnp . array ( 0.0 , dtype = jnp . float32 ) for name in self . losses } counts = { name : jnp . array ( 0 , dtype = jnp . uint32 ) for name in self . losses } return self . replace ( totals = totals , counts = counts )","title":"reset()"},{"location":"api/Losses/#jax_metrics.metrics.losses.Losses.update","text":"Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining **kwargs . Parameters: Name Type Description Default **kwargs data to update the metric with {} Returns: Type Description ~M Metric with updated state Source code in jax_metrics/metrics/losses.py def update ( self : M , ** kwargs ) -> M : if self . totals is None or self . counts is None : raise ValueError ( \"Losses not initialized, call 'reset()' first\" ) totals = { name : self . totals [ name ] + loss ( ** kwargs ) for name , loss in self . losses . items () } counts = { name : self . counts [ name ] + 1 for name in self . losses . keys ()} return self . replace ( totals = totals , counts = counts )","title":"update()"},{"location":"api/LossesAndMetrics/","text":"jax_metrics.LossesAndMetrics Source code in jax_metrics/metrics/losses_and_metrics.py class LossesAndMetrics ( Metric ): losses : tp . Optional [ Losses ] metrics : tp . Optional [ Metrics ] aux_losses : tp . Optional [ AuxLosses ] aux_metrics : tp . Optional [ AuxMetrics ] def __init__ ( self , losses : tp . Optional [ tp . Union [ Losses , tp . Any ]] = None , metrics : tp . Optional [ tp . Union [ Metrics , tp . Any ]] = None , aux_losses : tp . Optional [ tp . Union [ AuxLosses , tp . Any ]] = None , aux_metrics : tp . Optional [ tp . Union [ AuxMetrics , tp . Any ]] = None , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): super () . __init__ ( name = name , dtype = dtype ) self . losses = ( losses if isinstance ( losses , Losses ) else Losses ( losses ) if losses is not None else None ) self . metrics = ( metrics if isinstance ( metrics , Metrics ) else Metrics ( metrics ) if metrics is not None else None ) self . aux_losses = ( aux_losses if isinstance ( aux_losses , AuxLosses ) else AuxLosses ( aux_losses ) if aux_losses is not None else None ) self . aux_metrics = ( aux_metrics if isinstance ( aux_metrics , AuxMetrics ) else AuxMetrics ( aux_metrics ) if aux_metrics is not None else None ) def init ( self : M , * , aux_losses : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , aux_metrics : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , ) -> M : if self . aux_losses is not None : if aux_losses is None : raise ValueError ( \"`aux_losses` are expected, got None.\" ) aux_losses_ = self . aux_losses . init ( aux_losses ) else : if aux_losses is not None : raise ValueError ( f \"`aux_losses` are not expected, got { aux_losses } .\" ) aux_losses_ = None if self . aux_metrics is not None : if aux_metrics is None : raise ValueError ( \"`aux_metrics` are expected, got None.\" ) aux_metrics_ = self . aux_metrics . init ( aux_metrics ) else : if aux_metrics is not None : raise ValueError ( f \"`aux_metrics` are not expected, got { aux_metrics } .\" ) aux_metrics_ = None return self . replace ( aux_losses = aux_losses_ , aux_metrics = aux_metrics_ , ) . reset () def reset ( self : M ) -> M : if self . losses is not None : losses = self . losses . reset () else : losses = None if self . metrics is not None : metrics = self . metrics . reset () else : metrics = None if self . aux_losses is not None : aux_losses_ = self . aux_losses . reset () else : aux_losses_ = None if self . aux_metrics is not None : aux_metrics_ = self . aux_metrics . reset () else : aux_metrics_ = None return self . replace ( losses = losses , metrics = metrics , aux_losses = aux_losses_ , aux_metrics = aux_metrics_ , ) def update ( self : M , aux_losses : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , aux_metrics : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , ** kwargs , ) -> M : if self . losses is not None : losses = self . losses . update ( ** kwargs ) else : losses = None if self . metrics is not None : metrics = self . metrics . update ( ** kwargs ) else : metrics = None if self . aux_losses is not None : if aux_losses is None : raise ValueError ( \"`aux_losses` are expected, got None.\" ) aux_losses_ = self . aux_losses . update ( aux_values = aux_losses ) else : aux_losses_ = None if self . aux_metrics is not None : if aux_metrics is None : raise ValueError ( \"`aux_metrics` are expected, got None.\" ) aux_metrics_ = self . aux_metrics . update ( aux_values = aux_metrics ) else : aux_metrics_ = None return self . replace ( losses = losses , metrics = metrics , aux_losses = aux_losses_ , aux_metrics = aux_metrics_ , ) def compute ( self ) -> tp . Dict [ str , jnp . ndarray ]: if self . losses is not None : losses_logs = self . losses . compute () else : losses_logs = {} if self . metrics is not None : metrics_logs = self . metrics . compute () else : metrics_logs = {} if self . aux_losses is not None : aux_losses_logs = self . aux_losses . compute () else : aux_losses_logs = {} if self . aux_metrics is not None : aux_metrics_logs = self . aux_metrics . compute () else : aux_metrics_logs = {} loss = self . total_loss () return { \"loss\" : loss , ** losses_logs , ** metrics_logs , ** aux_losses_logs , ** aux_metrics_logs , } def compute_logs ( self ) -> tp . Dict [ str , jnp . ndarray ]: return self . compute () def __call__ ( self : M , aux_losses : tp . Optional [ tp . Any ] = None , aux_metrics : tp . Optional [ tp . Any ] = None , ** kwargs , ) -> tp . Tuple [ tp . Dict [ str , jnp . ndarray ], M ]: return super () . __call__ ( aux_losses = aux_losses , aux_metrics = aux_metrics , ** kwargs , ) def total_loss ( self ) -> jnp . ndarray : loss = jnp . array ( 0.0 , dtype = jnp . float32 ) if self . losses is not None : loss += self . losses . total_loss () if self . aux_losses is not None : loss += self . aux_losses . total_loss () return loss def loss_and_update ( self : M , ** kwargs ) -> tp . Tuple [ jnp . ndarray , M ]: batch_updates = self . batch_updates ( ** kwargs ) loss = batch_updates . total_loss () metrics = self . merge ( batch_updates ) return loss , metrics compute ( self ) Compute the current metric value. Source code in jax_metrics/metrics/losses_and_metrics.py def compute ( self ) -> tp . Dict [ str , jnp . ndarray ]: if self . losses is not None : losses_logs = self . losses . compute () else : losses_logs = {} if self . metrics is not None : metrics_logs = self . metrics . compute () else : metrics_logs = {} if self . aux_losses is not None : aux_losses_logs = self . aux_losses . compute () else : aux_losses_logs = {} if self . aux_metrics is not None : aux_metrics_logs = self . aux_metrics . compute () else : aux_metrics_logs = {} loss = self . total_loss () return { \"loss\" : loss , ** losses_logs , ** metrics_logs , ** aux_losses_logs , ** aux_metrics_logs , } compute_logs ( self ) Compute the current metric value(s) and returns it/them in a {metric_name: metric_value} dictionary. Returns: Type Description Dict[str, jax._src.numpy.lax_numpy.ndarray] A dictionary of metric values Source code in jax_metrics/metrics/losses_and_metrics.py def compute_logs ( self ) -> tp . Dict [ str , jnp . ndarray ]: return self . compute () init ( self , * , aux_losses = None , aux_metrics = None ) Initialize the metric's state. Source code in jax_metrics/metrics/losses_and_metrics.py def init ( self : M , * , aux_losses : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , aux_metrics : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , ) -> M : if self . aux_losses is not None : if aux_losses is None : raise ValueError ( \"`aux_losses` are expected, got None.\" ) aux_losses_ = self . aux_losses . init ( aux_losses ) else : if aux_losses is not None : raise ValueError ( f \"`aux_losses` are not expected, got { aux_losses } .\" ) aux_losses_ = None if self . aux_metrics is not None : if aux_metrics is None : raise ValueError ( \"`aux_metrics` are expected, got None.\" ) aux_metrics_ = self . aux_metrics . init ( aux_metrics ) else : if aux_metrics is not None : raise ValueError ( f \"`aux_metrics` are not expected, got { aux_metrics } .\" ) aux_metrics_ = None return self . replace ( aux_losses = aux_losses_ , aux_metrics = aux_metrics_ , ) . reset () reset ( self ) Resets the metric state. Returns: Type Description ~M Metric with the initial state Source code in jax_metrics/metrics/losses_and_metrics.py def reset ( self : M ) -> M : if self . losses is not None : losses = self . losses . reset () else : losses = None if self . metrics is not None : metrics = self . metrics . reset () else : metrics = None if self . aux_losses is not None : aux_losses_ = self . aux_losses . reset () else : aux_losses_ = None if self . aux_metrics is not None : aux_metrics_ = self . aux_metrics . reset () else : aux_metrics_ = None return self . replace ( losses = losses , metrics = metrics , aux_losses = aux_losses_ , aux_metrics = aux_metrics_ , ) update ( self , aux_losses = None , aux_metrics = None , ** kwargs ) Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining **kwargs . Parameters: Name Type Description Default **kwargs data to update the metric with {} Returns: Type Description ~M Metric with updated state Source code in jax_metrics/metrics/losses_and_metrics.py def update ( self : M , aux_losses : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , aux_metrics : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , ** kwargs , ) -> M : if self . losses is not None : losses = self . losses . update ( ** kwargs ) else : losses = None if self . metrics is not None : metrics = self . metrics . update ( ** kwargs ) else : metrics = None if self . aux_losses is not None : if aux_losses is None : raise ValueError ( \"`aux_losses` are expected, got None.\" ) aux_losses_ = self . aux_losses . update ( aux_values = aux_losses ) else : aux_losses_ = None if self . aux_metrics is not None : if aux_metrics is None : raise ValueError ( \"`aux_metrics` are expected, got None.\" ) aux_metrics_ = self . aux_metrics . update ( aux_values = aux_metrics ) else : aux_metrics_ = None return self . replace ( losses = losses , metrics = metrics , aux_losses = aux_losses_ , aux_metrics = aux_metrics_ , )","title":"LossesAndMetrics"},{"location":"api/LossesAndMetrics/#jax_metricslossesandmetrics","text":"Source code in jax_metrics/metrics/losses_and_metrics.py class LossesAndMetrics ( Metric ): losses : tp . Optional [ Losses ] metrics : tp . Optional [ Metrics ] aux_losses : tp . Optional [ AuxLosses ] aux_metrics : tp . Optional [ AuxMetrics ] def __init__ ( self , losses : tp . Optional [ tp . Union [ Losses , tp . Any ]] = None , metrics : tp . Optional [ tp . Union [ Metrics , tp . Any ]] = None , aux_losses : tp . Optional [ tp . Union [ AuxLosses , tp . Any ]] = None , aux_metrics : tp . Optional [ tp . Union [ AuxMetrics , tp . Any ]] = None , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): super () . __init__ ( name = name , dtype = dtype ) self . losses = ( losses if isinstance ( losses , Losses ) else Losses ( losses ) if losses is not None else None ) self . metrics = ( metrics if isinstance ( metrics , Metrics ) else Metrics ( metrics ) if metrics is not None else None ) self . aux_losses = ( aux_losses if isinstance ( aux_losses , AuxLosses ) else AuxLosses ( aux_losses ) if aux_losses is not None else None ) self . aux_metrics = ( aux_metrics if isinstance ( aux_metrics , AuxMetrics ) else AuxMetrics ( aux_metrics ) if aux_metrics is not None else None ) def init ( self : M , * , aux_losses : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , aux_metrics : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , ) -> M : if self . aux_losses is not None : if aux_losses is None : raise ValueError ( \"`aux_losses` are expected, got None.\" ) aux_losses_ = self . aux_losses . init ( aux_losses ) else : if aux_losses is not None : raise ValueError ( f \"`aux_losses` are not expected, got { aux_losses } .\" ) aux_losses_ = None if self . aux_metrics is not None : if aux_metrics is None : raise ValueError ( \"`aux_metrics` are expected, got None.\" ) aux_metrics_ = self . aux_metrics . init ( aux_metrics ) else : if aux_metrics is not None : raise ValueError ( f \"`aux_metrics` are not expected, got { aux_metrics } .\" ) aux_metrics_ = None return self . replace ( aux_losses = aux_losses_ , aux_metrics = aux_metrics_ , ) . reset () def reset ( self : M ) -> M : if self . losses is not None : losses = self . losses . reset () else : losses = None if self . metrics is not None : metrics = self . metrics . reset () else : metrics = None if self . aux_losses is not None : aux_losses_ = self . aux_losses . reset () else : aux_losses_ = None if self . aux_metrics is not None : aux_metrics_ = self . aux_metrics . reset () else : aux_metrics_ = None return self . replace ( losses = losses , metrics = metrics , aux_losses = aux_losses_ , aux_metrics = aux_metrics_ , ) def update ( self : M , aux_losses : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , aux_metrics : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , ** kwargs , ) -> M : if self . losses is not None : losses = self . losses . update ( ** kwargs ) else : losses = None if self . metrics is not None : metrics = self . metrics . update ( ** kwargs ) else : metrics = None if self . aux_losses is not None : if aux_losses is None : raise ValueError ( \"`aux_losses` are expected, got None.\" ) aux_losses_ = self . aux_losses . update ( aux_values = aux_losses ) else : aux_losses_ = None if self . aux_metrics is not None : if aux_metrics is None : raise ValueError ( \"`aux_metrics` are expected, got None.\" ) aux_metrics_ = self . aux_metrics . update ( aux_values = aux_metrics ) else : aux_metrics_ = None return self . replace ( losses = losses , metrics = metrics , aux_losses = aux_losses_ , aux_metrics = aux_metrics_ , ) def compute ( self ) -> tp . Dict [ str , jnp . ndarray ]: if self . losses is not None : losses_logs = self . losses . compute () else : losses_logs = {} if self . metrics is not None : metrics_logs = self . metrics . compute () else : metrics_logs = {} if self . aux_losses is not None : aux_losses_logs = self . aux_losses . compute () else : aux_losses_logs = {} if self . aux_metrics is not None : aux_metrics_logs = self . aux_metrics . compute () else : aux_metrics_logs = {} loss = self . total_loss () return { \"loss\" : loss , ** losses_logs , ** metrics_logs , ** aux_losses_logs , ** aux_metrics_logs , } def compute_logs ( self ) -> tp . Dict [ str , jnp . ndarray ]: return self . compute () def __call__ ( self : M , aux_losses : tp . Optional [ tp . Any ] = None , aux_metrics : tp . Optional [ tp . Any ] = None , ** kwargs , ) -> tp . Tuple [ tp . Dict [ str , jnp . ndarray ], M ]: return super () . __call__ ( aux_losses = aux_losses , aux_metrics = aux_metrics , ** kwargs , ) def total_loss ( self ) -> jnp . ndarray : loss = jnp . array ( 0.0 , dtype = jnp . float32 ) if self . losses is not None : loss += self . losses . total_loss () if self . aux_losses is not None : loss += self . aux_losses . total_loss () return loss def loss_and_update ( self : M , ** kwargs ) -> tp . Tuple [ jnp . ndarray , M ]: batch_updates = self . batch_updates ( ** kwargs ) loss = batch_updates . total_loss () metrics = self . merge ( batch_updates ) return loss , metrics","title":"jax_metrics.LossesAndMetrics"},{"location":"api/LossesAndMetrics/#jax_metrics.metrics.losses_and_metrics.LossesAndMetrics.compute","text":"Compute the current metric value. Source code in jax_metrics/metrics/losses_and_metrics.py def compute ( self ) -> tp . Dict [ str , jnp . ndarray ]: if self . losses is not None : losses_logs = self . losses . compute () else : losses_logs = {} if self . metrics is not None : metrics_logs = self . metrics . compute () else : metrics_logs = {} if self . aux_losses is not None : aux_losses_logs = self . aux_losses . compute () else : aux_losses_logs = {} if self . aux_metrics is not None : aux_metrics_logs = self . aux_metrics . compute () else : aux_metrics_logs = {} loss = self . total_loss () return { \"loss\" : loss , ** losses_logs , ** metrics_logs , ** aux_losses_logs , ** aux_metrics_logs , }","title":"compute()"},{"location":"api/LossesAndMetrics/#jax_metrics.metrics.losses_and_metrics.LossesAndMetrics.compute_logs","text":"Compute the current metric value(s) and returns it/them in a {metric_name: metric_value} dictionary. Returns: Type Description Dict[str, jax._src.numpy.lax_numpy.ndarray] A dictionary of metric values Source code in jax_metrics/metrics/losses_and_metrics.py def compute_logs ( self ) -> tp . Dict [ str , jnp . ndarray ]: return self . compute ()","title":"compute_logs()"},{"location":"api/LossesAndMetrics/#jax_metrics.metrics.losses_and_metrics.LossesAndMetrics.init","text":"Initialize the metric's state. Source code in jax_metrics/metrics/losses_and_metrics.py def init ( self : M , * , aux_losses : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , aux_metrics : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , ) -> M : if self . aux_losses is not None : if aux_losses is None : raise ValueError ( \"`aux_losses` are expected, got None.\" ) aux_losses_ = self . aux_losses . init ( aux_losses ) else : if aux_losses is not None : raise ValueError ( f \"`aux_losses` are not expected, got { aux_losses } .\" ) aux_losses_ = None if self . aux_metrics is not None : if aux_metrics is None : raise ValueError ( \"`aux_metrics` are expected, got None.\" ) aux_metrics_ = self . aux_metrics . init ( aux_metrics ) else : if aux_metrics is not None : raise ValueError ( f \"`aux_metrics` are not expected, got { aux_metrics } .\" ) aux_metrics_ = None return self . replace ( aux_losses = aux_losses_ , aux_metrics = aux_metrics_ , ) . reset ()","title":"init()"},{"location":"api/LossesAndMetrics/#jax_metrics.metrics.losses_and_metrics.LossesAndMetrics.reset","text":"Resets the metric state. Returns: Type Description ~M Metric with the initial state Source code in jax_metrics/metrics/losses_and_metrics.py def reset ( self : M ) -> M : if self . losses is not None : losses = self . losses . reset () else : losses = None if self . metrics is not None : metrics = self . metrics . reset () else : metrics = None if self . aux_losses is not None : aux_losses_ = self . aux_losses . reset () else : aux_losses_ = None if self . aux_metrics is not None : aux_metrics_ = self . aux_metrics . reset () else : aux_metrics_ = None return self . replace ( losses = losses , metrics = metrics , aux_losses = aux_losses_ , aux_metrics = aux_metrics_ , )","title":"reset()"},{"location":"api/LossesAndMetrics/#jax_metrics.metrics.losses_and_metrics.LossesAndMetrics.update","text":"Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining **kwargs . Parameters: Name Type Description Default **kwargs data to update the metric with {} Returns: Type Description ~M Metric with updated state Source code in jax_metrics/metrics/losses_and_metrics.py def update ( self : M , aux_losses : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , aux_metrics : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , ** kwargs , ) -> M : if self . losses is not None : losses = self . losses . update ( ** kwargs ) else : losses = None if self . metrics is not None : metrics = self . metrics . update ( ** kwargs ) else : metrics = None if self . aux_losses is not None : if aux_losses is None : raise ValueError ( \"`aux_losses` are expected, got None.\" ) aux_losses_ = self . aux_losses . update ( aux_values = aux_losses ) else : aux_losses_ = None if self . aux_metrics is not None : if aux_metrics is None : raise ValueError ( \"`aux_metrics` are expected, got None.\" ) aux_metrics_ = self . aux_metrics . update ( aux_values = aux_metrics ) else : aux_metrics_ = None return self . replace ( losses = losses , metrics = metrics , aux_losses = aux_losses_ , aux_metrics = aux_metrics_ , )","title":"update()"},{"location":"api/Metric/","text":"jax_metrics.Metric Encapsulates metric logic and state. Metrics accumulate state between calls such that their output value reflect the metric as if calculated on the whole data given up to that point. Source code in jax_metrics/metrics/metric.py class Metric ( to . Tree , to . Copy , to . ToString , to . ToDict , to . Repr , to . Map , to . Immutable ): \"\"\" Encapsulates metric logic and state. Metrics accumulate state between calls such that their output value reflect the metric as if calculated on the whole data given up to that point. \"\"\" def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" name: name of the metric dtype: dtype of the metric \"\"\" self . name = name if name is not None else utils . _get_name ( self ) self . dtype = dtype if dtype is not None else jnp . float32 def __call__ ( self : M , ** kwargs ) -> tp . Tuple [ tp . Any , M ]: metric : M = self batch_updates = metric . batch_updates ( ** kwargs ) batch_values = batch_updates . compute () metric = metric . merge ( batch_updates ) return batch_values , metric def init ( self : M ) -> M : \"\"\" Initialize the metric's state. \"\"\" return self . reset () @abstractmethod def reset ( self : M ) -> M : \"\"\" Resets the metric state. Returns: Metric with the initial state \"\"\" ... @abstractmethod def update ( self : M , ** kwargs ) -> M : \"\"\" Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining `**kwargs`. Arguments: **kwargs: data to update the metric with Returns: Metric with updated state \"\"\" ... @abstractmethod def compute ( self ) -> tp . Any : \"\"\" Compute the current metric value. \"\"\" ... def compute_logs ( self ) -> tp . Dict [ str , jnp . ndarray ]: \"\"\" Compute the current metric value(s) and returns it/them in a `{metric_name: metric_value}` dictionary. Returns: A dictionary of metric values \"\"\" return { self . name : self . compute ()} def batch_updates ( self : M , ** kwargs ) -> M : \"\"\" Compute metric updates for a batch of data. Equivalent to `.reset().update(**kwargs)`. Arguments: kwargs: data to update the metric with Returns: Metric with updated state \"\"\" return self . reset () . update ( ** kwargs ) def aggregate ( self : M ) -> M : \"\"\" Aggregate metric state. It assumes the metric's internal state has an additional 'device' dimension on the 0th axis. Example: ```python batch_updates = metric.batch_updates(**kwargs) batch_updates = jax.lax.all_gather(batch_updates, axis_name=\"device\") batch_updates = batch_updates.aggregate() metric = metric.merge(batch_updates) ``` Returns: Metric with aggregated state \"\"\" return jax . tree_map ( lambda x : jnp . sum ( x , axis = 0 ), self ) def merge ( self : M , other : M ) -> M : \"\"\" Merge the state of two metrics of the same type. Usually used to merge a metric with its batch_updates. Example: ```python batch_updates = metric.batch_updates(**kwargs) metric = metric.merge(batch_updates) ``` \"\"\" stacked = jax . tree_map ( lambda * xs : jnp . stack ( xs ), self , other ) return stacked . aggregate () def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedMetric\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python metrics = jm.Metrics([ jm.metrics.Mean().index_into(values=[\"a\"]), jm.metrics.Mean().index_into(values=[\"b\"]), ]).reset() metrics = metrics.update(values={ \"a\": loss0, \"b\": loss1, }) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. This also works with Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedMetric instance \"\"\" return IndexedMetric ( self , kwargs ) def map_arg ( self , ** kwargs : str ) -> \"MapArgsMetric\" : \"\"\" Returns a metric that renames the keyword arguments expected by `.update()`. Example: ```python mean = jm.metrics.Mean().map_arg(values=\"loss\").reset() ... loss = loss_fn(x, y) mean = mean.update(loss=loss) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A MapArgsMetric instance \"\"\" return MapArgsMetric ( self , kwargs ) def _not_initialized_error ( self ): return ValueError ( f \"Metric ' { self . name } ' has not been initialized, call 'reset()' first\" ) __init__ ( self , name = None , dtype = None ) special name: name of the metric dtype: dtype of the metric Source code in jax_metrics/metrics/metric.py def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" name: name of the metric dtype: dtype of the metric \"\"\" self . name = name if name is not None else utils . _get_name ( self ) self . dtype = dtype if dtype is not None else jnp . float32 aggregate ( self ) Aggregate metric state. It assumes the metric's internal state has an additional 'device' dimension on the 0th axis. Examples: batch_updates = metric . batch_updates ( ** kwargs ) batch_updates = jax . lax . all_gather ( batch_updates , axis_name = \"device\" ) batch_updates = batch_updates . aggregate () metric = metric . merge ( batch_updates ) Returns: Type Description ~M Metric with aggregated state Source code in jax_metrics/metrics/metric.py def aggregate ( self : M ) -> M : \"\"\" Aggregate metric state. It assumes the metric's internal state has an additional 'device' dimension on the 0th axis. Example: ```python batch_updates = metric.batch_updates(**kwargs) batch_updates = jax.lax.all_gather(batch_updates, axis_name=\"device\") batch_updates = batch_updates.aggregate() metric = metric.merge(batch_updates) ``` Returns: Metric with aggregated state \"\"\" return jax . tree_map ( lambda x : jnp . sum ( x , axis = 0 ), self ) batch_updates ( self , ** kwargs ) Compute metric updates for a batch of data. Equivalent to .reset().update(**kwargs) . Parameters: Name Type Description Default kwargs data to update the metric with {} Returns: Type Description ~M Metric with updated state Source code in jax_metrics/metrics/metric.py def batch_updates ( self : M , ** kwargs ) -> M : \"\"\" Compute metric updates for a batch of data. Equivalent to `.reset().update(**kwargs)`. Arguments: kwargs: data to update the metric with Returns: Metric with updated state \"\"\" return self . reset () . update ( ** kwargs ) compute ( self ) Compute the current metric value. Source code in jax_metrics/metrics/metric.py @abstractmethod def compute ( self ) -> tp . Any : \"\"\" Compute the current metric value. \"\"\" ... compute_logs ( self ) Compute the current metric value(s) and returns it/them in a {metric_name: metric_value} dictionary. Returns: Type Description Dict[str, jax._src.numpy.lax_numpy.ndarray] A dictionary of metric values Source code in jax_metrics/metrics/metric.py def compute_logs ( self ) -> tp . Dict [ str , jnp . ndarray ]: \"\"\" Compute the current metric value(s) and returns it/them in a `{metric_name: metric_value}` dictionary. Returns: A dictionary of metric values \"\"\" return { self . name : self . compute ()} index_into ( self , ** kwargs ) Returns a metric that \"indexes\" the specified keyword arguments expected by .update() . You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing ( __getitem__ ). Examples: metrics = jm . Metrics ([ jm . metrics . Mean () . index_into ( values = [ \"a\" ]), jm . metrics . Mean () . index_into ( values = [ \"b\" ]), ]) . reset () metrics = metrics . update ( values = { \"a\" : loss0 , \"b\" : loss1 , }) Here values is set to a dict of arrays, but thanks to .index_into() each loss can index into its correspoding array. This also works with Parameters: Name Type Description Default **kwargs Union[str, int, Sequence[Union[str, int]]] keyword arguments to be indexed {} Returns: Type Description IndexedMetric A IndexedMetric instance Source code in jax_metrics/metrics/metric.py def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedMetric\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python metrics = jm.Metrics([ jm.metrics.Mean().index_into(values=[\"a\"]), jm.metrics.Mean().index_into(values=[\"b\"]), ]).reset() metrics = metrics.update(values={ \"a\": loss0, \"b\": loss1, }) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. This also works with Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedMetric instance \"\"\" return IndexedMetric ( self , kwargs ) init ( self ) Initialize the metric's state. Source code in jax_metrics/metrics/metric.py def init ( self : M ) -> M : \"\"\" Initialize the metric's state. \"\"\" return self . reset () map_arg ( self , ** kwargs ) Returns a metric that renames the keyword arguments expected by .update() . Examples: mean = jm . metrics . Mean () . map_arg ( values = \"loss\" ) . reset () ... loss = loss_fn ( x , y ) mean = mean . update ( loss = loss ) Parameters: Name Type Description Default **kwargs str keyword arguments to be renamed {} Returns: Type Description MapArgsMetric A MapArgsMetric instance Source code in jax_metrics/metrics/metric.py def map_arg ( self , ** kwargs : str ) -> \"MapArgsMetric\" : \"\"\" Returns a metric that renames the keyword arguments expected by `.update()`. Example: ```python mean = jm.metrics.Mean().map_arg(values=\"loss\").reset() ... loss = loss_fn(x, y) mean = mean.update(loss=loss) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A MapArgsMetric instance \"\"\" return MapArgsMetric ( self , kwargs ) merge ( self , other ) Merge the state of two metrics of the same type. Usually used to merge a metric with its batch_updates. Examples: batch_updates = metric . batch_updates ( ** kwargs ) metric = metric . merge ( batch_updates ) Source code in jax_metrics/metrics/metric.py def merge ( self : M , other : M ) -> M : \"\"\" Merge the state of two metrics of the same type. Usually used to merge a metric with its batch_updates. Example: ```python batch_updates = metric.batch_updates(**kwargs) metric = metric.merge(batch_updates) ``` \"\"\" stacked = jax . tree_map ( lambda * xs : jnp . stack ( xs ), self , other ) return stacked . aggregate () reset ( self ) Resets the metric state. Returns: Type Description ~M Metric with the initial state Source code in jax_metrics/metrics/metric.py @abstractmethod def reset ( self : M ) -> M : \"\"\" Resets the metric state. Returns: Metric with the initial state \"\"\" ... update ( self , ** kwargs ) Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining **kwargs . Parameters: Name Type Description Default **kwargs data to update the metric with {} Returns: Type Description ~M Metric with updated state Source code in jax_metrics/metrics/metric.py @abstractmethod def update ( self : M , ** kwargs ) -> M : \"\"\" Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining `**kwargs`. Arguments: **kwargs: data to update the metric with Returns: Metric with updated state \"\"\" ...","title":"Metric"},{"location":"api/Metric/#jax_metricsmetric","text":"Encapsulates metric logic and state. Metrics accumulate state between calls such that their output value reflect the metric as if calculated on the whole data given up to that point. Source code in jax_metrics/metrics/metric.py class Metric ( to . Tree , to . Copy , to . ToString , to . ToDict , to . Repr , to . Map , to . Immutable ): \"\"\" Encapsulates metric logic and state. Metrics accumulate state between calls such that their output value reflect the metric as if calculated on the whole data given up to that point. \"\"\" def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" name: name of the metric dtype: dtype of the metric \"\"\" self . name = name if name is not None else utils . _get_name ( self ) self . dtype = dtype if dtype is not None else jnp . float32 def __call__ ( self : M , ** kwargs ) -> tp . Tuple [ tp . Any , M ]: metric : M = self batch_updates = metric . batch_updates ( ** kwargs ) batch_values = batch_updates . compute () metric = metric . merge ( batch_updates ) return batch_values , metric def init ( self : M ) -> M : \"\"\" Initialize the metric's state. \"\"\" return self . reset () @abstractmethod def reset ( self : M ) -> M : \"\"\" Resets the metric state. Returns: Metric with the initial state \"\"\" ... @abstractmethod def update ( self : M , ** kwargs ) -> M : \"\"\" Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining `**kwargs`. Arguments: **kwargs: data to update the metric with Returns: Metric with updated state \"\"\" ... @abstractmethod def compute ( self ) -> tp . Any : \"\"\" Compute the current metric value. \"\"\" ... def compute_logs ( self ) -> tp . Dict [ str , jnp . ndarray ]: \"\"\" Compute the current metric value(s) and returns it/them in a `{metric_name: metric_value}` dictionary. Returns: A dictionary of metric values \"\"\" return { self . name : self . compute ()} def batch_updates ( self : M , ** kwargs ) -> M : \"\"\" Compute metric updates for a batch of data. Equivalent to `.reset().update(**kwargs)`. Arguments: kwargs: data to update the metric with Returns: Metric with updated state \"\"\" return self . reset () . update ( ** kwargs ) def aggregate ( self : M ) -> M : \"\"\" Aggregate metric state. It assumes the metric's internal state has an additional 'device' dimension on the 0th axis. Example: ```python batch_updates = metric.batch_updates(**kwargs) batch_updates = jax.lax.all_gather(batch_updates, axis_name=\"device\") batch_updates = batch_updates.aggregate() metric = metric.merge(batch_updates) ``` Returns: Metric with aggregated state \"\"\" return jax . tree_map ( lambda x : jnp . sum ( x , axis = 0 ), self ) def merge ( self : M , other : M ) -> M : \"\"\" Merge the state of two metrics of the same type. Usually used to merge a metric with its batch_updates. Example: ```python batch_updates = metric.batch_updates(**kwargs) metric = metric.merge(batch_updates) ``` \"\"\" stacked = jax . tree_map ( lambda * xs : jnp . stack ( xs ), self , other ) return stacked . aggregate () def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedMetric\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python metrics = jm.Metrics([ jm.metrics.Mean().index_into(values=[\"a\"]), jm.metrics.Mean().index_into(values=[\"b\"]), ]).reset() metrics = metrics.update(values={ \"a\": loss0, \"b\": loss1, }) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. This also works with Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedMetric instance \"\"\" return IndexedMetric ( self , kwargs ) def map_arg ( self , ** kwargs : str ) -> \"MapArgsMetric\" : \"\"\" Returns a metric that renames the keyword arguments expected by `.update()`. Example: ```python mean = jm.metrics.Mean().map_arg(values=\"loss\").reset() ... loss = loss_fn(x, y) mean = mean.update(loss=loss) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A MapArgsMetric instance \"\"\" return MapArgsMetric ( self , kwargs ) def _not_initialized_error ( self ): return ValueError ( f \"Metric ' { self . name } ' has not been initialized, call 'reset()' first\" )","title":"jax_metrics.Metric"},{"location":"api/Metric/#jax_metrics.metrics.metric.Metric.__init__","text":"name: name of the metric dtype: dtype of the metric Source code in jax_metrics/metrics/metric.py def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" name: name of the metric dtype: dtype of the metric \"\"\" self . name = name if name is not None else utils . _get_name ( self ) self . dtype = dtype if dtype is not None else jnp . float32","title":"__init__()"},{"location":"api/Metric/#jax_metrics.metrics.metric.Metric.aggregate","text":"Aggregate metric state. It assumes the metric's internal state has an additional 'device' dimension on the 0th axis. Examples: batch_updates = metric . batch_updates ( ** kwargs ) batch_updates = jax . lax . all_gather ( batch_updates , axis_name = \"device\" ) batch_updates = batch_updates . aggregate () metric = metric . merge ( batch_updates ) Returns: Type Description ~M Metric with aggregated state Source code in jax_metrics/metrics/metric.py def aggregate ( self : M ) -> M : \"\"\" Aggregate metric state. It assumes the metric's internal state has an additional 'device' dimension on the 0th axis. Example: ```python batch_updates = metric.batch_updates(**kwargs) batch_updates = jax.lax.all_gather(batch_updates, axis_name=\"device\") batch_updates = batch_updates.aggregate() metric = metric.merge(batch_updates) ``` Returns: Metric with aggregated state \"\"\" return jax . tree_map ( lambda x : jnp . sum ( x , axis = 0 ), self )","title":"aggregate()"},{"location":"api/Metric/#jax_metrics.metrics.metric.Metric.batch_updates","text":"Compute metric updates for a batch of data. Equivalent to .reset().update(**kwargs) . Parameters: Name Type Description Default kwargs data to update the metric with {} Returns: Type Description ~M Metric with updated state Source code in jax_metrics/metrics/metric.py def batch_updates ( self : M , ** kwargs ) -> M : \"\"\" Compute metric updates for a batch of data. Equivalent to `.reset().update(**kwargs)`. Arguments: kwargs: data to update the metric with Returns: Metric with updated state \"\"\" return self . reset () . update ( ** kwargs )","title":"batch_updates()"},{"location":"api/Metric/#jax_metrics.metrics.metric.Metric.compute","text":"Compute the current metric value. Source code in jax_metrics/metrics/metric.py @abstractmethod def compute ( self ) -> tp . Any : \"\"\" Compute the current metric value. \"\"\" ...","title":"compute()"},{"location":"api/Metric/#jax_metrics.metrics.metric.Metric.compute_logs","text":"Compute the current metric value(s) and returns it/them in a {metric_name: metric_value} dictionary. Returns: Type Description Dict[str, jax._src.numpy.lax_numpy.ndarray] A dictionary of metric values Source code in jax_metrics/metrics/metric.py def compute_logs ( self ) -> tp . Dict [ str , jnp . ndarray ]: \"\"\" Compute the current metric value(s) and returns it/them in a `{metric_name: metric_value}` dictionary. Returns: A dictionary of metric values \"\"\" return { self . name : self . compute ()}","title":"compute_logs()"},{"location":"api/Metric/#jax_metrics.metrics.metric.Metric.index_into","text":"Returns a metric that \"indexes\" the specified keyword arguments expected by .update() . You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing ( __getitem__ ). Examples: metrics = jm . Metrics ([ jm . metrics . Mean () . index_into ( values = [ \"a\" ]), jm . metrics . Mean () . index_into ( values = [ \"b\" ]), ]) . reset () metrics = metrics . update ( values = { \"a\" : loss0 , \"b\" : loss1 , }) Here values is set to a dict of arrays, but thanks to .index_into() each loss can index into its correspoding array. This also works with Parameters: Name Type Description Default **kwargs Union[str, int, Sequence[Union[str, int]]] keyword arguments to be indexed {} Returns: Type Description IndexedMetric A IndexedMetric instance Source code in jax_metrics/metrics/metric.py def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedMetric\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python metrics = jm.Metrics([ jm.metrics.Mean().index_into(values=[\"a\"]), jm.metrics.Mean().index_into(values=[\"b\"]), ]).reset() metrics = metrics.update(values={ \"a\": loss0, \"b\": loss1, }) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. This also works with Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedMetric instance \"\"\" return IndexedMetric ( self , kwargs )","title":"index_into()"},{"location":"api/Metric/#jax_metrics.metrics.metric.Metric.init","text":"Initialize the metric's state. Source code in jax_metrics/metrics/metric.py def init ( self : M ) -> M : \"\"\" Initialize the metric's state. \"\"\" return self . reset ()","title":"init()"},{"location":"api/Metric/#jax_metrics.metrics.metric.Metric.map_arg","text":"Returns a metric that renames the keyword arguments expected by .update() . Examples: mean = jm . metrics . Mean () . map_arg ( values = \"loss\" ) . reset () ... loss = loss_fn ( x , y ) mean = mean . update ( loss = loss ) Parameters: Name Type Description Default **kwargs str keyword arguments to be renamed {} Returns: Type Description MapArgsMetric A MapArgsMetric instance Source code in jax_metrics/metrics/metric.py def map_arg ( self , ** kwargs : str ) -> \"MapArgsMetric\" : \"\"\" Returns a metric that renames the keyword arguments expected by `.update()`. Example: ```python mean = jm.metrics.Mean().map_arg(values=\"loss\").reset() ... loss = loss_fn(x, y) mean = mean.update(loss=loss) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A MapArgsMetric instance \"\"\" return MapArgsMetric ( self , kwargs )","title":"map_arg()"},{"location":"api/Metric/#jax_metrics.metrics.metric.Metric.merge","text":"Merge the state of two metrics of the same type. Usually used to merge a metric with its batch_updates. Examples: batch_updates = metric . batch_updates ( ** kwargs ) metric = metric . merge ( batch_updates ) Source code in jax_metrics/metrics/metric.py def merge ( self : M , other : M ) -> M : \"\"\" Merge the state of two metrics of the same type. Usually used to merge a metric with its batch_updates. Example: ```python batch_updates = metric.batch_updates(**kwargs) metric = metric.merge(batch_updates) ``` \"\"\" stacked = jax . tree_map ( lambda * xs : jnp . stack ( xs ), self , other ) return stacked . aggregate ()","title":"merge()"},{"location":"api/Metric/#jax_metrics.metrics.metric.Metric.reset","text":"Resets the metric state. Returns: Type Description ~M Metric with the initial state Source code in jax_metrics/metrics/metric.py @abstractmethod def reset ( self : M ) -> M : \"\"\" Resets the metric state. Returns: Metric with the initial state \"\"\" ...","title":"reset()"},{"location":"api/Metric/#jax_metrics.metrics.metric.Metric.update","text":"Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining **kwargs . Parameters: Name Type Description Default **kwargs data to update the metric with {} Returns: Type Description ~M Metric with updated state Source code in jax_metrics/metrics/metric.py @abstractmethod def update ( self : M , ** kwargs ) -> M : \"\"\" Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining `**kwargs`. Arguments: **kwargs: data to update the metric with Returns: Metric with updated state \"\"\" ...","title":"update()"},{"location":"api/Metrics/","text":"jax_metrics.Metrics Source code in jax_metrics/metrics/metrics.py class Metrics ( Metric ): metrics : tp . Dict [ str , Metric ] def __init__ ( self , metrics : tp . Any , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): super () . __init__ ( name = name , dtype = dtype ) names : tp . Set [ str ] = set () def get_name ( path , metric , parent_iterable ): name = utils . _get_name ( metric ) if path : if parent_iterable : return f \" { path } / { name } \" else : return path else : return name self . metrics = { utils . _unique_name ( names , get_name ( path , metric , parent_iterable )): metric for path , metric , parent_iterable in utils . _flatten_names ( metrics ) } def reset ( self : M ) -> M : metrics = { name : metric . reset () for name , metric in self . metrics . items ()} return self . replace ( metrics = metrics ) def update ( self : M , ** kwargs ) -> M : \"\"\" Update all metrics with the given values. Each metric will receive the same keyword arguments but each can internally select the values to use. If a required value is not provided, the metric will raise a TypeError. Arguments: **kwargs: Keyword arguments to pass to each metric. Returns: Metrics instance with updated state. \"\"\" metrics = { name : metric . update ( ** kwargs ) for name , metric in self . metrics . items () } return self . replace ( metrics = metrics ) def compute ( self ) -> tp . Dict [ str , jnp . ndarray ]: outputs = {} names = set () for name , metric in self . metrics . items (): value = metric . compute () for path , value , parent_iterable in utils . _flatten_names ( value ): name = utils . _unique_name ( names , name ) if path : if parent_iterable : name = f \" { path } / { name } \" else : name = path outputs [ name ] = value return outputs def __call__ ( self : M , ** kwargs ) -> tp . Tuple [ tp . Dict [ str , jnp . ndarray ], M ]: return super () . __call__ ( ** kwargs ) def slice ( self , ** kwargs : types . IndexLike ) -> \"Metrics\" : metrics = { name : metric . index_into ( ** kwargs ) for name , metric in self . metrics . items () } return self . replace ( metrics = metrics ) compute ( self ) Compute the current metric value. Source code in jax_metrics/metrics/metrics.py def compute ( self ) -> tp . Dict [ str , jnp . ndarray ]: outputs = {} names = set () for name , metric in self . metrics . items (): value = metric . compute () for path , value , parent_iterable in utils . _flatten_names ( value ): name = utils . _unique_name ( names , name ) if path : if parent_iterable : name = f \" { path } / { name } \" else : name = path outputs [ name ] = value return outputs reset ( self ) Resets the metric state. Returns: Type Description ~M Metric with the initial state Source code in jax_metrics/metrics/metrics.py def reset ( self : M ) -> M : metrics = { name : metric . reset () for name , metric in self . metrics . items ()} return self . replace ( metrics = metrics ) update ( self , ** kwargs ) Update all metrics with the given values. Each metric will receive the same keyword arguments but each can internally select the values to use. If a required value is not provided, the metric will raise a TypeError. Parameters: Name Type Description Default **kwargs Keyword arguments to pass to each metric. {} Returns: Type Description ~M Metrics instance with updated state. Source code in jax_metrics/metrics/metrics.py def update ( self : M , ** kwargs ) -> M : \"\"\" Update all metrics with the given values. Each metric will receive the same keyword arguments but each can internally select the values to use. If a required value is not provided, the metric will raise a TypeError. Arguments: **kwargs: Keyword arguments to pass to each metric. Returns: Metrics instance with updated state. \"\"\" metrics = { name : metric . update ( ** kwargs ) for name , metric in self . metrics . items () } return self . replace ( metrics = metrics )","title":"Metrics"},{"location":"api/Metrics/#jax_metricsmetrics","text":"Source code in jax_metrics/metrics/metrics.py class Metrics ( Metric ): metrics : tp . Dict [ str , Metric ] def __init__ ( self , metrics : tp . Any , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): super () . __init__ ( name = name , dtype = dtype ) names : tp . Set [ str ] = set () def get_name ( path , metric , parent_iterable ): name = utils . _get_name ( metric ) if path : if parent_iterable : return f \" { path } / { name } \" else : return path else : return name self . metrics = { utils . _unique_name ( names , get_name ( path , metric , parent_iterable )): metric for path , metric , parent_iterable in utils . _flatten_names ( metrics ) } def reset ( self : M ) -> M : metrics = { name : metric . reset () for name , metric in self . metrics . items ()} return self . replace ( metrics = metrics ) def update ( self : M , ** kwargs ) -> M : \"\"\" Update all metrics with the given values. Each metric will receive the same keyword arguments but each can internally select the values to use. If a required value is not provided, the metric will raise a TypeError. Arguments: **kwargs: Keyword arguments to pass to each metric. Returns: Metrics instance with updated state. \"\"\" metrics = { name : metric . update ( ** kwargs ) for name , metric in self . metrics . items () } return self . replace ( metrics = metrics ) def compute ( self ) -> tp . Dict [ str , jnp . ndarray ]: outputs = {} names = set () for name , metric in self . metrics . items (): value = metric . compute () for path , value , parent_iterable in utils . _flatten_names ( value ): name = utils . _unique_name ( names , name ) if path : if parent_iterable : name = f \" { path } / { name } \" else : name = path outputs [ name ] = value return outputs def __call__ ( self : M , ** kwargs ) -> tp . Tuple [ tp . Dict [ str , jnp . ndarray ], M ]: return super () . __call__ ( ** kwargs ) def slice ( self , ** kwargs : types . IndexLike ) -> \"Metrics\" : metrics = { name : metric . index_into ( ** kwargs ) for name , metric in self . metrics . items () } return self . replace ( metrics = metrics )","title":"jax_metrics.Metrics"},{"location":"api/Metrics/#jax_metrics.metrics.metrics.Metrics.compute","text":"Compute the current metric value. Source code in jax_metrics/metrics/metrics.py def compute ( self ) -> tp . Dict [ str , jnp . ndarray ]: outputs = {} names = set () for name , metric in self . metrics . items (): value = metric . compute () for path , value , parent_iterable in utils . _flatten_names ( value ): name = utils . _unique_name ( names , name ) if path : if parent_iterable : name = f \" { path } / { name } \" else : name = path outputs [ name ] = value return outputs","title":"compute()"},{"location":"api/Metrics/#jax_metrics.metrics.metrics.Metrics.reset","text":"Resets the metric state. Returns: Type Description ~M Metric with the initial state Source code in jax_metrics/metrics/metrics.py def reset ( self : M ) -> M : metrics = { name : metric . reset () for name , metric in self . metrics . items ()} return self . replace ( metrics = metrics )","title":"reset()"},{"location":"api/Metrics/#jax_metrics.metrics.metrics.Metrics.update","text":"Update all metrics with the given values. Each metric will receive the same keyword arguments but each can internally select the values to use. If a required value is not provided, the metric will raise a TypeError. Parameters: Name Type Description Default **kwargs Keyword arguments to pass to each metric. {} Returns: Type Description ~M Metrics instance with updated state. Source code in jax_metrics/metrics/metrics.py def update ( self : M , ** kwargs ) -> M : \"\"\" Update all metrics with the given values. Each metric will receive the same keyword arguments but each can internally select the values to use. If a required value is not provided, the metric will raise a TypeError. Arguments: **kwargs: Keyword arguments to pass to each metric. Returns: Metrics instance with updated state. \"\"\" metrics = { name : metric . update ( ** kwargs ) for name , metric in self . metrics . items () } return self . replace ( metrics = metrics )","title":"update()"},{"location":"api/Named/","text":"jax_metrics.Named Named( args, *kwds) Source code in jax_metrics/types.py class Named ( tp . Generic [ A ]): name : str value : A def tree_flatten ( self ): return ( self . value ,), self . name @classmethod def tree_unflatten ( cls , name , children ): return cls ( name , children [ 0 ])","title":"Named"},{"location":"api/Named/#jax_metricsnamed","text":"Named( args, *kwds) Source code in jax_metrics/types.py class Named ( tp . Generic [ A ]): name : str value : A def tree_flatten ( self ): return ( self . value ,), self . name @classmethod def tree_unflatten ( cls , name , children ): return cls ( name , children [ 0 ])","title":"jax_metrics.Named"},{"location":"api/losses/CosineSimilarity/","text":"jax_metrics.losses.CosineSimilarity Computes the mean squared logarithmic errors between target and predictions. loss = -sum(l2_norm(target) * l2_norm(preds)) Usage: target = jnp . array ([[ 0. , 1. ], [ 1. , 1. ]]) preds = jnp . array ([[ 1. , 0. ], [ 1. , 1. ]]) # Using 'auto'/'sum_over_batch_size' reduction type. cosine_loss = jm . losses . CosineSimilarity ( axis = 1 ) assert cosine_loss ( target , preds ) == - 0.49999997 # Calling with 'sample_weight'. assert cosine_loss ( target , preds , sample_weight = jnp . array ([ 0.8 , 0.2 ])) == - 0.099999994 # Using 'sum' reduction type. cosine_loss = jm . losses . CosineSimilarity ( axis = 1 , reduction = jm . losses . Reduction . SUM ) assert cosine_loss ( target , preds ) == - 0.99999994 # Using 'none' reduction type. cosine_loss = jm . losses . CosineSimilarity ( axis = 1 , reduction = jm . losses . Reduction . NONE ) assert jnp . equal ( cosine_loss ( target , preds ), jnp . array ([ - 0. , - 0.99999994 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . CosineSimilarity ( axis = 1 ), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/cosine_similarity.py class CosineSimilarity ( Loss ): \"\"\" Computes the mean squared logarithmic errors between target and predictions. `loss = -sum(l2_norm(target) * l2_norm(preds))` Usage: ```python target = jnp.array([[0., 1.], [1., 1.]]) preds = jnp.array([[1., 0.], [1., 1.]]) # Using 'auto'/'sum_over_batch_size' reduction type. cosine_loss = jm.losses.CosineSimilarity(axis=1) assert cosine_loss(target, preds) == -0.49999997 # Calling with 'sample_weight'. assert cosine_loss(target, preds, sample_weight=jnp.array([0.8, 0.2])) == -0.099999994 # Using 'sum' reduction type. cosine_loss = jm.losses.CosineSimilarity(axis=1, reduction=jm.losses.Reduction.SUM ) assert cosine_loss(target, preds) == -0.99999994 # Using 'none' reduction type. cosine_loss = jm.losses.CosineSimilarity(axis=1, reduction=jm.losses.Reduction.NONE ) assert jnp.equal(cosine_loss(target, preds), jnp.array([-0., -0.99999994])).all() ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.CosineSimilarity(axis=1), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , axis : int = - 1 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: axis: (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . axis = axis return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `CosineSimilarity` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return cosine_similarity ( target , preds , self . axis ) __init__ ( self , axis =- 1 , reduction = None , weight = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default axis int (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. -1 reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/cosine_similarity.py def __init__ ( self , axis : int = - 1 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: axis: (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . axis = axis return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) call ( self , target , preds , sample_weight = None , ** _ ) Invokes the CosineSimilarity instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/cosine_similarity.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `CosineSimilarity` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return cosine_similarity ( target , preds , self . axis )","title":"CosineSimilarity"},{"location":"api/losses/CosineSimilarity/#jax_metricslossescosinesimilarity","text":"Computes the mean squared logarithmic errors between target and predictions. loss = -sum(l2_norm(target) * l2_norm(preds)) Usage: target = jnp . array ([[ 0. , 1. ], [ 1. , 1. ]]) preds = jnp . array ([[ 1. , 0. ], [ 1. , 1. ]]) # Using 'auto'/'sum_over_batch_size' reduction type. cosine_loss = jm . losses . CosineSimilarity ( axis = 1 ) assert cosine_loss ( target , preds ) == - 0.49999997 # Calling with 'sample_weight'. assert cosine_loss ( target , preds , sample_weight = jnp . array ([ 0.8 , 0.2 ])) == - 0.099999994 # Using 'sum' reduction type. cosine_loss = jm . losses . CosineSimilarity ( axis = 1 , reduction = jm . losses . Reduction . SUM ) assert cosine_loss ( target , preds ) == - 0.99999994 # Using 'none' reduction type. cosine_loss = jm . losses . CosineSimilarity ( axis = 1 , reduction = jm . losses . Reduction . NONE ) assert jnp . equal ( cosine_loss ( target , preds ), jnp . array ([ - 0. , - 0.99999994 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . CosineSimilarity ( axis = 1 ), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/cosine_similarity.py class CosineSimilarity ( Loss ): \"\"\" Computes the mean squared logarithmic errors between target and predictions. `loss = -sum(l2_norm(target) * l2_norm(preds))` Usage: ```python target = jnp.array([[0., 1.], [1., 1.]]) preds = jnp.array([[1., 0.], [1., 1.]]) # Using 'auto'/'sum_over_batch_size' reduction type. cosine_loss = jm.losses.CosineSimilarity(axis=1) assert cosine_loss(target, preds) == -0.49999997 # Calling with 'sample_weight'. assert cosine_loss(target, preds, sample_weight=jnp.array([0.8, 0.2])) == -0.099999994 # Using 'sum' reduction type. cosine_loss = jm.losses.CosineSimilarity(axis=1, reduction=jm.losses.Reduction.SUM ) assert cosine_loss(target, preds) == -0.99999994 # Using 'none' reduction type. cosine_loss = jm.losses.CosineSimilarity(axis=1, reduction=jm.losses.Reduction.NONE ) assert jnp.equal(cosine_loss(target, preds), jnp.array([-0., -0.99999994])).all() ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.CosineSimilarity(axis=1), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , axis : int = - 1 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: axis: (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . axis = axis return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `CosineSimilarity` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return cosine_similarity ( target , preds , self . axis )","title":"jax_metrics.losses.CosineSimilarity"},{"location":"api/losses/CosineSimilarity/#jax_metrics.losses.cosine_similarity.CosineSimilarity.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default axis int (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. -1 reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/cosine_similarity.py def __init__ ( self , axis : int = - 1 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: axis: (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . axis = axis return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs )","title":"__init__()"},{"location":"api/losses/CosineSimilarity/#jax_metrics.losses.cosine_similarity.CosineSimilarity.call","text":"Invokes the CosineSimilarity instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/cosine_similarity.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `CosineSimilarity` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return cosine_similarity ( target , preds , self . axis )","title":"call()"},{"location":"api/losses/Crossentropy/","text":"jax_metrics.losses.Crossentropy Computes the crossentropy loss between the target and predictions. Use this crossentropy loss function when there are two or more label classes. We expect target to be provided as integers. If you want to provide target using one-hot representation, please use CategoricalCrossentropy loss. There should be # classes floating point values per feature for preds and a single floating point value per feature for target . In the snippet below, there is a single floating point value per example for target and # classes floating pointing values per example for preds . The shape of target is [batch_size] and the shape of preds is [batch_size, num_classes] . Usage: target = jnp . array ([ 1 , 2 ]) preds = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = jm . losses . Crossentropy () result = scce ( target , preds ) # 1.177 assert np . isclose ( result , 1.177 , rtol = 0.01 ) # Calling with 'sample_weight'. result = scce ( target , preds , sample_weight = jnp . array ([ 0.3 , 0.7 ])) # 0.814 assert np . isclose ( result , 0.814 , rtol = 0.01 ) # Using 'sum' reduction type. scce = jm . losses . Crossentropy ( reduction = jm . losses . Reduction . SUM ) result = scce ( target , preds ) # 2.354 assert np . isclose ( result , 2.354 , rtol = 0.01 ) # Using 'none' reduction type. scce = jm . losses . Crossentropy ( reduction = jm . losses . Reduction . NONE ) result = scce ( target , preds ) # [0.0513, 2.303] assert jnp . all ( np . isclose ( result , [ 0.0513 , 2.303 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . Crossentropy (), metrics = elegy . metrics . Accuracy (), optimizer = optax . adam ( 1e-3 ), ) Source code in jax_metrics/losses/crossentropy.py class Crossentropy ( Loss ): \"\"\" Computes the crossentropy loss between the target and predictions. Use this crossentropy loss function when there are two or more label classes. We expect target to be provided as integers. If you want to provide target using `one-hot` representation, please use `CategoricalCrossentropy` loss. There should be `# classes` floating point values per feature for `preds` and a single floating point value per feature for `target`. In the snippet below, there is a single floating point value per example for `target` and `# classes` floating pointing values per example for `preds`. The shape of `target` is `[batch_size]` and the shape of `preds` is `[batch_size, num_classes]`. Usage: ```python target = jnp.array([1, 2]) preds = jnp.array([[0.05, 0.95, 0], [0.1, 0.8, 0.1]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = jm.losses.Crossentropy() result = scce(target, preds) # 1.177 assert np.isclose(result, 1.177, rtol=0.01) # Calling with 'sample_weight'. result = scce(target, preds, sample_weight=jnp.array([0.3, 0.7])) # 0.814 assert np.isclose(result, 0.814, rtol=0.01) # Using 'sum' reduction type. scce = jm.losses.Crossentropy( reduction=jm.losses.Reduction.SUM ) result = scce(target, preds) # 2.354 assert np.isclose(result, 2.354, rtol=0.01) # Using 'none' reduction type. scce = jm.losses.Crossentropy( reduction=jm.losses.Reduction.NONE ) result = scce(target, preds) # [0.0513, 2.303] assert jnp.all(np.isclose(result, [0.0513, 2.303], rtol=0.01)) ``` Usage with the `Elegy` API: ```python model = elegy.Model( module_fn, loss=jm.losses.Crossentropy(), metrics=elegy.metrics.Accuracy(), optimizer=optax.adam(1e-3), ) ``` \"\"\" def __init__ ( self , * , from_logits : bool = True , binary : bool = False , label_smoothing : tp . Optional [ float ] = None , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `preds` is expected to be a logits tensor. By default, we assume that `preds` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" super () . __init__ ( reduction = reduction , weight = weight , name = name ) self . _from_logits = from_logits self . _binary = binary self . _label_smoothing = label_smoothing def call ( self , target , preds , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: target: Ground truth values. preds: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return crossentropy ( target , preds , binary = self . _binary , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , ) __init__ ( self , * , from_logits = True , binary = False , label_smoothing = None , reduction = None , weight = None , name = None ) special Initializes SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether preds is expected to be a logits tensor. By default, we assume that preds encodes a probability distribution. Note - Using from_logits=True is more numerically stable. True reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/crossentropy.py def __init__ ( self , * , from_logits : bool = True , binary : bool = False , label_smoothing : tp . Optional [ float ] = None , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `preds` is expected to be a logits tensor. By default, we assume that `preds` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" super () . __init__ ( reduction = reduction , weight = weight , name = name ) self . _from_logits = from_logits self . _binary = binary self . _label_smoothing = label_smoothing call ( self , target , preds , sample_weight = None , ** _ ) Invokes the SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default target Ground truth values. required preds The predicted values. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in jax_metrics/losses/crossentropy.py def call ( self , target , preds , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: target: Ground truth values. preds: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return crossentropy ( target , preds , binary = self . _binary , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , )","title":"Crossentropy"},{"location":"api/losses/Crossentropy/#jax_metricslossescrossentropy","text":"Computes the crossentropy loss between the target and predictions. Use this crossentropy loss function when there are two or more label classes. We expect target to be provided as integers. If you want to provide target using one-hot representation, please use CategoricalCrossentropy loss. There should be # classes floating point values per feature for preds and a single floating point value per feature for target . In the snippet below, there is a single floating point value per example for target and # classes floating pointing values per example for preds . The shape of target is [batch_size] and the shape of preds is [batch_size, num_classes] . Usage: target = jnp . array ([ 1 , 2 ]) preds = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = jm . losses . Crossentropy () result = scce ( target , preds ) # 1.177 assert np . isclose ( result , 1.177 , rtol = 0.01 ) # Calling with 'sample_weight'. result = scce ( target , preds , sample_weight = jnp . array ([ 0.3 , 0.7 ])) # 0.814 assert np . isclose ( result , 0.814 , rtol = 0.01 ) # Using 'sum' reduction type. scce = jm . losses . Crossentropy ( reduction = jm . losses . Reduction . SUM ) result = scce ( target , preds ) # 2.354 assert np . isclose ( result , 2.354 , rtol = 0.01 ) # Using 'none' reduction type. scce = jm . losses . Crossentropy ( reduction = jm . losses . Reduction . NONE ) result = scce ( target , preds ) # [0.0513, 2.303] assert jnp . all ( np . isclose ( result , [ 0.0513 , 2.303 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . Crossentropy (), metrics = elegy . metrics . Accuracy (), optimizer = optax . adam ( 1e-3 ), ) Source code in jax_metrics/losses/crossentropy.py class Crossentropy ( Loss ): \"\"\" Computes the crossentropy loss between the target and predictions. Use this crossentropy loss function when there are two or more label classes. We expect target to be provided as integers. If you want to provide target using `one-hot` representation, please use `CategoricalCrossentropy` loss. There should be `# classes` floating point values per feature for `preds` and a single floating point value per feature for `target`. In the snippet below, there is a single floating point value per example for `target` and `# classes` floating pointing values per example for `preds`. The shape of `target` is `[batch_size]` and the shape of `preds` is `[batch_size, num_classes]`. Usage: ```python target = jnp.array([1, 2]) preds = jnp.array([[0.05, 0.95, 0], [0.1, 0.8, 0.1]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = jm.losses.Crossentropy() result = scce(target, preds) # 1.177 assert np.isclose(result, 1.177, rtol=0.01) # Calling with 'sample_weight'. result = scce(target, preds, sample_weight=jnp.array([0.3, 0.7])) # 0.814 assert np.isclose(result, 0.814, rtol=0.01) # Using 'sum' reduction type. scce = jm.losses.Crossentropy( reduction=jm.losses.Reduction.SUM ) result = scce(target, preds) # 2.354 assert np.isclose(result, 2.354, rtol=0.01) # Using 'none' reduction type. scce = jm.losses.Crossentropy( reduction=jm.losses.Reduction.NONE ) result = scce(target, preds) # [0.0513, 2.303] assert jnp.all(np.isclose(result, [0.0513, 2.303], rtol=0.01)) ``` Usage with the `Elegy` API: ```python model = elegy.Model( module_fn, loss=jm.losses.Crossentropy(), metrics=elegy.metrics.Accuracy(), optimizer=optax.adam(1e-3), ) ``` \"\"\" def __init__ ( self , * , from_logits : bool = True , binary : bool = False , label_smoothing : tp . Optional [ float ] = None , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `preds` is expected to be a logits tensor. By default, we assume that `preds` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" super () . __init__ ( reduction = reduction , weight = weight , name = name ) self . _from_logits = from_logits self . _binary = binary self . _label_smoothing = label_smoothing def call ( self , target , preds , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: target: Ground truth values. preds: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return crossentropy ( target , preds , binary = self . _binary , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , )","title":"jax_metrics.losses.Crossentropy"},{"location":"api/losses/Crossentropy/#jax_metrics.losses.crossentropy.Crossentropy.__init__","text":"Initializes SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether preds is expected to be a logits tensor. By default, we assume that preds encodes a probability distribution. Note - Using from_logits=True is more numerically stable. True reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/crossentropy.py def __init__ ( self , * , from_logits : bool = True , binary : bool = False , label_smoothing : tp . Optional [ float ] = None , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `preds` is expected to be a logits tensor. By default, we assume that `preds` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" super () . __init__ ( reduction = reduction , weight = weight , name = name ) self . _from_logits = from_logits self . _binary = binary self . _label_smoothing = label_smoothing","title":"__init__()"},{"location":"api/losses/Crossentropy/#jax_metrics.losses.crossentropy.Crossentropy.call","text":"Invokes the SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default target Ground truth values. required preds The predicted values. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in jax_metrics/losses/crossentropy.py def call ( self , target , preds , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: target: Ground truth values. preds: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return crossentropy ( target , preds , binary = self . _binary , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , )","title":"call()"},{"location":"api/losses/Huber/","text":"jax_metrics.losses.Huber Computes the Huber loss between target and predictions. For each value x in error = target - preds: loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: target = jnp . array ([[ 0 , 1 ], [ 0 , 0 ]]) preds = jnp . array ([[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. huber_loss = jm . losses . Huber () assert huber_loss ( target , preds ) == 0.155 # Calling with 'sample_weight'. assert ( huber_loss ( target , preds , sample_weight = jnp . array ([ 0.8 , 0.2 ])) == 0.08500001 ) # Using 'sum' reduction type. huber_loss = jm . losses . Huber ( reduction = jm . losses . Reduction . SUM ) assert huber_loss ( target , preds ) == 0.31 # Using 'none' reduction type. huber_loss = jm . losses . Huber ( reduction = jm . losses . Reduction . NONE ) assert jnp . equal ( huber_loss ( target , preds ), jnp . array ([ 0.18 , 0.13000001 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . Huber ( delta = 1.0 ), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/huber.py class Huber ( Loss ): r \"\"\" Computes the Huber loss between target and predictions. For each value x in error = target - preds: $$ loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} $$ where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: ```python target = jnp.array([[0, 1], [0, 0]]) preds = jnp.array([[0.6, 0.4], [0.4, 0.6]]) # Using 'auto'/'sum_over_batch_size' reduction type. huber_loss = jm.losses.Huber() assert huber_loss(target, preds) == 0.155 # Calling with 'sample_weight'. assert ( huber_loss(target, preds, sample_weight=jnp.array([0.8, 0.2])) == 0.08500001 ) # Using 'sum' reduction type. huber_loss = jm.losses.Huber( reduction=jm.losses.Reduction.SUM ) assert huber_loss(target, preds) == 0.31 # Using 'none' reduction type. huber_loss = jm.losses.Huber( reduction=jm.losses.Reduction.NONE ) assert jnp.equal(huber_loss(target, preds), jnp.array([0.18, 0.13000001])).all() ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.Huber(delta=1.0), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , delta : float = 1.0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: delta: (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . delta = delta return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `Huber` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return huber ( target , preds , self . delta ) __init__ ( self , delta = 1.0 , reduction = None , weight = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default delta float (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. 1.0 reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/huber.py def __init__ ( self , delta : float = 1.0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: delta: (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . delta = delta return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) call ( self , target , preds , sample_weight = None , ** _ ) Invokes the Huber instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/huber.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `Huber` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return huber ( target , preds , self . delta )","title":"Huber"},{"location":"api/losses/Huber/#jax_metricslosseshuber","text":"Computes the Huber loss between target and predictions. For each value x in error = target - preds: loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: target = jnp . array ([[ 0 , 1 ], [ 0 , 0 ]]) preds = jnp . array ([[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. huber_loss = jm . losses . Huber () assert huber_loss ( target , preds ) == 0.155 # Calling with 'sample_weight'. assert ( huber_loss ( target , preds , sample_weight = jnp . array ([ 0.8 , 0.2 ])) == 0.08500001 ) # Using 'sum' reduction type. huber_loss = jm . losses . Huber ( reduction = jm . losses . Reduction . SUM ) assert huber_loss ( target , preds ) == 0.31 # Using 'none' reduction type. huber_loss = jm . losses . Huber ( reduction = jm . losses . Reduction . NONE ) assert jnp . equal ( huber_loss ( target , preds ), jnp . array ([ 0.18 , 0.13000001 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . Huber ( delta = 1.0 ), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/huber.py class Huber ( Loss ): r \"\"\" Computes the Huber loss between target and predictions. For each value x in error = target - preds: $$ loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} $$ where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: ```python target = jnp.array([[0, 1], [0, 0]]) preds = jnp.array([[0.6, 0.4], [0.4, 0.6]]) # Using 'auto'/'sum_over_batch_size' reduction type. huber_loss = jm.losses.Huber() assert huber_loss(target, preds) == 0.155 # Calling with 'sample_weight'. assert ( huber_loss(target, preds, sample_weight=jnp.array([0.8, 0.2])) == 0.08500001 ) # Using 'sum' reduction type. huber_loss = jm.losses.Huber( reduction=jm.losses.Reduction.SUM ) assert huber_loss(target, preds) == 0.31 # Using 'none' reduction type. huber_loss = jm.losses.Huber( reduction=jm.losses.Reduction.NONE ) assert jnp.equal(huber_loss(target, preds), jnp.array([0.18, 0.13000001])).all() ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.Huber(delta=1.0), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , delta : float = 1.0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: delta: (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . delta = delta return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `Huber` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return huber ( target , preds , self . delta )","title":"jax_metrics.losses.Huber"},{"location":"api/losses/Huber/#jax_metrics.losses.huber.Huber.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default delta float (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. 1.0 reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/huber.py def __init__ ( self , delta : float = 1.0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: delta: (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . delta = delta return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs )","title":"__init__()"},{"location":"api/losses/Huber/#jax_metrics.losses.huber.Huber.call","text":"Invokes the Huber instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/huber.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `Huber` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return huber ( target , preds , self . delta )","title":"call()"},{"location":"api/losses/Loss/","text":"jax_metrics.losses.Loss Loss base class. To be implemented by subclasses: call() : Contains the logic for loss calculation. Example subclass implementation: class MeanSquaredError ( Loss ): def call ( self , target , preds ): return jnp . mean ( jnp . square ( preds - target ), axis =- 1 ) Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this. Source code in jax_metrics/losses/loss.py class Loss ( ABC ): \"\"\" Loss base class. To be implemented by subclasses: * `call()`: Contains the logic for loss calculation. Example subclass implementation: ```python class MeanSquaredError(Loss): def call(self, target, preds): return jnp.mean(jnp.square(preds - target), axis=-1) ``` Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this. \"\"\" def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ types . ScalarLike ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. name: Optional name for the instance, if not provided lower snake_case version of the name of the class is used instead. \"\"\" self . name = name if name is not None else utils . _get_name ( self ) self . weight = ( jnp . asarray ( weight , dtype = jnp . float32 ) if weight is not None else jnp . array ( 1.0 , dtype = jnp . float32 ) ) self . reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) def __call__ ( self , ** kwargs , ) -> jnp . ndarray : sample_weight : tp . Optional [ jnp . ndarray ] = kwargs . pop ( \"sample_weight\" , None ) values = self . call ( ** kwargs ) return reduce_loss ( values , sample_weight , self . weight , self . reduction ) @abstractmethod def call ( self , ** kwargs ) -> jnp . ndarray : ... def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedLoss\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python losses = jm.Losses([ jm.losses.Crossentropy().index_into(target=[\"a\"]), jm.losses.Crossentropy().index_into(target=[\"b\"]), ]).reset() losses = losses.update( target={ \"a\": y0, \"b\": y1, }, preds=logits, ) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedLoss instance \"\"\" return IndexedLoss ( self , kwargs ) def map_arg ( self , ** kwargs : str ) -> \"MapArgsLoss\" : \"\"\" Returns a loss that renames the keyword arguments expected by `__call__`. Example: ```python crossentropy_loss = jm.losses.Crossentropy().map_arg(target=\"y_true\", preds=\"y_pred\") ... loss = crossentropy_loss(y_true=y, y_pred=logits) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A MapArgsLoss instance \"\"\" return MapArgsLoss ( self , kwargs ) __init__ ( self , reduction = None , weight = None , name = None ) special Initializes Loss class. Parameters: Name Type Description Default reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Union[float, numpy.ndarray, jax._src.numpy.lax_numpy.ndarray] Optional weight contribution for the total loss. Defaults to 1 . None name Optional[str] Optional name for the instance, if not provided lower snake_case version of the name of the class is used instead. None Source code in jax_metrics/losses/loss.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ types . ScalarLike ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. name: Optional name for the instance, if not provided lower snake_case version of the name of the class is used instead. \"\"\" self . name = name if name is not None else utils . _get_name ( self ) self . weight = ( jnp . asarray ( weight , dtype = jnp . float32 ) if weight is not None else jnp . array ( 1.0 , dtype = jnp . float32 ) ) self . reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) index_into ( self , ** kwargs ) Returns a metric that \"indexes\" the specified keyword arguments expected by .update() . You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing ( __getitem__ ). Examples: losses = jm . Losses ([ jm . losses . Crossentropy () . index_into ( target = [ \"a\" ]), jm . losses . Crossentropy () . index_into ( target = [ \"b\" ]), ]) . reset () losses = losses . update ( target = { \"a\" : y0 , \"b\" : y1 , }, preds = logits , ) Here values is set to a dict of arrays, but thanks to .index_into() each loss can index into its correspoding array. Parameters: Name Type Description Default **kwargs Union[str, int, Sequence[Union[str, int]]] keyword arguments to be indexed {} Returns: Type Description IndexedLoss A IndexedLoss instance Source code in jax_metrics/losses/loss.py def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedLoss\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python losses = jm.Losses([ jm.losses.Crossentropy().index_into(target=[\"a\"]), jm.losses.Crossentropy().index_into(target=[\"b\"]), ]).reset() losses = losses.update( target={ \"a\": y0, \"b\": y1, }, preds=logits, ) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedLoss instance \"\"\" return IndexedLoss ( self , kwargs ) map_arg ( self , ** kwargs ) Returns a loss that renames the keyword arguments expected by __call__ . Examples: crossentropy_loss = jm . losses . Crossentropy () . map_arg ( target = \"y_true\" , preds = \"y_pred\" ) ... loss = crossentropy_loss ( y_true = y , y_pred = logits ) Parameters: Name Type Description Default **kwargs str keyword arguments to be renamed {} Returns: Type Description MapArgsLoss A MapArgsLoss instance Source code in jax_metrics/losses/loss.py def map_arg ( self , ** kwargs : str ) -> \"MapArgsLoss\" : \"\"\" Returns a loss that renames the keyword arguments expected by `__call__`. Example: ```python crossentropy_loss = jm.losses.Crossentropy().map_arg(target=\"y_true\", preds=\"y_pred\") ... loss = crossentropy_loss(y_true=y, y_pred=logits) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A MapArgsLoss instance \"\"\" return MapArgsLoss ( self , kwargs )","title":"Loss"},{"location":"api/losses/Loss/#jax_metricslossesloss","text":"Loss base class. To be implemented by subclasses: call() : Contains the logic for loss calculation. Example subclass implementation: class MeanSquaredError ( Loss ): def call ( self , target , preds ): return jnp . mean ( jnp . square ( preds - target ), axis =- 1 ) Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this. Source code in jax_metrics/losses/loss.py class Loss ( ABC ): \"\"\" Loss base class. To be implemented by subclasses: * `call()`: Contains the logic for loss calculation. Example subclass implementation: ```python class MeanSquaredError(Loss): def call(self, target, preds): return jnp.mean(jnp.square(preds - target), axis=-1) ``` Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this. \"\"\" def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ types . ScalarLike ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. name: Optional name for the instance, if not provided lower snake_case version of the name of the class is used instead. \"\"\" self . name = name if name is not None else utils . _get_name ( self ) self . weight = ( jnp . asarray ( weight , dtype = jnp . float32 ) if weight is not None else jnp . array ( 1.0 , dtype = jnp . float32 ) ) self . reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) def __call__ ( self , ** kwargs , ) -> jnp . ndarray : sample_weight : tp . Optional [ jnp . ndarray ] = kwargs . pop ( \"sample_weight\" , None ) values = self . call ( ** kwargs ) return reduce_loss ( values , sample_weight , self . weight , self . reduction ) @abstractmethod def call ( self , ** kwargs ) -> jnp . ndarray : ... def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedLoss\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python losses = jm.Losses([ jm.losses.Crossentropy().index_into(target=[\"a\"]), jm.losses.Crossentropy().index_into(target=[\"b\"]), ]).reset() losses = losses.update( target={ \"a\": y0, \"b\": y1, }, preds=logits, ) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedLoss instance \"\"\" return IndexedLoss ( self , kwargs ) def map_arg ( self , ** kwargs : str ) -> \"MapArgsLoss\" : \"\"\" Returns a loss that renames the keyword arguments expected by `__call__`. Example: ```python crossentropy_loss = jm.losses.Crossentropy().map_arg(target=\"y_true\", preds=\"y_pred\") ... loss = crossentropy_loss(y_true=y, y_pred=logits) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A MapArgsLoss instance \"\"\" return MapArgsLoss ( self , kwargs )","title":"jax_metrics.losses.Loss"},{"location":"api/losses/Loss/#jax_metrics.losses.loss.Loss.__init__","text":"Initializes Loss class. Parameters: Name Type Description Default reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Union[float, numpy.ndarray, jax._src.numpy.lax_numpy.ndarray] Optional weight contribution for the total loss. Defaults to 1 . None name Optional[str] Optional name for the instance, if not provided lower snake_case version of the name of the class is used instead. None Source code in jax_metrics/losses/loss.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ types . ScalarLike ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. name: Optional name for the instance, if not provided lower snake_case version of the name of the class is used instead. \"\"\" self . name = name if name is not None else utils . _get_name ( self ) self . weight = ( jnp . asarray ( weight , dtype = jnp . float32 ) if weight is not None else jnp . array ( 1.0 , dtype = jnp . float32 ) ) self . reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE )","title":"__init__()"},{"location":"api/losses/Loss/#jax_metrics.losses.loss.Loss.index_into","text":"Returns a metric that \"indexes\" the specified keyword arguments expected by .update() . You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing ( __getitem__ ). Examples: losses = jm . Losses ([ jm . losses . Crossentropy () . index_into ( target = [ \"a\" ]), jm . losses . Crossentropy () . index_into ( target = [ \"b\" ]), ]) . reset () losses = losses . update ( target = { \"a\" : y0 , \"b\" : y1 , }, preds = logits , ) Here values is set to a dict of arrays, but thanks to .index_into() each loss can index into its correspoding array. Parameters: Name Type Description Default **kwargs Union[str, int, Sequence[Union[str, int]]] keyword arguments to be indexed {} Returns: Type Description IndexedLoss A IndexedLoss instance Source code in jax_metrics/losses/loss.py def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedLoss\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python losses = jm.Losses([ jm.losses.Crossentropy().index_into(target=[\"a\"]), jm.losses.Crossentropy().index_into(target=[\"b\"]), ]).reset() losses = losses.update( target={ \"a\": y0, \"b\": y1, }, preds=logits, ) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedLoss instance \"\"\" return IndexedLoss ( self , kwargs )","title":"index_into()"},{"location":"api/losses/Loss/#jax_metrics.losses.loss.Loss.map_arg","text":"Returns a loss that renames the keyword arguments expected by __call__ . Examples: crossentropy_loss = jm . losses . Crossentropy () . map_arg ( target = \"y_true\" , preds = \"y_pred\" ) ... loss = crossentropy_loss ( y_true = y , y_pred = logits ) Parameters: Name Type Description Default **kwargs str keyword arguments to be renamed {} Returns: Type Description MapArgsLoss A MapArgsLoss instance Source code in jax_metrics/losses/loss.py def map_arg ( self , ** kwargs : str ) -> \"MapArgsLoss\" : \"\"\" Returns a loss that renames the keyword arguments expected by `__call__`. Example: ```python crossentropy_loss = jm.losses.Crossentropy().map_arg(target=\"y_true\", preds=\"y_pred\") ... loss = crossentropy_loss(y_true=y, y_pred=logits) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A MapArgsLoss instance \"\"\" return MapArgsLoss ( self , kwargs )","title":"map_arg()"},{"location":"api/losses/MeanAbsoluteError/","text":"jax_metrics.losses.MeanAbsoluteError Computes the mean absolute errors between target and predictions. loss = mean(abs(target - preds)) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mae = jm . losses . MeanAbsoluteError () assert mae ( target , preds ) == 0.5 # Calling with 'sample_weight'. assert mae ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mae = jm . losses . MeanAbsoluteError ( reduction = jm . losses . Reduction . SUM ) assert mae ( target , preds ) == 1.0 # Using 'none' reduction type. mae = jm . losses . MeanAbsoluteError ( reduction = jm . losses . Reduction . NONE ) assert list ( mae ( target , preds )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanAbsoluteError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_absolute_error.py class MeanAbsoluteError ( Loss ): \"\"\" Computes the mean absolute errors between target and predictions. `loss = mean(abs(target - preds))` Usage: ```python target = jnp.array([[0.0, 1.0], [0.0, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. mae = jm.losses.MeanAbsoluteError() assert mae(target, preds) == 0.5 # Calling with 'sample_weight'. assert mae(target, preds, sample_weight=jnp.array([0.7, 0.3])) == 0.25 # Using 'sum' reduction type. mae = jm.losses.MeanAbsoluteError(reduction=jm.losses.Reduction.SUM) assert mae(target, preds) == 1.0 # Using 'none' reduction type. mae = jm.losses.MeanAbsoluteError(reduction=jm.losses.Reduction.NONE) assert list(mae(target, preds)) == [0.5, 0.5] ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanAbsoluteError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsoluteError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_error ( target , preds ) __init__ ( self , reduction = None , weight = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/mean_absolute_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) call ( self , target , preds , sample_weight = None , ** _ ) Invokes the MeanAbsoluteError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_absolute_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsoluteError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_error ( target , preds )","title":"MeanAbsoluteError"},{"location":"api/losses/MeanAbsoluteError/#jax_metricslossesmeanabsoluteerror","text":"Computes the mean absolute errors between target and predictions. loss = mean(abs(target - preds)) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mae = jm . losses . MeanAbsoluteError () assert mae ( target , preds ) == 0.5 # Calling with 'sample_weight'. assert mae ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mae = jm . losses . MeanAbsoluteError ( reduction = jm . losses . Reduction . SUM ) assert mae ( target , preds ) == 1.0 # Using 'none' reduction type. mae = jm . losses . MeanAbsoluteError ( reduction = jm . losses . Reduction . NONE ) assert list ( mae ( target , preds )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanAbsoluteError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_absolute_error.py class MeanAbsoluteError ( Loss ): \"\"\" Computes the mean absolute errors between target and predictions. `loss = mean(abs(target - preds))` Usage: ```python target = jnp.array([[0.0, 1.0], [0.0, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. mae = jm.losses.MeanAbsoluteError() assert mae(target, preds) == 0.5 # Calling with 'sample_weight'. assert mae(target, preds, sample_weight=jnp.array([0.7, 0.3])) == 0.25 # Using 'sum' reduction type. mae = jm.losses.MeanAbsoluteError(reduction=jm.losses.Reduction.SUM) assert mae(target, preds) == 1.0 # Using 'none' reduction type. mae = jm.losses.MeanAbsoluteError(reduction=jm.losses.Reduction.NONE) assert list(mae(target, preds)) == [0.5, 0.5] ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanAbsoluteError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsoluteError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_error ( target , preds )","title":"jax_metrics.losses.MeanAbsoluteError"},{"location":"api/losses/MeanAbsoluteError/#jax_metrics.losses.mean_absolute_error.MeanAbsoluteError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/mean_absolute_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs )","title":"__init__()"},{"location":"api/losses/MeanAbsoluteError/#jax_metrics.losses.mean_absolute_error.MeanAbsoluteError.call","text":"Invokes the MeanAbsoluteError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_absolute_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsoluteError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_error ( target , preds )","title":"call()"},{"location":"api/losses/MeanAbsolutePercentageError/","text":"jax_metrics.losses.MeanAbsolutePercentageError Computes the mean absolute errors between target and predictions. loss = mean(abs((target - preds) / target)) Usage: target = jnp . array ([[ 1.0 , 1.0 ], [ 0.9 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mape = jm . losses . MeanAbsolutePercentageError () result = mape ( target , preds ) assert np . isclose ( result , 2.78 , rtol = 0.01 ) # Calling with 'sample_weight'. assert np . isclose ( mape ( target , preds , sample_weight = jnp . array ([ 0.1 , 0.9 ])), 2.5 , rtol = 0.01 ) # Using 'sum' reduction type. mape = jm . losses . MeanAbsolutePercentageError ( reduction = jm . losses . Reduction . SUM ) assert np . isclose ( mape ( target , preds ), 5.6 , rtol = 0.01 ) # Using 'none' reduction type. mape = jm . losses . MeanAbsolutePercentageError ( reduction = jm . losses . Reduction . NONE ) assert jnp . all ( np . isclose ( result , [ 0. , 5.6 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanAbsolutePercentageError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_absolute_percentage_error.py class MeanAbsolutePercentageError ( Loss ): \"\"\" Computes the mean absolute errors between target and predictions. `loss = mean(abs((target - preds) / target))` Usage: ```python target = jnp.array([[1.0, 1.0], [0.9, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. mape = jm.losses.MeanAbsolutePercentageError() result = mape(target, preds) assert np.isclose(result, 2.78, rtol=0.01) # Calling with 'sample_weight'. assert np.isclose(mape(target, preds, sample_weight=jnp.array([0.1, 0.9])), 2.5, rtol=0.01) # Using 'sum' reduction type. mape = jm.losses.MeanAbsolutePercentageError(reduction=jm.losses.Reduction.SUM) assert np.isclose(mape(target, preds), 5.6, rtol=0.01) # Using 'none' reduction type. mape = jm.losses.MeanAbsolutePercentageError(reduction=jm.losses.Reduction.NONE) assert jnp.all(np.isclose(result, [0. , 5.6], rtol=0.01)) ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanAbsolutePercentageError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. for more details. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsolutePercentageError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_percentage_error ( target , preds ) __init__ ( self , reduction = None , weight = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. for more details. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/mean_absolute_percentage_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. for more details. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) call ( self , target , preds , sample_weight = None , ** _ ) Invokes the MeanAbsolutePercentageError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_absolute_percentage_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsolutePercentageError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_percentage_error ( target , preds )","title":"MeanAbsolutePercentageError"},{"location":"api/losses/MeanAbsolutePercentageError/#jax_metricslossesmeanabsolutepercentageerror","text":"Computes the mean absolute errors between target and predictions. loss = mean(abs((target - preds) / target)) Usage: target = jnp . array ([[ 1.0 , 1.0 ], [ 0.9 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mape = jm . losses . MeanAbsolutePercentageError () result = mape ( target , preds ) assert np . isclose ( result , 2.78 , rtol = 0.01 ) # Calling with 'sample_weight'. assert np . isclose ( mape ( target , preds , sample_weight = jnp . array ([ 0.1 , 0.9 ])), 2.5 , rtol = 0.01 ) # Using 'sum' reduction type. mape = jm . losses . MeanAbsolutePercentageError ( reduction = jm . losses . Reduction . SUM ) assert np . isclose ( mape ( target , preds ), 5.6 , rtol = 0.01 ) # Using 'none' reduction type. mape = jm . losses . MeanAbsolutePercentageError ( reduction = jm . losses . Reduction . NONE ) assert jnp . all ( np . isclose ( result , [ 0. , 5.6 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanAbsolutePercentageError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_absolute_percentage_error.py class MeanAbsolutePercentageError ( Loss ): \"\"\" Computes the mean absolute errors between target and predictions. `loss = mean(abs((target - preds) / target))` Usage: ```python target = jnp.array([[1.0, 1.0], [0.9, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. mape = jm.losses.MeanAbsolutePercentageError() result = mape(target, preds) assert np.isclose(result, 2.78, rtol=0.01) # Calling with 'sample_weight'. assert np.isclose(mape(target, preds, sample_weight=jnp.array([0.1, 0.9])), 2.5, rtol=0.01) # Using 'sum' reduction type. mape = jm.losses.MeanAbsolutePercentageError(reduction=jm.losses.Reduction.SUM) assert np.isclose(mape(target, preds), 5.6, rtol=0.01) # Using 'none' reduction type. mape = jm.losses.MeanAbsolutePercentageError(reduction=jm.losses.Reduction.NONE) assert jnp.all(np.isclose(result, [0. , 5.6], rtol=0.01)) ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanAbsolutePercentageError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. for more details. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsolutePercentageError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_percentage_error ( target , preds )","title":"jax_metrics.losses.MeanAbsolutePercentageError"},{"location":"api/losses/MeanAbsolutePercentageError/#jax_metrics.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. for more details. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/mean_absolute_percentage_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. for more details. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs )","title":"__init__()"},{"location":"api/losses/MeanAbsolutePercentageError/#jax_metrics.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError.call","text":"Invokes the MeanAbsolutePercentageError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_absolute_percentage_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsolutePercentageError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_percentage_error ( target , preds )","title":"call()"},{"location":"api/losses/MeanSquaredError/","text":"jax_metrics.losses.MeanSquaredError Computes the mean of squares of errors between target and predictions. loss = square(target - preds) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = jm . losses . MeanSquaredError () assert mse ( target , preds ) == 0.5 # Calling with 'sample_weight'. assert mse ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mse = jm . losses . MeanSquaredError ( reduction = jm . losses . Reduction . SUM ) assert mse ( target , preds ) == 1.0 # Using 'none' reduction type. mse = jm . losses . MeanSquaredError ( reduction = jm . losses . Reduction . NONE ) assert list ( mse ( target , preds )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanSquaredError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_squared_error.py class MeanSquaredError ( Loss ): \"\"\" Computes the mean of squares of errors between target and predictions. `loss = square(target - preds)` Usage: ```python target = jnp.array([[0.0, 1.0], [0.0, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = jm.losses.MeanSquaredError() assert mse(target, preds) == 0.5 # Calling with 'sample_weight'. assert mse(target, preds, sample_weight=jnp.array([0.7, 0.3])) == 0.25 # Using 'sum' reduction type. mse = jm.losses.MeanSquaredError(reduction=jm.losses.Reduction.SUM) assert mse(target, preds) == 1.0 # Using 'none' reduction type. mse = jm.losses.MeanSquaredError(reduction=jm.losses.Reduction.NONE) assert list(mse(target, preds)) == [0.5, 0.5] ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanSquaredError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , name = name ) def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( target , preds ) __init__ ( self , reduction = None , weight = None , name = None ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/mean_squared_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , name = name ) call ( self , target , preds , sample_weight = None , ** _ ) Invokes the MeanSquaredError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_squared_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( target , preds )","title":"MeanSquaredError"},{"location":"api/losses/MeanSquaredError/#jax_metricslossesmeansquarederror","text":"Computes the mean of squares of errors between target and predictions. loss = square(target - preds) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = jm . losses . MeanSquaredError () assert mse ( target , preds ) == 0.5 # Calling with 'sample_weight'. assert mse ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mse = jm . losses . MeanSquaredError ( reduction = jm . losses . Reduction . SUM ) assert mse ( target , preds ) == 1.0 # Using 'none' reduction type. mse = jm . losses . MeanSquaredError ( reduction = jm . losses . Reduction . NONE ) assert list ( mse ( target , preds )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanSquaredError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_squared_error.py class MeanSquaredError ( Loss ): \"\"\" Computes the mean of squares of errors between target and predictions. `loss = square(target - preds)` Usage: ```python target = jnp.array([[0.0, 1.0], [0.0, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = jm.losses.MeanSquaredError() assert mse(target, preds) == 0.5 # Calling with 'sample_weight'. assert mse(target, preds, sample_weight=jnp.array([0.7, 0.3])) == 0.25 # Using 'sum' reduction type. mse = jm.losses.MeanSquaredError(reduction=jm.losses.Reduction.SUM) assert mse(target, preds) == 1.0 # Using 'none' reduction type. mse = jm.losses.MeanSquaredError(reduction=jm.losses.Reduction.NONE) assert list(mse(target, preds)) == [0.5, 0.5] ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanSquaredError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , name = name ) def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( target , preds )","title":"jax_metrics.losses.MeanSquaredError"},{"location":"api/losses/MeanSquaredError/#jax_metrics.losses.mean_squared_error.MeanSquaredError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/mean_squared_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , name = name )","title":"__init__()"},{"location":"api/losses/MeanSquaredError/#jax_metrics.losses.mean_squared_error.MeanSquaredError.call","text":"Invokes the MeanSquaredError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_squared_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( target , preds )","title":"call()"},{"location":"api/losses/MeanSquaredLogarithmicError/","text":"jax_metrics.losses.MeanSquaredLogarithmicError Computes the mean squared logarithmic errors between target and predictions. loss = mean(square(log(target + 1) - log(preds + 1)), axis=-1) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. msle = jm . losses . MeanSquaredLogarithmicError () assert msle ( target , preds ) == 0.24022643 # Calling with 'sample_weight'. assert msle ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) = 0.12011322 # Using 'sum' reduction type. msle = jm . losses . MeanSquaredLogarithmicError ( reduction = jm . losses . Reduction . SUM ) assert msle ( target , preds ) == 0.48045287 # Using 'none' reduction type. msle = jm . losses . MeanSquaredLogarithmicError ( reduction = jm . losses . Reduction . NONE ) assert jnp . equal ( msle ( target , preds ), jnp . array ([ 0.24022643 , 0.24022643 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanSquaredLogarithmicError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_squared_logarithmic_error.py class MeanSquaredLogarithmicError ( Loss ): \"\"\" Computes the mean squared logarithmic errors between target and predictions. `loss = mean(square(log(target + 1) - log(preds + 1)), axis=-1)` Usage: ```python target = jnp.array([[0.0, 1.0], [0.0, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. msle = jm.losses.MeanSquaredLogarithmicError() assert msle(target, preds) == 0.24022643 # Calling with 'sample_weight'. assert msle(target, preds, sample_weight=jnp.array([0.7, 0.3])) = 0.12011322 # Using 'sum' reduction type. msle = jm.losses.MeanSquaredLogarithmicError(reduction=jm.losses.Reduction.SUM) assert msle(target, preds) == 0.48045287 # Using 'none' reduction type. msle = jm.losses.MeanSquaredLogarithmicError(reduction=jm.losses.Reduction.NONE) assert jnp.equal(msle(target, preds), jnp.array([0.24022643, 0.24022643])).all() ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanSquaredLogarithmicError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredLogarithmicError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_logarithmic_error ( target , preds ) __init__ ( self , reduction = None , weight = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/mean_squared_logarithmic_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) call ( self , target , preds , sample_weight = None , ** _ ) Invokes the MeanSquaredLogarithmicError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_squared_logarithmic_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredLogarithmicError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_logarithmic_error ( target , preds )","title":"MeanSquaredLogarithmicError"},{"location":"api/losses/MeanSquaredLogarithmicError/#jax_metricslossesmeansquaredlogarithmicerror","text":"Computes the mean squared logarithmic errors between target and predictions. loss = mean(square(log(target + 1) - log(preds + 1)), axis=-1) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. msle = jm . losses . MeanSquaredLogarithmicError () assert msle ( target , preds ) == 0.24022643 # Calling with 'sample_weight'. assert msle ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) = 0.12011322 # Using 'sum' reduction type. msle = jm . losses . MeanSquaredLogarithmicError ( reduction = jm . losses . Reduction . SUM ) assert msle ( target , preds ) == 0.48045287 # Using 'none' reduction type. msle = jm . losses . MeanSquaredLogarithmicError ( reduction = jm . losses . Reduction . NONE ) assert jnp . equal ( msle ( target , preds ), jnp . array ([ 0.24022643 , 0.24022643 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanSquaredLogarithmicError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_squared_logarithmic_error.py class MeanSquaredLogarithmicError ( Loss ): \"\"\" Computes the mean squared logarithmic errors between target and predictions. `loss = mean(square(log(target + 1) - log(preds + 1)), axis=-1)` Usage: ```python target = jnp.array([[0.0, 1.0], [0.0, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. msle = jm.losses.MeanSquaredLogarithmicError() assert msle(target, preds) == 0.24022643 # Calling with 'sample_weight'. assert msle(target, preds, sample_weight=jnp.array([0.7, 0.3])) = 0.12011322 # Using 'sum' reduction type. msle = jm.losses.MeanSquaredLogarithmicError(reduction=jm.losses.Reduction.SUM) assert msle(target, preds) == 0.48045287 # Using 'none' reduction type. msle = jm.losses.MeanSquaredLogarithmicError(reduction=jm.losses.Reduction.NONE) assert jnp.equal(msle(target, preds), jnp.array([0.24022643, 0.24022643])).all() ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanSquaredLogarithmicError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredLogarithmicError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_logarithmic_error ( target , preds )","title":"jax_metrics.losses.MeanSquaredLogarithmicError"},{"location":"api/losses/MeanSquaredLogarithmicError/#jax_metrics.losses.mean_squared_logarithmic_error.MeanSquaredLogarithmicError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/mean_squared_logarithmic_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs )","title":"__init__()"},{"location":"api/losses/MeanSquaredLogarithmicError/#jax_metrics.losses.mean_squared_logarithmic_error.MeanSquaredLogarithmicError.call","text":"Invokes the MeanSquaredLogarithmicError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_squared_logarithmic_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredLogarithmicError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_logarithmic_error ( target , preds )","title":"call()"},{"location":"api/losses/Reduction/","text":"jax_metrics.losses.Reduction Types of loss reduction. Contains the following values: * NONE : Weighted losses with one dimension reduced (axis=-1, or axis specified by loss function). When this reduction type used with built-in Keras training loops like fit / evaluate , the unreduced vector loss is passed to the optimizer but the reported loss will be a scalar value. * SUM : Scalar sum of weighted losses. * SUM_OVER_BATCH_SIZE : Scalar SUM divided by number of elements in losses. Source code in jax_metrics/losses/loss.py class Reduction ( Enum ): \"\"\" Types of loss reduction. Contains the following values: * `NONE`: Weighted losses with one dimension reduced (axis=-1, or axis specified by loss function). When this reduction type used with built-in Keras training loops like `fit`/`evaluate`, the unreduced vector loss is passed to the optimizer but the reported loss will be a scalar value. * `SUM`: Scalar sum of weighted losses. * `SUM_OVER_BATCH_SIZE`: Scalar `SUM` divided by number of elements in losses. \"\"\" # AUTO = \"auto\" NONE = \"none\" SUM = \"sum\" SUM_OVER_BATCH_SIZE = \"sum_over_batch_size\" @classmethod def all ( cls ): return ( # cls.AUTO, cls . NONE , cls . SUM , cls . SUM_OVER_BATCH_SIZE , ) @classmethod def validate ( cls , key ): if key not in cls . all (): raise ValueError ( \"Invalid Reduction Key %s .\" % key )","title":"Reduction"},{"location":"api/losses/Reduction/#jax_metricslossesreduction","text":"Types of loss reduction. Contains the following values: * NONE : Weighted losses with one dimension reduced (axis=-1, or axis specified by loss function). When this reduction type used with built-in Keras training loops like fit / evaluate , the unreduced vector loss is passed to the optimizer but the reported loss will be a scalar value. * SUM : Scalar sum of weighted losses. * SUM_OVER_BATCH_SIZE : Scalar SUM divided by number of elements in losses. Source code in jax_metrics/losses/loss.py class Reduction ( Enum ): \"\"\" Types of loss reduction. Contains the following values: * `NONE`: Weighted losses with one dimension reduced (axis=-1, or axis specified by loss function). When this reduction type used with built-in Keras training loops like `fit`/`evaluate`, the unreduced vector loss is passed to the optimizer but the reported loss will be a scalar value. * `SUM`: Scalar sum of weighted losses. * `SUM_OVER_BATCH_SIZE`: Scalar `SUM` divided by number of elements in losses. \"\"\" # AUTO = \"auto\" NONE = \"none\" SUM = \"sum\" SUM_OVER_BATCH_SIZE = \"sum_over_batch_size\" @classmethod def all ( cls ): return ( # cls.AUTO, cls . NONE , cls . SUM , cls . SUM_OVER_BATCH_SIZE , ) @classmethod def validate ( cls , key ): if key not in cls . all (): raise ValueError ( \"Invalid Reduction Key %s .\" % key )","title":"jax_metrics.losses.Reduction"},{"location":"api/losses/cosine_similarity/","text":"jax_metrics.losses.cosine_similarity CosineSimilarity ( Loss ) Computes the mean squared logarithmic errors between target and predictions. loss = -sum(l2_norm(target) * l2_norm(preds)) Usage: target = jnp . array ([[ 0. , 1. ], [ 1. , 1. ]]) preds = jnp . array ([[ 1. , 0. ], [ 1. , 1. ]]) # Using 'auto'/'sum_over_batch_size' reduction type. cosine_loss = jm . losses . CosineSimilarity ( axis = 1 ) assert cosine_loss ( target , preds ) == - 0.49999997 # Calling with 'sample_weight'. assert cosine_loss ( target , preds , sample_weight = jnp . array ([ 0.8 , 0.2 ])) == - 0.099999994 # Using 'sum' reduction type. cosine_loss = jm . losses . CosineSimilarity ( axis = 1 , reduction = jm . losses . Reduction . SUM ) assert cosine_loss ( target , preds ) == - 0.99999994 # Using 'none' reduction type. cosine_loss = jm . losses . CosineSimilarity ( axis = 1 , reduction = jm . losses . Reduction . NONE ) assert jnp . equal ( cosine_loss ( target , preds ), jnp . array ([ - 0. , - 0.99999994 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . CosineSimilarity ( axis = 1 ), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/cosine_similarity.py class CosineSimilarity ( Loss ): \"\"\" Computes the mean squared logarithmic errors between target and predictions. `loss = -sum(l2_norm(target) * l2_norm(preds))` Usage: ```python target = jnp.array([[0., 1.], [1., 1.]]) preds = jnp.array([[1., 0.], [1., 1.]]) # Using 'auto'/'sum_over_batch_size' reduction type. cosine_loss = jm.losses.CosineSimilarity(axis=1) assert cosine_loss(target, preds) == -0.49999997 # Calling with 'sample_weight'. assert cosine_loss(target, preds, sample_weight=jnp.array([0.8, 0.2])) == -0.099999994 # Using 'sum' reduction type. cosine_loss = jm.losses.CosineSimilarity(axis=1, reduction=jm.losses.Reduction.SUM ) assert cosine_loss(target, preds) == -0.99999994 # Using 'none' reduction type. cosine_loss = jm.losses.CosineSimilarity(axis=1, reduction=jm.losses.Reduction.NONE ) assert jnp.equal(cosine_loss(target, preds), jnp.array([-0., -0.99999994])).all() ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.CosineSimilarity(axis=1), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , axis : int = - 1 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: axis: (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . axis = axis return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `CosineSimilarity` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return cosine_similarity ( target , preds , self . axis ) __init__ ( self , axis =- 1 , reduction = None , weight = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default axis int (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. -1 reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/cosine_similarity.py def __init__ ( self , axis : int = - 1 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: axis: (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . axis = axis return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) call ( self , target , preds , sample_weight = None , ** _ ) Invokes the CosineSimilarity instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/cosine_similarity.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `CosineSimilarity` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return cosine_similarity ( target , preds , self . axis ) cosine_similarity ( target , preds , axis ) Computes the cosine similarity between target and predictions. loss = - sum ( l2_norm ( target ) * l2_norm ( preds )) Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = jm . losses . cosine_similarity ( target , preds , axis = 1 ) assert loss . shape == ( 2 ,) target = target / jnp . maximum ( jnp . linalg . norm ( target , axis = 1 , keepdims = True ), jnp . sqrt ( types . EPSILON )) preds = preds / jnp . maximum ( jnp . linalg . norm ( preds , axis = 1 , keepdims = True ), jnp . sqrt ( types . EPSILON )) assert jnp . array_equal ( loss , - jnp . sum ( target * preds , axis = 1 )) Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] . required axis int The dimension along which the cosine similarity is computed. required Returns: Type Description ndarray cosine similarity Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Source code in jax_metrics/losses/cosine_similarity.py def cosine_similarity ( target : jnp . ndarray , preds : jnp . ndarray , axis : int ) -> jnp . ndarray : \"\"\" Computes the cosine similarity between target and predictions. ```python loss = -sum(l2_norm(target) * l2_norm(preds)) ``` Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = jm.losses.cosine_similarity(target, preds, axis=1) assert loss.shape == (2,) target = target / jnp.maximum(jnp.linalg.norm(target, axis=1, keepdims=True), jnp.sqrt(types.EPSILON)) preds = preds / jnp.maximum(jnp.linalg.norm(preds, axis=1, keepdims=True), jnp.sqrt(types.EPSILON)) assert jnp.array_equal(loss, -jnp.sum(target * preds, axis=1)) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. axis: The dimension along which the cosine similarity is computed. Returns: cosine similarity Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) \"\"\" target = target / jnp . maximum ( jnp . linalg . norm ( target , axis = axis , keepdims = True ), jnp . sqrt ( types . EPSILON ) ) preds = preds / jnp . maximum ( jnp . linalg . norm ( preds , axis = axis , keepdims = True ), jnp . sqrt ( types . EPSILON ) ) return - jnp . sum ( target * preds , axis = axis )","title":"cosine_similarity"},{"location":"api/losses/cosine_similarity/#jax_metricslossescosine_similarity","text":"","title":"jax_metrics.losses.cosine_similarity"},{"location":"api/losses/cosine_similarity/#jax_metrics.losses.cosine_similarity.CosineSimilarity","text":"Computes the mean squared logarithmic errors between target and predictions. loss = -sum(l2_norm(target) * l2_norm(preds)) Usage: target = jnp . array ([[ 0. , 1. ], [ 1. , 1. ]]) preds = jnp . array ([[ 1. , 0. ], [ 1. , 1. ]]) # Using 'auto'/'sum_over_batch_size' reduction type. cosine_loss = jm . losses . CosineSimilarity ( axis = 1 ) assert cosine_loss ( target , preds ) == - 0.49999997 # Calling with 'sample_weight'. assert cosine_loss ( target , preds , sample_weight = jnp . array ([ 0.8 , 0.2 ])) == - 0.099999994 # Using 'sum' reduction type. cosine_loss = jm . losses . CosineSimilarity ( axis = 1 , reduction = jm . losses . Reduction . SUM ) assert cosine_loss ( target , preds ) == - 0.99999994 # Using 'none' reduction type. cosine_loss = jm . losses . CosineSimilarity ( axis = 1 , reduction = jm . losses . Reduction . NONE ) assert jnp . equal ( cosine_loss ( target , preds ), jnp . array ([ - 0. , - 0.99999994 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . CosineSimilarity ( axis = 1 ), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/cosine_similarity.py class CosineSimilarity ( Loss ): \"\"\" Computes the mean squared logarithmic errors between target and predictions. `loss = -sum(l2_norm(target) * l2_norm(preds))` Usage: ```python target = jnp.array([[0., 1.], [1., 1.]]) preds = jnp.array([[1., 0.], [1., 1.]]) # Using 'auto'/'sum_over_batch_size' reduction type. cosine_loss = jm.losses.CosineSimilarity(axis=1) assert cosine_loss(target, preds) == -0.49999997 # Calling with 'sample_weight'. assert cosine_loss(target, preds, sample_weight=jnp.array([0.8, 0.2])) == -0.099999994 # Using 'sum' reduction type. cosine_loss = jm.losses.CosineSimilarity(axis=1, reduction=jm.losses.Reduction.SUM ) assert cosine_loss(target, preds) == -0.99999994 # Using 'none' reduction type. cosine_loss = jm.losses.CosineSimilarity(axis=1, reduction=jm.losses.Reduction.NONE ) assert jnp.equal(cosine_loss(target, preds), jnp.array([-0., -0.99999994])).all() ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.CosineSimilarity(axis=1), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , axis : int = - 1 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: axis: (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . axis = axis return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `CosineSimilarity` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return cosine_similarity ( target , preds , self . axis )","title":"CosineSimilarity"},{"location":"api/losses/cosine_similarity/#jax_metrics.losses.cosine_similarity.CosineSimilarity.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default axis int (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. -1 reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/cosine_similarity.py def __init__ ( self , axis : int = - 1 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: axis: (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . axis = axis return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs )","title":"__init__()"},{"location":"api/losses/cosine_similarity/#jax_metrics.losses.cosine_similarity.CosineSimilarity.call","text":"Invokes the CosineSimilarity instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/cosine_similarity.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `CosineSimilarity` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return cosine_similarity ( target , preds , self . axis )","title":"call()"},{"location":"api/losses/cosine_similarity/#jax_metrics.losses.cosine_similarity.cosine_similarity","text":"Computes the cosine similarity between target and predictions. loss = - sum ( l2_norm ( target ) * l2_norm ( preds )) Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = jm . losses . cosine_similarity ( target , preds , axis = 1 ) assert loss . shape == ( 2 ,) target = target / jnp . maximum ( jnp . linalg . norm ( target , axis = 1 , keepdims = True ), jnp . sqrt ( types . EPSILON )) preds = preds / jnp . maximum ( jnp . linalg . norm ( preds , axis = 1 , keepdims = True ), jnp . sqrt ( types . EPSILON )) assert jnp . array_equal ( loss , - jnp . sum ( target * preds , axis = 1 )) Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] . required axis int The dimension along which the cosine similarity is computed. required Returns: Type Description ndarray cosine similarity Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Source code in jax_metrics/losses/cosine_similarity.py def cosine_similarity ( target : jnp . ndarray , preds : jnp . ndarray , axis : int ) -> jnp . ndarray : \"\"\" Computes the cosine similarity between target and predictions. ```python loss = -sum(l2_norm(target) * l2_norm(preds)) ``` Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = jm.losses.cosine_similarity(target, preds, axis=1) assert loss.shape == (2,) target = target / jnp.maximum(jnp.linalg.norm(target, axis=1, keepdims=True), jnp.sqrt(types.EPSILON)) preds = preds / jnp.maximum(jnp.linalg.norm(preds, axis=1, keepdims=True), jnp.sqrt(types.EPSILON)) assert jnp.array_equal(loss, -jnp.sum(target * preds, axis=1)) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. axis: The dimension along which the cosine similarity is computed. Returns: cosine similarity Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) \"\"\" target = target / jnp . maximum ( jnp . linalg . norm ( target , axis = axis , keepdims = True ), jnp . sqrt ( types . EPSILON ) ) preds = preds / jnp . maximum ( jnp . linalg . norm ( preds , axis = axis , keepdims = True ), jnp . sqrt ( types . EPSILON ) ) return - jnp . sum ( target * preds , axis = axis )","title":"cosine_similarity()"},{"location":"api/losses/crossentropy/","text":"jax_metrics.losses.crossentropy Crossentropy ( Loss ) Computes the crossentropy loss between the target and predictions. Use this crossentropy loss function when there are two or more label classes. We expect target to be provided as integers. If you want to provide target using one-hot representation, please use CategoricalCrossentropy loss. There should be # classes floating point values per feature for preds and a single floating point value per feature for target . In the snippet below, there is a single floating point value per example for target and # classes floating pointing values per example for preds . The shape of target is [batch_size] and the shape of preds is [batch_size, num_classes] . Usage: target = jnp . array ([ 1 , 2 ]) preds = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = jm . losses . Crossentropy () result = scce ( target , preds ) # 1.177 assert np . isclose ( result , 1.177 , rtol = 0.01 ) # Calling with 'sample_weight'. result = scce ( target , preds , sample_weight = jnp . array ([ 0.3 , 0.7 ])) # 0.814 assert np . isclose ( result , 0.814 , rtol = 0.01 ) # Using 'sum' reduction type. scce = jm . losses . Crossentropy ( reduction = jm . losses . Reduction . SUM ) result = scce ( target , preds ) # 2.354 assert np . isclose ( result , 2.354 , rtol = 0.01 ) # Using 'none' reduction type. scce = jm . losses . Crossentropy ( reduction = jm . losses . Reduction . NONE ) result = scce ( target , preds ) # [0.0513, 2.303] assert jnp . all ( np . isclose ( result , [ 0.0513 , 2.303 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . Crossentropy (), metrics = elegy . metrics . Accuracy (), optimizer = optax . adam ( 1e-3 ), ) Source code in jax_metrics/losses/crossentropy.py class Crossentropy ( Loss ): \"\"\" Computes the crossentropy loss between the target and predictions. Use this crossentropy loss function when there are two or more label classes. We expect target to be provided as integers. If you want to provide target using `one-hot` representation, please use `CategoricalCrossentropy` loss. There should be `# classes` floating point values per feature for `preds` and a single floating point value per feature for `target`. In the snippet below, there is a single floating point value per example for `target` and `# classes` floating pointing values per example for `preds`. The shape of `target` is `[batch_size]` and the shape of `preds` is `[batch_size, num_classes]`. Usage: ```python target = jnp.array([1, 2]) preds = jnp.array([[0.05, 0.95, 0], [0.1, 0.8, 0.1]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = jm.losses.Crossentropy() result = scce(target, preds) # 1.177 assert np.isclose(result, 1.177, rtol=0.01) # Calling with 'sample_weight'. result = scce(target, preds, sample_weight=jnp.array([0.3, 0.7])) # 0.814 assert np.isclose(result, 0.814, rtol=0.01) # Using 'sum' reduction type. scce = jm.losses.Crossentropy( reduction=jm.losses.Reduction.SUM ) result = scce(target, preds) # 2.354 assert np.isclose(result, 2.354, rtol=0.01) # Using 'none' reduction type. scce = jm.losses.Crossentropy( reduction=jm.losses.Reduction.NONE ) result = scce(target, preds) # [0.0513, 2.303] assert jnp.all(np.isclose(result, [0.0513, 2.303], rtol=0.01)) ``` Usage with the `Elegy` API: ```python model = elegy.Model( module_fn, loss=jm.losses.Crossentropy(), metrics=elegy.metrics.Accuracy(), optimizer=optax.adam(1e-3), ) ``` \"\"\" def __init__ ( self , * , from_logits : bool = True , binary : bool = False , label_smoothing : tp . Optional [ float ] = None , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `preds` is expected to be a logits tensor. By default, we assume that `preds` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" super () . __init__ ( reduction = reduction , weight = weight , name = name ) self . _from_logits = from_logits self . _binary = binary self . _label_smoothing = label_smoothing def call ( self , target , preds , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: target: Ground truth values. preds: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return crossentropy ( target , preds , binary = self . _binary , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , ) __init__ ( self , * , from_logits = True , binary = False , label_smoothing = None , reduction = None , weight = None , name = None ) special Initializes SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether preds is expected to be a logits tensor. By default, we assume that preds encodes a probability distribution. Note - Using from_logits=True is more numerically stable. True reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/crossentropy.py def __init__ ( self , * , from_logits : bool = True , binary : bool = False , label_smoothing : tp . Optional [ float ] = None , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `preds` is expected to be a logits tensor. By default, we assume that `preds` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" super () . __init__ ( reduction = reduction , weight = weight , name = name ) self . _from_logits = from_logits self . _binary = binary self . _label_smoothing = label_smoothing call ( self , target , preds , sample_weight = None , ** _ ) Invokes the SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default target Ground truth values. required preds The predicted values. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in jax_metrics/losses/crossentropy.py def call ( self , target , preds , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: target: Ground truth values. preds: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return crossentropy ( target , preds , binary = self . _binary , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , )","title":"crossentropy"},{"location":"api/losses/crossentropy/#jax_metricslossescrossentropy","text":"","title":"jax_metrics.losses.crossentropy"},{"location":"api/losses/crossentropy/#jax_metrics.losses.crossentropy.Crossentropy","text":"Computes the crossentropy loss between the target and predictions. Use this crossentropy loss function when there are two or more label classes. We expect target to be provided as integers. If you want to provide target using one-hot representation, please use CategoricalCrossentropy loss. There should be # classes floating point values per feature for preds and a single floating point value per feature for target . In the snippet below, there is a single floating point value per example for target and # classes floating pointing values per example for preds . The shape of target is [batch_size] and the shape of preds is [batch_size, num_classes] . Usage: target = jnp . array ([ 1 , 2 ]) preds = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = jm . losses . Crossentropy () result = scce ( target , preds ) # 1.177 assert np . isclose ( result , 1.177 , rtol = 0.01 ) # Calling with 'sample_weight'. result = scce ( target , preds , sample_weight = jnp . array ([ 0.3 , 0.7 ])) # 0.814 assert np . isclose ( result , 0.814 , rtol = 0.01 ) # Using 'sum' reduction type. scce = jm . losses . Crossentropy ( reduction = jm . losses . Reduction . SUM ) result = scce ( target , preds ) # 2.354 assert np . isclose ( result , 2.354 , rtol = 0.01 ) # Using 'none' reduction type. scce = jm . losses . Crossentropy ( reduction = jm . losses . Reduction . NONE ) result = scce ( target , preds ) # [0.0513, 2.303] assert jnp . all ( np . isclose ( result , [ 0.0513 , 2.303 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . Crossentropy (), metrics = elegy . metrics . Accuracy (), optimizer = optax . adam ( 1e-3 ), ) Source code in jax_metrics/losses/crossentropy.py class Crossentropy ( Loss ): \"\"\" Computes the crossentropy loss between the target and predictions. Use this crossentropy loss function when there are two or more label classes. We expect target to be provided as integers. If you want to provide target using `one-hot` representation, please use `CategoricalCrossentropy` loss. There should be `# classes` floating point values per feature for `preds` and a single floating point value per feature for `target`. In the snippet below, there is a single floating point value per example for `target` and `# classes` floating pointing values per example for `preds`. The shape of `target` is `[batch_size]` and the shape of `preds` is `[batch_size, num_classes]`. Usage: ```python target = jnp.array([1, 2]) preds = jnp.array([[0.05, 0.95, 0], [0.1, 0.8, 0.1]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = jm.losses.Crossentropy() result = scce(target, preds) # 1.177 assert np.isclose(result, 1.177, rtol=0.01) # Calling with 'sample_weight'. result = scce(target, preds, sample_weight=jnp.array([0.3, 0.7])) # 0.814 assert np.isclose(result, 0.814, rtol=0.01) # Using 'sum' reduction type. scce = jm.losses.Crossentropy( reduction=jm.losses.Reduction.SUM ) result = scce(target, preds) # 2.354 assert np.isclose(result, 2.354, rtol=0.01) # Using 'none' reduction type. scce = jm.losses.Crossentropy( reduction=jm.losses.Reduction.NONE ) result = scce(target, preds) # [0.0513, 2.303] assert jnp.all(np.isclose(result, [0.0513, 2.303], rtol=0.01)) ``` Usage with the `Elegy` API: ```python model = elegy.Model( module_fn, loss=jm.losses.Crossentropy(), metrics=elegy.metrics.Accuracy(), optimizer=optax.adam(1e-3), ) ``` \"\"\" def __init__ ( self , * , from_logits : bool = True , binary : bool = False , label_smoothing : tp . Optional [ float ] = None , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `preds` is expected to be a logits tensor. By default, we assume that `preds` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" super () . __init__ ( reduction = reduction , weight = weight , name = name ) self . _from_logits = from_logits self . _binary = binary self . _label_smoothing = label_smoothing def call ( self , target , preds , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: target: Ground truth values. preds: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return crossentropy ( target , preds , binary = self . _binary , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , )","title":"Crossentropy"},{"location":"api/losses/crossentropy/#jax_metrics.losses.crossentropy.Crossentropy.__init__","text":"Initializes SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether preds is expected to be a logits tensor. By default, we assume that preds encodes a probability distribution. Note - Using from_logits=True is more numerically stable. True reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/crossentropy.py def __init__ ( self , * , from_logits : bool = True , binary : bool = False , label_smoothing : tp . Optional [ float ] = None , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `preds` is expected to be a logits tensor. By default, we assume that `preds` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" super () . __init__ ( reduction = reduction , weight = weight , name = name ) self . _from_logits = from_logits self . _binary = binary self . _label_smoothing = label_smoothing","title":"__init__()"},{"location":"api/losses/crossentropy/#jax_metrics.losses.crossentropy.Crossentropy.call","text":"Invokes the SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default target Ground truth values. required preds The predicted values. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in jax_metrics/losses/crossentropy.py def call ( self , target , preds , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: target: Ground truth values. preds: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return crossentropy ( target , preds , binary = self . _binary , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , )","title":"call()"},{"location":"api/losses/huber/","text":"jax_metrics.losses.huber Huber ( Loss ) Computes the Huber loss between target and predictions. For each value x in error = target - preds: loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: target = jnp . array ([[ 0 , 1 ], [ 0 , 0 ]]) preds = jnp . array ([[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. huber_loss = jm . losses . Huber () assert huber_loss ( target , preds ) == 0.155 # Calling with 'sample_weight'. assert ( huber_loss ( target , preds , sample_weight = jnp . array ([ 0.8 , 0.2 ])) == 0.08500001 ) # Using 'sum' reduction type. huber_loss = jm . losses . Huber ( reduction = jm . losses . Reduction . SUM ) assert huber_loss ( target , preds ) == 0.31 # Using 'none' reduction type. huber_loss = jm . losses . Huber ( reduction = jm . losses . Reduction . NONE ) assert jnp . equal ( huber_loss ( target , preds ), jnp . array ([ 0.18 , 0.13000001 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . Huber ( delta = 1.0 ), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/huber.py class Huber ( Loss ): r \"\"\" Computes the Huber loss between target and predictions. For each value x in error = target - preds: $$ loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} $$ where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: ```python target = jnp.array([[0, 1], [0, 0]]) preds = jnp.array([[0.6, 0.4], [0.4, 0.6]]) # Using 'auto'/'sum_over_batch_size' reduction type. huber_loss = jm.losses.Huber() assert huber_loss(target, preds) == 0.155 # Calling with 'sample_weight'. assert ( huber_loss(target, preds, sample_weight=jnp.array([0.8, 0.2])) == 0.08500001 ) # Using 'sum' reduction type. huber_loss = jm.losses.Huber( reduction=jm.losses.Reduction.SUM ) assert huber_loss(target, preds) == 0.31 # Using 'none' reduction type. huber_loss = jm.losses.Huber( reduction=jm.losses.Reduction.NONE ) assert jnp.equal(huber_loss(target, preds), jnp.array([0.18, 0.13000001])).all() ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.Huber(delta=1.0), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , delta : float = 1.0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: delta: (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . delta = delta return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `Huber` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return huber ( target , preds , self . delta ) __init__ ( self , delta = 1.0 , reduction = None , weight = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default delta float (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. 1.0 reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/huber.py def __init__ ( self , delta : float = 1.0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: delta: (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . delta = delta return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) call ( self , target , preds , sample_weight = None , ** _ ) Invokes the Huber instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/huber.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `Huber` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return huber ( target , preds , self . delta ) huber ( target , preds , delta ) Computes the Huber loss between target and predictions. For each value x in error = target - preds: loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = jm . losses . huber ( target , preds , delta = 1.0 ) assert loss . shape == ( 2 ,) preds = preds . astype ( float ) target = target . astype ( float ) delta = 1.0 error = jnp . subtract ( preds , target ) abs_error = jnp . abs ( error ) quadratic = jnp . minimum ( abs_error , delta ) linear = jnp . subtract ( abs_error , quadratic ) assert jnp . array_equal ( loss , jnp . mean ( jnp . add ( jnp . multiply ( 0.5 , jnp . multiply ( quadratic , quadratic ) ), jnp . multiply ( delta , linear )), axis =- 1 )) Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] . required delta float A float, the point where the Huber loss function changes from a quadratic to linear. required Returns: Type Description ndarray huber loss Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Source code in jax_metrics/losses/huber.py def huber ( target : jnp . ndarray , preds : jnp . ndarray , delta : float ) -> jnp . ndarray : r \"\"\" Computes the Huber loss between target and predictions. For each value x in error = target - preds: $$ loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} $$ where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = jm.losses.huber(target, preds, delta=1.0) assert loss.shape == (2,) preds = preds.astype(float) target = target.astype(float) delta = 1.0 error = jnp.subtract(preds, target) abs_error = jnp.abs(error) quadratic = jnp.minimum(abs_error, delta) linear = jnp.subtract(abs_error, quadratic) assert jnp.array_equal(loss, jnp.mean( jnp.add( jnp.multiply( 0.5, jnp.multiply(quadratic, quadratic) ), jnp.multiply(delta, linear)), axis=-1 )) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. delta: A float, the point where the Huber loss function changes from a quadratic to linear. Returns: huber loss Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) \"\"\" preds = preds . astype ( float ) target = target . astype ( float ) delta = float ( delta ) error = jnp . subtract ( preds , target ) abs_error = jnp . abs ( error ) quadratic = jnp . minimum ( abs_error , delta ) linear = jnp . subtract ( abs_error , quadratic ) return jnp . mean ( jnp . add ( jnp . multiply ( 0.5 , jnp . multiply ( quadratic , quadratic )), jnp . multiply ( delta , linear ), ), axis =- 1 , )","title":"huber"},{"location":"api/losses/huber/#jax_metricslosseshuber","text":"","title":"jax_metrics.losses.huber"},{"location":"api/losses/huber/#jax_metrics.losses.huber.Huber","text":"Computes the Huber loss between target and predictions. For each value x in error = target - preds: loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: target = jnp . array ([[ 0 , 1 ], [ 0 , 0 ]]) preds = jnp . array ([[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. huber_loss = jm . losses . Huber () assert huber_loss ( target , preds ) == 0.155 # Calling with 'sample_weight'. assert ( huber_loss ( target , preds , sample_weight = jnp . array ([ 0.8 , 0.2 ])) == 0.08500001 ) # Using 'sum' reduction type. huber_loss = jm . losses . Huber ( reduction = jm . losses . Reduction . SUM ) assert huber_loss ( target , preds ) == 0.31 # Using 'none' reduction type. huber_loss = jm . losses . Huber ( reduction = jm . losses . Reduction . NONE ) assert jnp . equal ( huber_loss ( target , preds ), jnp . array ([ 0.18 , 0.13000001 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . Huber ( delta = 1.0 ), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/huber.py class Huber ( Loss ): r \"\"\" Computes the Huber loss between target and predictions. For each value x in error = target - preds: $$ loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} $$ where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: ```python target = jnp.array([[0, 1], [0, 0]]) preds = jnp.array([[0.6, 0.4], [0.4, 0.6]]) # Using 'auto'/'sum_over_batch_size' reduction type. huber_loss = jm.losses.Huber() assert huber_loss(target, preds) == 0.155 # Calling with 'sample_weight'. assert ( huber_loss(target, preds, sample_weight=jnp.array([0.8, 0.2])) == 0.08500001 ) # Using 'sum' reduction type. huber_loss = jm.losses.Huber( reduction=jm.losses.Reduction.SUM ) assert huber_loss(target, preds) == 0.31 # Using 'none' reduction type. huber_loss = jm.losses.Huber( reduction=jm.losses.Reduction.NONE ) assert jnp.equal(huber_loss(target, preds), jnp.array([0.18, 0.13000001])).all() ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.Huber(delta=1.0), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , delta : float = 1.0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: delta: (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . delta = delta return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `Huber` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return huber ( target , preds , self . delta )","title":"Huber"},{"location":"api/losses/huber/#jax_metrics.losses.huber.Huber.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default delta float (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. 1.0 reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/huber.py def __init__ ( self , delta : float = 1.0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: delta: (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . delta = delta return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs )","title":"__init__()"},{"location":"api/losses/huber/#jax_metrics.losses.huber.Huber.call","text":"Invokes the Huber instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/huber.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `Huber` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return huber ( target , preds , self . delta )","title":"call()"},{"location":"api/losses/huber/#jax_metrics.losses.huber.huber","text":"Computes the Huber loss between target and predictions. For each value x in error = target - preds: loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = jm . losses . huber ( target , preds , delta = 1.0 ) assert loss . shape == ( 2 ,) preds = preds . astype ( float ) target = target . astype ( float ) delta = 1.0 error = jnp . subtract ( preds , target ) abs_error = jnp . abs ( error ) quadratic = jnp . minimum ( abs_error , delta ) linear = jnp . subtract ( abs_error , quadratic ) assert jnp . array_equal ( loss , jnp . mean ( jnp . add ( jnp . multiply ( 0.5 , jnp . multiply ( quadratic , quadratic ) ), jnp . multiply ( delta , linear )), axis =- 1 )) Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] . required delta float A float, the point where the Huber loss function changes from a quadratic to linear. required Returns: Type Description ndarray huber loss Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Source code in jax_metrics/losses/huber.py def huber ( target : jnp . ndarray , preds : jnp . ndarray , delta : float ) -> jnp . ndarray : r \"\"\" Computes the Huber loss between target and predictions. For each value x in error = target - preds: $$ loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} $$ where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = jm.losses.huber(target, preds, delta=1.0) assert loss.shape == (2,) preds = preds.astype(float) target = target.astype(float) delta = 1.0 error = jnp.subtract(preds, target) abs_error = jnp.abs(error) quadratic = jnp.minimum(abs_error, delta) linear = jnp.subtract(abs_error, quadratic) assert jnp.array_equal(loss, jnp.mean( jnp.add( jnp.multiply( 0.5, jnp.multiply(quadratic, quadratic) ), jnp.multiply(delta, linear)), axis=-1 )) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. delta: A float, the point where the Huber loss function changes from a quadratic to linear. Returns: huber loss Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) \"\"\" preds = preds . astype ( float ) target = target . astype ( float ) delta = float ( delta ) error = jnp . subtract ( preds , target ) abs_error = jnp . abs ( error ) quadratic = jnp . minimum ( abs_error , delta ) linear = jnp . subtract ( abs_error , quadratic ) return jnp . mean ( jnp . add ( jnp . multiply ( 0.5 , jnp . multiply ( quadratic , quadratic )), jnp . multiply ( delta , linear ), ), axis =- 1 , )","title":"huber()"},{"location":"api/losses/mean_absolute_error/","text":"jax_metrics.losses.mean_absolute_error MeanAbsoluteError ( Loss ) Computes the mean absolute errors between target and predictions. loss = mean(abs(target - preds)) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mae = jm . losses . MeanAbsoluteError () assert mae ( target , preds ) == 0.5 # Calling with 'sample_weight'. assert mae ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mae = jm . losses . MeanAbsoluteError ( reduction = jm . losses . Reduction . SUM ) assert mae ( target , preds ) == 1.0 # Using 'none' reduction type. mae = jm . losses . MeanAbsoluteError ( reduction = jm . losses . Reduction . NONE ) assert list ( mae ( target , preds )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanAbsoluteError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_absolute_error.py class MeanAbsoluteError ( Loss ): \"\"\" Computes the mean absolute errors between target and predictions. `loss = mean(abs(target - preds))` Usage: ```python target = jnp.array([[0.0, 1.0], [0.0, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. mae = jm.losses.MeanAbsoluteError() assert mae(target, preds) == 0.5 # Calling with 'sample_weight'. assert mae(target, preds, sample_weight=jnp.array([0.7, 0.3])) == 0.25 # Using 'sum' reduction type. mae = jm.losses.MeanAbsoluteError(reduction=jm.losses.Reduction.SUM) assert mae(target, preds) == 1.0 # Using 'none' reduction type. mae = jm.losses.MeanAbsoluteError(reduction=jm.losses.Reduction.NONE) assert list(mae(target, preds)) == [0.5, 0.5] ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanAbsoluteError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsoluteError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_error ( target , preds ) __init__ ( self , reduction = None , weight = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/mean_absolute_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) call ( self , target , preds , sample_weight = None , ** _ ) Invokes the MeanAbsoluteError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_absolute_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsoluteError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_error ( target , preds ) mean_absolute_error ( target , preds ) Computes the mean absolute error between target and predictions. After computing the absolute distance between the inputs, the mean value over the last dimension is returned. loss = mean ( abs ( target - preds ), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = jm . losses . mean_absolute_error ( target , preds ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , jnp . mean ( jnp . abs ( target - preds ), axis =- 1 )) Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean absolute error values. shape = [batch_size, d0, .. dN-1] . Source code in jax_metrics/losses/mean_absolute_error.py def mean_absolute_error ( target : jnp . ndarray , preds : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean absolute error between target and predictions. After computing the absolute distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(abs(target - preds), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = jm.losses.mean_absolute_error(target, preds) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.abs(target - preds), axis=-1)) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean absolute error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" target = target . astype ( preds . dtype ) return jnp . mean ( jnp . abs ( preds - target ), axis =- 1 )","title":"mean_absolute_error"},{"location":"api/losses/mean_absolute_error/#jax_metricslossesmean_absolute_error","text":"","title":"jax_metrics.losses.mean_absolute_error"},{"location":"api/losses/mean_absolute_error/#jax_metrics.losses.mean_absolute_error.MeanAbsoluteError","text":"Computes the mean absolute errors between target and predictions. loss = mean(abs(target - preds)) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mae = jm . losses . MeanAbsoluteError () assert mae ( target , preds ) == 0.5 # Calling with 'sample_weight'. assert mae ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mae = jm . losses . MeanAbsoluteError ( reduction = jm . losses . Reduction . SUM ) assert mae ( target , preds ) == 1.0 # Using 'none' reduction type. mae = jm . losses . MeanAbsoluteError ( reduction = jm . losses . Reduction . NONE ) assert list ( mae ( target , preds )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanAbsoluteError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_absolute_error.py class MeanAbsoluteError ( Loss ): \"\"\" Computes the mean absolute errors between target and predictions. `loss = mean(abs(target - preds))` Usage: ```python target = jnp.array([[0.0, 1.0], [0.0, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. mae = jm.losses.MeanAbsoluteError() assert mae(target, preds) == 0.5 # Calling with 'sample_weight'. assert mae(target, preds, sample_weight=jnp.array([0.7, 0.3])) == 0.25 # Using 'sum' reduction type. mae = jm.losses.MeanAbsoluteError(reduction=jm.losses.Reduction.SUM) assert mae(target, preds) == 1.0 # Using 'none' reduction type. mae = jm.losses.MeanAbsoluteError(reduction=jm.losses.Reduction.NONE) assert list(mae(target, preds)) == [0.5, 0.5] ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanAbsoluteError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsoluteError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_error ( target , preds )","title":"MeanAbsoluteError"},{"location":"api/losses/mean_absolute_error/#jax_metrics.losses.mean_absolute_error.MeanAbsoluteError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/mean_absolute_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs )","title":"__init__()"},{"location":"api/losses/mean_absolute_error/#jax_metrics.losses.mean_absolute_error.MeanAbsoluteError.call","text":"Invokes the MeanAbsoluteError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_absolute_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsoluteError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_error ( target , preds )","title":"call()"},{"location":"api/losses/mean_absolute_error/#jax_metrics.losses.mean_absolute_error.mean_absolute_error","text":"Computes the mean absolute error between target and predictions. After computing the absolute distance between the inputs, the mean value over the last dimension is returned. loss = mean ( abs ( target - preds ), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = jm . losses . mean_absolute_error ( target , preds ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , jnp . mean ( jnp . abs ( target - preds ), axis =- 1 )) Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean absolute error values. shape = [batch_size, d0, .. dN-1] . Source code in jax_metrics/losses/mean_absolute_error.py def mean_absolute_error ( target : jnp . ndarray , preds : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean absolute error between target and predictions. After computing the absolute distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(abs(target - preds), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = jm.losses.mean_absolute_error(target, preds) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.abs(target - preds), axis=-1)) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean absolute error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" target = target . astype ( preds . dtype ) return jnp . mean ( jnp . abs ( preds - target ), axis =- 1 )","title":"mean_absolute_error()"},{"location":"api/losses/mean_absolute_percentage_error/","text":"jax_metrics.losses.mean_absolute_percentage_error MeanAbsolutePercentageError ( Loss ) Computes the mean absolute errors between target and predictions. loss = mean(abs((target - preds) / target)) Usage: target = jnp . array ([[ 1.0 , 1.0 ], [ 0.9 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mape = jm . losses . MeanAbsolutePercentageError () result = mape ( target , preds ) assert np . isclose ( result , 2.78 , rtol = 0.01 ) # Calling with 'sample_weight'. assert np . isclose ( mape ( target , preds , sample_weight = jnp . array ([ 0.1 , 0.9 ])), 2.5 , rtol = 0.01 ) # Using 'sum' reduction type. mape = jm . losses . MeanAbsolutePercentageError ( reduction = jm . losses . Reduction . SUM ) assert np . isclose ( mape ( target , preds ), 5.6 , rtol = 0.01 ) # Using 'none' reduction type. mape = jm . losses . MeanAbsolutePercentageError ( reduction = jm . losses . Reduction . NONE ) assert jnp . all ( np . isclose ( result , [ 0. , 5.6 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanAbsolutePercentageError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_absolute_percentage_error.py class MeanAbsolutePercentageError ( Loss ): \"\"\" Computes the mean absolute errors between target and predictions. `loss = mean(abs((target - preds) / target))` Usage: ```python target = jnp.array([[1.0, 1.0], [0.9, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. mape = jm.losses.MeanAbsolutePercentageError() result = mape(target, preds) assert np.isclose(result, 2.78, rtol=0.01) # Calling with 'sample_weight'. assert np.isclose(mape(target, preds, sample_weight=jnp.array([0.1, 0.9])), 2.5, rtol=0.01) # Using 'sum' reduction type. mape = jm.losses.MeanAbsolutePercentageError(reduction=jm.losses.Reduction.SUM) assert np.isclose(mape(target, preds), 5.6, rtol=0.01) # Using 'none' reduction type. mape = jm.losses.MeanAbsolutePercentageError(reduction=jm.losses.Reduction.NONE) assert jnp.all(np.isclose(result, [0. , 5.6], rtol=0.01)) ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanAbsolutePercentageError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. for more details. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsolutePercentageError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_percentage_error ( target , preds ) __init__ ( self , reduction = None , weight = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. for more details. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/mean_absolute_percentage_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. for more details. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) call ( self , target , preds , sample_weight = None , ** _ ) Invokes the MeanAbsolutePercentageError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_absolute_percentage_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsolutePercentageError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_percentage_error ( target , preds ) mean_absolute_percentage_error ( target , preds ) Computes the mean absolute percentage error (MAPE) between target and predictions. After computing the absolute distance between the true value and the prediction value and divide by the true value, the mean value over the last dimension is returned. Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = jm . losses . mean_absolute_percentage_error ( target , preds ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , 100. * jnp . mean ( jnp . abs (( preds - target ) / jnp . clip ( target , types . EPSILON , None )))) Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean absolute percentage error values. shape = [batch_size, d0, .. dN-1] . Source code in jax_metrics/losses/mean_absolute_percentage_error.py def mean_absolute_percentage_error ( target : jnp . ndarray , preds : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean absolute percentage error (MAPE) between target and predictions. After computing the absolute distance between the true value and the prediction value and divide by the true value, the mean value over the last dimension is returned. Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = jm.losses.mean_absolute_percentage_error(target, preds) assert loss.shape == (2,) assert jnp.array_equal(loss, 100. * jnp.mean(jnp.abs((preds - target) / jnp.clip(target, types.EPSILON, None)))) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean absolute percentage error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" target = target . astype ( preds . dtype ) diff = jnp . abs (( preds - target ) / jnp . maximum ( jnp . abs ( target ), types . EPSILON )) return 100.0 * jnp . mean ( diff , axis =- 1 )","title":"mean_absolute_percentage_error"},{"location":"api/losses/mean_absolute_percentage_error/#jax_metricslossesmean_absolute_percentage_error","text":"","title":"jax_metrics.losses.mean_absolute_percentage_error"},{"location":"api/losses/mean_absolute_percentage_error/#jax_metrics.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError","text":"Computes the mean absolute errors between target and predictions. loss = mean(abs((target - preds) / target)) Usage: target = jnp . array ([[ 1.0 , 1.0 ], [ 0.9 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mape = jm . losses . MeanAbsolutePercentageError () result = mape ( target , preds ) assert np . isclose ( result , 2.78 , rtol = 0.01 ) # Calling with 'sample_weight'. assert np . isclose ( mape ( target , preds , sample_weight = jnp . array ([ 0.1 , 0.9 ])), 2.5 , rtol = 0.01 ) # Using 'sum' reduction type. mape = jm . losses . MeanAbsolutePercentageError ( reduction = jm . losses . Reduction . SUM ) assert np . isclose ( mape ( target , preds ), 5.6 , rtol = 0.01 ) # Using 'none' reduction type. mape = jm . losses . MeanAbsolutePercentageError ( reduction = jm . losses . Reduction . NONE ) assert jnp . all ( np . isclose ( result , [ 0. , 5.6 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanAbsolutePercentageError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_absolute_percentage_error.py class MeanAbsolutePercentageError ( Loss ): \"\"\" Computes the mean absolute errors between target and predictions. `loss = mean(abs((target - preds) / target))` Usage: ```python target = jnp.array([[1.0, 1.0], [0.9, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. mape = jm.losses.MeanAbsolutePercentageError() result = mape(target, preds) assert np.isclose(result, 2.78, rtol=0.01) # Calling with 'sample_weight'. assert np.isclose(mape(target, preds, sample_weight=jnp.array([0.1, 0.9])), 2.5, rtol=0.01) # Using 'sum' reduction type. mape = jm.losses.MeanAbsolutePercentageError(reduction=jm.losses.Reduction.SUM) assert np.isclose(mape(target, preds), 5.6, rtol=0.01) # Using 'none' reduction type. mape = jm.losses.MeanAbsolutePercentageError(reduction=jm.losses.Reduction.NONE) assert jnp.all(np.isclose(result, [0. , 5.6], rtol=0.01)) ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanAbsolutePercentageError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. for more details. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsolutePercentageError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_percentage_error ( target , preds )","title":"MeanAbsolutePercentageError"},{"location":"api/losses/mean_absolute_percentage_error/#jax_metrics.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. for more details. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/mean_absolute_percentage_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. for more details. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs )","title":"__init__()"},{"location":"api/losses/mean_absolute_percentage_error/#jax_metrics.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError.call","text":"Invokes the MeanAbsolutePercentageError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_absolute_percentage_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsolutePercentageError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_percentage_error ( target , preds )","title":"call()"},{"location":"api/losses/mean_absolute_percentage_error/#jax_metrics.losses.mean_absolute_percentage_error.mean_absolute_percentage_error","text":"Computes the mean absolute percentage error (MAPE) between target and predictions. After computing the absolute distance between the true value and the prediction value and divide by the true value, the mean value over the last dimension is returned. Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = jm . losses . mean_absolute_percentage_error ( target , preds ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , 100. * jnp . mean ( jnp . abs (( preds - target ) / jnp . clip ( target , types . EPSILON , None )))) Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean absolute percentage error values. shape = [batch_size, d0, .. dN-1] . Source code in jax_metrics/losses/mean_absolute_percentage_error.py def mean_absolute_percentage_error ( target : jnp . ndarray , preds : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean absolute percentage error (MAPE) between target and predictions. After computing the absolute distance between the true value and the prediction value and divide by the true value, the mean value over the last dimension is returned. Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = jm.losses.mean_absolute_percentage_error(target, preds) assert loss.shape == (2,) assert jnp.array_equal(loss, 100. * jnp.mean(jnp.abs((preds - target) / jnp.clip(target, types.EPSILON, None)))) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean absolute percentage error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" target = target . astype ( preds . dtype ) diff = jnp . abs (( preds - target ) / jnp . maximum ( jnp . abs ( target ), types . EPSILON )) return 100.0 * jnp . mean ( diff , axis =- 1 )","title":"mean_absolute_percentage_error()"},{"location":"api/losses/mean_squared_error/","text":"jax_metrics.losses.mean_squared_error MeanSquaredError ( Loss ) Computes the mean of squares of errors between target and predictions. loss = square(target - preds) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = jm . losses . MeanSquaredError () assert mse ( target , preds ) == 0.5 # Calling with 'sample_weight'. assert mse ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mse = jm . losses . MeanSquaredError ( reduction = jm . losses . Reduction . SUM ) assert mse ( target , preds ) == 1.0 # Using 'none' reduction type. mse = jm . losses . MeanSquaredError ( reduction = jm . losses . Reduction . NONE ) assert list ( mse ( target , preds )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanSquaredError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_squared_error.py class MeanSquaredError ( Loss ): \"\"\" Computes the mean of squares of errors between target and predictions. `loss = square(target - preds)` Usage: ```python target = jnp.array([[0.0, 1.0], [0.0, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = jm.losses.MeanSquaredError() assert mse(target, preds) == 0.5 # Calling with 'sample_weight'. assert mse(target, preds, sample_weight=jnp.array([0.7, 0.3])) == 0.25 # Using 'sum' reduction type. mse = jm.losses.MeanSquaredError(reduction=jm.losses.Reduction.SUM) assert mse(target, preds) == 1.0 # Using 'none' reduction type. mse = jm.losses.MeanSquaredError(reduction=jm.losses.Reduction.NONE) assert list(mse(target, preds)) == [0.5, 0.5] ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanSquaredError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , name = name ) def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( target , preds ) __init__ ( self , reduction = None , weight = None , name = None ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/mean_squared_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , name = name ) call ( self , target , preds , sample_weight = None , ** _ ) Invokes the MeanSquaredError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_squared_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( target , preds ) mean_squared_error ( target , preds ) Computes the mean squared error between target and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean ( square ( target - preds ), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = jm . losses . mean_squared_error ( target , preds ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , jnp . mean ( jnp . square ( target - preds ), axis =- 1 )) Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean squared error values. shape = [batch_size, d0, .. dN-1] . Source code in jax_metrics/losses/mean_squared_error.py def mean_squared_error ( target : jnp . ndarray , preds : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean squared error between target and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(square(target - preds), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = jm.losses.mean_squared_error(target, preds) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.square(target - preds), axis=-1)) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" target = target . astype ( preds . dtype ) return jnp . mean ( jnp . square ( preds - target ), axis =- 1 )","title":"mean_squared_error"},{"location":"api/losses/mean_squared_error/#jax_metricslossesmean_squared_error","text":"","title":"jax_metrics.losses.mean_squared_error"},{"location":"api/losses/mean_squared_error/#jax_metrics.losses.mean_squared_error.MeanSquaredError","text":"Computes the mean of squares of errors between target and predictions. loss = square(target - preds) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = jm . losses . MeanSquaredError () assert mse ( target , preds ) == 0.5 # Calling with 'sample_weight'. assert mse ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mse = jm . losses . MeanSquaredError ( reduction = jm . losses . Reduction . SUM ) assert mse ( target , preds ) == 1.0 # Using 'none' reduction type. mse = jm . losses . MeanSquaredError ( reduction = jm . losses . Reduction . NONE ) assert list ( mse ( target , preds )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanSquaredError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_squared_error.py class MeanSquaredError ( Loss ): \"\"\" Computes the mean of squares of errors between target and predictions. `loss = square(target - preds)` Usage: ```python target = jnp.array([[0.0, 1.0], [0.0, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = jm.losses.MeanSquaredError() assert mse(target, preds) == 0.5 # Calling with 'sample_weight'. assert mse(target, preds, sample_weight=jnp.array([0.7, 0.3])) == 0.25 # Using 'sum' reduction type. mse = jm.losses.MeanSquaredError(reduction=jm.losses.Reduction.SUM) assert mse(target, preds) == 1.0 # Using 'none' reduction type. mse = jm.losses.MeanSquaredError(reduction=jm.losses.Reduction.NONE) assert list(mse(target, preds)) == [0.5, 0.5] ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanSquaredError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , name = name ) def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( target , preds )","title":"MeanSquaredError"},{"location":"api/losses/mean_squared_error/#jax_metrics.losses.mean_squared_error.MeanSquaredError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/mean_squared_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , name = name )","title":"__init__()"},{"location":"api/losses/mean_squared_error/#jax_metrics.losses.mean_squared_error.MeanSquaredError.call","text":"Invokes the MeanSquaredError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_squared_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( target , preds )","title":"call()"},{"location":"api/losses/mean_squared_error/#jax_metrics.losses.mean_squared_error.mean_squared_error","text":"Computes the mean squared error between target and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean ( square ( target - preds ), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = jm . losses . mean_squared_error ( target , preds ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , jnp . mean ( jnp . square ( target - preds ), axis =- 1 )) Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean squared error values. shape = [batch_size, d0, .. dN-1] . Source code in jax_metrics/losses/mean_squared_error.py def mean_squared_error ( target : jnp . ndarray , preds : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean squared error between target and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(square(target - preds), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = jm.losses.mean_squared_error(target, preds) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.square(target - preds), axis=-1)) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" target = target . astype ( preds . dtype ) return jnp . mean ( jnp . square ( preds - target ), axis =- 1 )","title":"mean_squared_error()"},{"location":"api/losses/mean_squared_logarithmic_error/","text":"jax_metrics.losses.mean_squared_logarithmic_error MeanSquaredLogarithmicError ( Loss ) Computes the mean squared logarithmic errors between target and predictions. loss = mean(square(log(target + 1) - log(preds + 1)), axis=-1) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. msle = jm . losses . MeanSquaredLogarithmicError () assert msle ( target , preds ) == 0.24022643 # Calling with 'sample_weight'. assert msle ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) = 0.12011322 # Using 'sum' reduction type. msle = jm . losses . MeanSquaredLogarithmicError ( reduction = jm . losses . Reduction . SUM ) assert msle ( target , preds ) == 0.48045287 # Using 'none' reduction type. msle = jm . losses . MeanSquaredLogarithmicError ( reduction = jm . losses . Reduction . NONE ) assert jnp . equal ( msle ( target , preds ), jnp . array ([ 0.24022643 , 0.24022643 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanSquaredLogarithmicError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_squared_logarithmic_error.py class MeanSquaredLogarithmicError ( Loss ): \"\"\" Computes the mean squared logarithmic errors between target and predictions. `loss = mean(square(log(target + 1) - log(preds + 1)), axis=-1)` Usage: ```python target = jnp.array([[0.0, 1.0], [0.0, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. msle = jm.losses.MeanSquaredLogarithmicError() assert msle(target, preds) == 0.24022643 # Calling with 'sample_weight'. assert msle(target, preds, sample_weight=jnp.array([0.7, 0.3])) = 0.12011322 # Using 'sum' reduction type. msle = jm.losses.MeanSquaredLogarithmicError(reduction=jm.losses.Reduction.SUM) assert msle(target, preds) == 0.48045287 # Using 'none' reduction type. msle = jm.losses.MeanSquaredLogarithmicError(reduction=jm.losses.Reduction.NONE) assert jnp.equal(msle(target, preds), jnp.array([0.24022643, 0.24022643])).all() ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanSquaredLogarithmicError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredLogarithmicError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_logarithmic_error ( target , preds ) __init__ ( self , reduction = None , weight = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/mean_squared_logarithmic_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) call ( self , target , preds , sample_weight = None , ** _ ) Invokes the MeanSquaredLogarithmicError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_squared_logarithmic_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredLogarithmicError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_logarithmic_error ( target , preds ) mean_squared_logarithmic_error ( target , preds ) Computes the mean squared logarithmic error between target and predictions. loss = mean ( square ( log ( target + 1 ) - log ( preds + 1 )), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = jm . losses . mean_squared_logarithmic_error ( target , preds ) assert loss . shape == ( 2 ,) first_log = jnp . log ( jnp . maximum ( target , types . EPSILON ) + 1.0 ) second_log = jnp . log ( jnp . maximum ( preds , types . EPSILON ) + 1.0 ) assert jnp . array_equal ( loss , jnp . mean ( jnp . square ( first_log - second_log ), axis =- 1 )) Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean squared logarithmic error values. shape = [batch_size, d0, .. dN-1] . Source code in jax_metrics/losses/mean_squared_logarithmic_error.py def mean_squared_logarithmic_error ( target : jnp . ndarray , preds : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean squared logarithmic error between target and predictions. ```python loss = mean(square(log(target + 1) - log(preds + 1)), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = jm.losses.mean_squared_logarithmic_error(target, preds) assert loss.shape == (2,) first_log = jnp.log(jnp.maximum(target, types.EPSILON) + 1.0) second_log = jnp.log(jnp.maximum(preds, types.EPSILON) + 1.0) assert jnp.array_equal(loss, jnp.mean(jnp.square(first_log - second_log), axis=-1)) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared logarithmic error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" target = target . astype ( preds . dtype ) first_log = jnp . log ( jnp . maximum ( target , types . EPSILON ) + 1.0 ) second_log = jnp . log ( jnp . maximum ( preds , types . EPSILON ) + 1.0 ) return jnp . mean ( jnp . square ( first_log - second_log ), axis =- 1 )","title":"mean_squared_logarithmic_error"},{"location":"api/losses/mean_squared_logarithmic_error/#jax_metricslossesmean_squared_logarithmic_error","text":"","title":"jax_metrics.losses.mean_squared_logarithmic_error"},{"location":"api/losses/mean_squared_logarithmic_error/#jax_metrics.losses.mean_squared_logarithmic_error.MeanSquaredLogarithmicError","text":"Computes the mean squared logarithmic errors between target and predictions. loss = mean(square(log(target + 1) - log(preds + 1)), axis=-1) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. msle = jm . losses . MeanSquaredLogarithmicError () assert msle ( target , preds ) == 0.24022643 # Calling with 'sample_weight'. assert msle ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) = 0.12011322 # Using 'sum' reduction type. msle = jm . losses . MeanSquaredLogarithmicError ( reduction = jm . losses . Reduction . SUM ) assert msle ( target , preds ) == 0.48045287 # Using 'none' reduction type. msle = jm . losses . MeanSquaredLogarithmicError ( reduction = jm . losses . Reduction . NONE ) assert jnp . equal ( msle ( target , preds ), jnp . array ([ 0.24022643 , 0.24022643 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanSquaredLogarithmicError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_squared_logarithmic_error.py class MeanSquaredLogarithmicError ( Loss ): \"\"\" Computes the mean squared logarithmic errors between target and predictions. `loss = mean(square(log(target + 1) - log(preds + 1)), axis=-1)` Usage: ```python target = jnp.array([[0.0, 1.0], [0.0, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. msle = jm.losses.MeanSquaredLogarithmicError() assert msle(target, preds) == 0.24022643 # Calling with 'sample_weight'. assert msle(target, preds, sample_weight=jnp.array([0.7, 0.3])) = 0.12011322 # Using 'sum' reduction type. msle = jm.losses.MeanSquaredLogarithmicError(reduction=jm.losses.Reduction.SUM) assert msle(target, preds) == 0.48045287 # Using 'none' reduction type. msle = jm.losses.MeanSquaredLogarithmicError(reduction=jm.losses.Reduction.NONE) assert jnp.equal(msle(target, preds), jnp.array([0.24022643, 0.24022643])).all() ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanSquaredLogarithmicError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs ) def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredLogarithmicError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_logarithmic_error ( target , preds )","title":"MeanSquaredLogarithmicError"},{"location":"api/losses/mean_squared_logarithmic_error/#jax_metrics.losses.mean_squared_logarithmic_error.MeanSquaredLogarithmicError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[jax_metrics.losses.loss.Reduction] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/mean_squared_logarithmic_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ** kwargs , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , ** kwargs )","title":"__init__()"},{"location":"api/losses/mean_squared_logarithmic_error/#jax_metrics.losses.mean_squared_logarithmic_error.MeanSquaredLogarithmicError.call","text":"Invokes the MeanSquaredLogarithmicError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_squared_logarithmic_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredLogarithmicError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_logarithmic_error ( target , preds )","title":"call()"},{"location":"api/losses/mean_squared_logarithmic_error/#jax_metrics.losses.mean_squared_logarithmic_error.mean_squared_logarithmic_error","text":"Computes the mean squared logarithmic error between target and predictions. loss = mean ( square ( log ( target + 1 ) - log ( preds + 1 )), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = jm . losses . mean_squared_logarithmic_error ( target , preds ) assert loss . shape == ( 2 ,) first_log = jnp . log ( jnp . maximum ( target , types . EPSILON ) + 1.0 ) second_log = jnp . log ( jnp . maximum ( preds , types . EPSILON ) + 1.0 ) assert jnp . array_equal ( loss , jnp . mean ( jnp . square ( first_log - second_log ), axis =- 1 )) Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean squared logarithmic error values. shape = [batch_size, d0, .. dN-1] . Source code in jax_metrics/losses/mean_squared_logarithmic_error.py def mean_squared_logarithmic_error ( target : jnp . ndarray , preds : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean squared logarithmic error between target and predictions. ```python loss = mean(square(log(target + 1) - log(preds + 1)), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = jm.losses.mean_squared_logarithmic_error(target, preds) assert loss.shape == (2,) first_log = jnp.log(jnp.maximum(target, types.EPSILON) + 1.0) second_log = jnp.log(jnp.maximum(preds, types.EPSILON) + 1.0) assert jnp.array_equal(loss, jnp.mean(jnp.square(first_log - second_log), axis=-1)) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared logarithmic error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" target = target . astype ( preds . dtype ) first_log = jnp . log ( jnp . maximum ( target , types . EPSILON ) + 1.0 ) second_log = jnp . log ( jnp . maximum ( preds , types . EPSILON ) + 1.0 ) return jnp . mean ( jnp . square ( first_log - second_log ), axis =- 1 )","title":"mean_squared_logarithmic_error()"},{"location":"api/metrics/Accuracy/","text":"jax_metrics.metrics.Accuracy Computes Accuracy, ported from torchmetrics . .. math:: \\text{Accuracy} = \\frac{1}{N}\\sum_i^N 1(y_i = \\hat{y}_i) Where :math: y is a tensor of target values, and :math: \\hat{y} is a tensor of predictions. For multi-class and multi-dimensional multi-class data with probability or logits predictions, the parameter top_k generalizes this metric to a Top-K accuracy metric: for each sample the top-K highest probability or logit score items are considered to find the correct label. For multi-label and multi-dimensional multi-class inputs, this metric computes the \"glob\" accuracy by default, which counts all target or sub-samples separately. This can be changed to subset accuracy (which requires all target or sub-samples in the sample to be correctly predicted) by setting subset_accuracy=True . Parameters: Name Type Description Default num_classes Optional[int] Number of classes. Necessary for 'macro' , 'weighted' and None average methods. None threshold float Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities. 0.5 average Union[str, jax_metrics.metrics.utils.AverageMethod] Defines the reduction that is applied. Should be one of the following: 'micro' [default]: Calculate the metric globally, across all samples and classes. 'macro' : Calculate the metric for each class separately, and average the metrics across classes (with equal weights for each class). 'weighted' : Calculate the metric for each class separately, and average the metrics across classes, weighting each class by its support ( tp + fn ). 'none' or None : Calculate the metric for each class separately, and return the metric for every class. 'samples' : Calculate the metric for each sample, and average the metrics across samples (with equal weights for each sample). .. note:: What is considered a sample in the multi-dimensional multi-class case depends on the value of mdmc_average . .. note:: If 'none' and a given class doesn't occur in the preds or target , the value for the class will be nan . <AverageMethod.MICRO: 1> mdmc_average Union[str, jax_metrics.metrics.utils.MDMCAverageMethod] Defines how averaging is done for multi-dimensional multi-class inputs (on top of the average parameter). Should be one of the following: None [default]: Should be left unchanged if your data is not multi-dimensional multi-class. 'samplewise' : In this case, the statistics are computed separately for each sample on the N axis, and then averaged over samples. The computation for each sample is done by treating the flattened extra axes ... (see :ref: references/modules:input types ) as the N dimension within the sample, and computing the metric for the sample based on that. 'global' : In this case the N and ... dimensions of the inputs (see :ref: references/modules:input types ) are flattened into a new N_X sample axis, i.e. the inputs are treated as if they were (N_X, C) . From here on the average parameter applies as usual. <MDMCAverageMethod.GLOBAL: 1> ignore_index Optional[int] Integer specifying a target class to ignore. If given, this class index does not contribute to the returned score, regardless of reduction method. If an index is ignored, and average=None or 'none' , the score for the ignored class will be returned as nan . None top_k Optional[int] Number of highest probability or logit score predictions considered to find the correct label, relevant only for (multi-dimensional) multi-class inputs. The default value ( None ) will be interpreted as 1 for these inputs. Should be left at default ( None ) for all other types of inputs. None multiclass Optional[bool] Used only in certain special cases, where you want to treat inputs as a different type than what they appear to be. See the parameter's :ref: documentation section <references/modules:using the multiclass parameter> for a more detailed explanation and examples. None subset_accuracy bool Whether to compute subset accuracy for multi-label and multi-dimensional multi-class inputs (has no effect for other input types). For multi-label inputs, if the parameter is set to True , then all target for each sample must be correctly predicted for the sample to count as correct. If it is set to False , then all target are counted separately - this is equivalent to flattening inputs beforehand (i.e. preds = preds.flatten() and same for target ). For multi-dimensional multi-class inputs, if the parameter is set to True , then all sub-sample (on the extra axis) must be correct for the sample to be counted as correct. If it is set to False , then all sub-samples are counter separately - this is equivalent, in the case of label predictions, to flattening the inputs beforehand (i.e. preds = preds.flatten() and same for target ). Note that the top_k parameter still applies in both cases, if set. False Source code in jax_metrics/metrics/accuracy.py class Accuracy ( Metric ): r \"\"\" Computes Accuracy, ported from [torchmetrics](https://github.com/PytorchLightning/metrics). .. math:: \\text{Accuracy} = \\frac{1}{N}\\sum_i^N 1(y_i = \\hat{y}_i) Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. For multi-class and multi-dimensional multi-class data with probability or logits predictions, the parameter `top_k` generalizes this metric to a Top-K accuracy metric: for each sample the top-K highest probability or logit score items are considered to find the correct label. For multi-label and multi-dimensional multi-class inputs, this metric computes the \"glob\" accuracy by default, which counts all target or sub-samples separately. This can be changed to subset accuracy (which requires all target or sub-samples in the sample to be correctly predicted) by setting `subset_accuracy=True`. Arguments: num_classes: Number of classes. Necessary for `'macro'`, `'weighted'` and `None` average methods. threshold: Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities. average: Defines the reduction that is applied. Should be one of the following: - `'micro'` [default]: Calculate the metric globally, across all samples and classes. - `'macro'`: Calculate the metric for each class separately, and average the metrics across classes (with equal weights for each class). - `'weighted'`: Calculate the metric for each class separately, and average the metrics across classes, weighting each class by its support (`tp + fn`). - `'none'` or `None`: Calculate the metric for each class separately, and return the metric for every class. - `'samples'`: Calculate the metric for each sample, and average the metrics across samples (with equal weights for each sample). .. note:: What is considered a sample in the multi-dimensional multi-class case depends on the value of `mdmc_average`. .. note:: If `'none'` and a given class doesn't occur in the `preds` or `target`, the value for the class will be `nan`. mdmc_average: Defines how averaging is done for multi-dimensional multi-class inputs (on top of the `average` parameter). Should be one of the following: - `None` [default]: Should be left unchanged if your data is not multi-dimensional multi-class. - `'samplewise'`: In this case, the statistics are computed separately for each sample on the `N` axis, and then averaged over samples. The computation for each sample is done by treating the flattened extra axes `...` (see :ref:`references/modules:input types`) as the `N` dimension within the sample, and computing the metric for the sample based on that. - `'global'`: In this case the `N` and `...` dimensions of the inputs (see :ref:`references/modules:input types`) are flattened into a new `N_X` sample axis, i.e. the inputs are treated as if they were `(N_X, C)`. From here on the `average` parameter applies as usual. ignore_index: Integer specifying a target class to ignore. If given, this class index does not contribute to the returned score, regardless of reduction method. If an index is ignored, and `average=None` or `'none'`, the score for the ignored class will be returned as `nan`. top_k: Number of highest probability or logit score predictions considered to find the correct label, relevant only for (multi-dimensional) multi-class inputs. The default value (`None`) will be interpreted as 1 for these inputs. Should be left at default (`None`) for all other types of inputs. multiclass: Used only in certain special cases, where you want to treat inputs as a different type than what they appear to be. See the parameter's :ref:`documentation section <references/modules:using the multiclass parameter>` for a more detailed explanation and examples. subset_accuracy: Whether to compute subset accuracy for multi-label and multi-dimensional multi-class inputs (has no effect for other input types). - For multi-label inputs, if the parameter is set to `True`, then all target for each sample must be correctly predicted for the sample to count as correct. If it is set to `False`, then all target are counted separately - this is equivalent to flattening inputs beforehand (i.e. `preds = preds.flatten()` and same for `target`). - For multi-dimensional multi-class inputs, if the parameter is set to `True`, then all sub-sample (on the extra axis) must be correct for the sample to be counted as correct. If it is set to `False`, then all sub-samples are counter separately - this is equivalent, in the case of label predictions, to flattening the inputs beforehand (i.e. `preds = preds.flatten()` and same for `target`). Note that the `top_k` parameter still applies in both cases, if set. Raises: ValueError: If `top_k` is not an `integer` larger than `0`. ValueError: If `average` is none of `\"micro\"`, `\"macro\"`, `\"weighted\"`, `\"samples\"`, `\"none\"`, `None`. ValueError: If two different input modes are provided, eg. using `multi-label` with `multi-class`. ValueError: If `top_k` parameter is set for `multi-label` inputs. \"\"\" tp : typing . Optional [ jnp . ndarray ] = to . node () fp : typing . Optional [ jnp . ndarray ] = to . node () tn : typing . Optional [ jnp . ndarray ] = to . node () fn : typing . Optional [ jnp . ndarray ] = to . node () def __init__ ( self , threshold : float = 0.5 , num_classes : typing . Optional [ int ] = None , average : typing . Union [ str , AverageMethod ] = AverageMethod . MICRO , mdmc_average : typing . Union [ str , MDMCAverageMethod ] = MDMCAverageMethod . GLOBAL , ignore_index : typing . Optional [ int ] = None , top_k : typing . Optional [ int ] = None , multiclass : typing . Optional [ bool ] = None , subset_accuracy : bool = False , # compute_on_step: bool = True, # dist_sync_on_step: bool = False, # process_group: typing.Optional[typing.Any] = None, # dist_sync_fn: typing.Callable = None, mode : DataType = DataType . MULTICLASS , name : typing . Optional [ str ] = None , dtype : typing . Optional [ jnp . dtype ] = None , ): super () . __init__ ( name = name , dtype = dtype ) if isinstance ( average , str ): average = AverageMethod [ average . upper ()] if isinstance ( mdmc_average , str ): mdmc_average = MDMCAverageMethod [ mdmc_average . upper ()] average = ( AverageMethod . MACRO if average in [ AverageMethod . WEIGHTED , AverageMethod . NONE ] else average ) if average not in [ AverageMethod . MICRO , AverageMethod . MACRO , # AverageMethod.SAMPLES, ]: raise ValueError ( f \"The `reduce` { average } is not valid.\" ) if average == AverageMethod . MACRO and ( not num_classes or num_classes < 1 ): raise ValueError ( \"When you set `reduce` as 'macro', you have to provide the number of classes.\" ) if top_k is not None and ( not isinstance ( top_k , int ) or top_k <= 0 ): raise ValueError ( f \"The `top_k` should be an integer larger than 0, got { top_k } \" ) if ( num_classes and ignore_index is not None and ( not 0 <= ignore_index < num_classes or num_classes == 1 ) ): raise ValueError ( f \"The `ignore_index` { ignore_index } is not valid for inputs with { num_classes } classes\" ) # Update states if average == AverageMethod . SAMPLES : raise ValueError ( f \"The `average` method ' { average } ' is not yet supported.\" ) if mdmc_average == MDMCAverageMethod . SAMPLEWISE : raise ValueError ( f \"The `mdmc_average` method ' { mdmc_average } ' is not yet supported.\" ) self . average = average self . mdmc_average = mdmc_average self . num_classes = num_classes self . threshold = threshold self . multiclass = multiclass self . ignore_index = ignore_index self . top_k = top_k self . subset_accuracy = subset_accuracy self . mode = mode self . tp = None self . fp = None self . tn = None self . fn = None def reset ( self ) -> \"Accuracy\" : # nodes if self . average == AverageMethod . MICRO : zeros_shape = [] elif self . average == AverageMethod . MACRO : zeros_shape = [ self . num_classes ] else : raise ValueError ( f 'Wrong reduce=\" { self . average } \"' ) initial_value = jnp . zeros ( zeros_shape , dtype = jnp . uint32 ) return self . replace ( tp = initial_value , fp = initial_value , tn = initial_value , fn = initial_value , ) def update ( self , preds : jnp . ndarray , target : jnp . ndarray , ** _ ) -> \"Accuracy\" : \"\"\"Updates Accuracy metric state. Example: ``python accuracy = Accuracy().reset() accuracy = accuracy.update(preds=preds, target=target) Args: preds: Predictions from model (logits, probabilities, or target) target: Ground truth target Returns: Updated Accuracy instance \"\"\" if self . tp is None or self . fp is None or self . tn is None or self . fn is None : raise ValueError ( \"Accuracy metric has not been initialized, call 'reset()' first.\" ) tp , fp , tn , fn = metric_utils . _stat_scores_update ( preds , target , intended_mode = self . mode , average_method = self . average , mdmc_average_method = self . mdmc_average , threshold = self . threshold , num_classes = self . num_classes , top_k = self . top_k , multiclass = self . multiclass , ) return self . replace ( tp = self . tp + tp , fp = self . fp + fp , tn = self . tn + tn , fn = self . fn + fn , ) def compute ( self ) -> jnp . ndarray : \"\"\" Computes accuracy based on inputs passed in to `update` previously. Returns: Accuracy score \"\"\" # if self.mode is None: # raise RuntimeError(\"You have to have determined mode.\") if self . tp is None or self . fp is None or self . tn is None or self . fn is None : raise ValueError ( \"Accuracy metric has not been initialized, call 'reset()' first.\" ) return metric_utils . _accuracy_compute ( self . tp , self . fp , self . tn , self . fn , self . average , self . mdmc_average , self . mode , ) compute ( self ) Computes accuracy based on inputs passed in to update previously. Returns: Type Description ndarray Accuracy score Source code in jax_metrics/metrics/accuracy.py def compute ( self ) -> jnp . ndarray : \"\"\" Computes accuracy based on inputs passed in to `update` previously. Returns: Accuracy score \"\"\" # if self.mode is None: # raise RuntimeError(\"You have to have determined mode.\") if self . tp is None or self . fp is None or self . tn is None or self . fn is None : raise ValueError ( \"Accuracy metric has not been initialized, call 'reset()' first.\" ) return metric_utils . _accuracy_compute ( self . tp , self . fp , self . tn , self . fn , self . average , self . mdmc_average , self . mode , ) reset ( self ) Resets the metric state. Returns: Type Description Accuracy Metric with the initial state Source code in jax_metrics/metrics/accuracy.py def reset ( self ) -> \"Accuracy\" : # nodes if self . average == AverageMethod . MICRO : zeros_shape = [] elif self . average == AverageMethod . MACRO : zeros_shape = [ self . num_classes ] else : raise ValueError ( f 'Wrong reduce=\" { self . average } \"' ) initial_value = jnp . zeros ( zeros_shape , dtype = jnp . uint32 ) return self . replace ( tp = initial_value , fp = initial_value , tn = initial_value , fn = initial_value , ) update ( self , preds , target , ** _ ) Updates Accuracy metric state. Examples: ``python accuracy = Accuracy().reset() accuracy = accuracy.update(preds=preds, target=target) Parameters: Name Type Description Default preds ndarray Predictions from model (logits, probabilities, or target) required target ndarray Ground truth target required Returns: Type Description Accuracy Updated Accuracy instance Source code in jax_metrics/metrics/accuracy.py def update ( self , preds : jnp . ndarray , target : jnp . ndarray , ** _ ) -> \"Accuracy\" : \"\"\"Updates Accuracy metric state. Example: ``python accuracy = Accuracy().reset() accuracy = accuracy.update(preds=preds, target=target) Args: preds: Predictions from model (logits, probabilities, or target) target: Ground truth target Returns: Updated Accuracy instance \"\"\" if self . tp is None or self . fp is None or self . tn is None or self . fn is None : raise ValueError ( \"Accuracy metric has not been initialized, call 'reset()' first.\" ) tp , fp , tn , fn = metric_utils . _stat_scores_update ( preds , target , intended_mode = self . mode , average_method = self . average , mdmc_average_method = self . mdmc_average , threshold = self . threshold , num_classes = self . num_classes , top_k = self . top_k , multiclass = self . multiclass , ) return self . replace ( tp = self . tp + tp , fp = self . fp + fp , tn = self . tn + tn , fn = self . fn + fn , )","title":"Accuracy"},{"location":"api/metrics/Accuracy/#jax_metricsmetricsaccuracy","text":"Computes Accuracy, ported from torchmetrics . .. math:: \\text{Accuracy} = \\frac{1}{N}\\sum_i^N 1(y_i = \\hat{y}_i) Where :math: y is a tensor of target values, and :math: \\hat{y} is a tensor of predictions. For multi-class and multi-dimensional multi-class data with probability or logits predictions, the parameter top_k generalizes this metric to a Top-K accuracy metric: for each sample the top-K highest probability or logit score items are considered to find the correct label. For multi-label and multi-dimensional multi-class inputs, this metric computes the \"glob\" accuracy by default, which counts all target or sub-samples separately. This can be changed to subset accuracy (which requires all target or sub-samples in the sample to be correctly predicted) by setting subset_accuracy=True . Parameters: Name Type Description Default num_classes Optional[int] Number of classes. Necessary for 'macro' , 'weighted' and None average methods. None threshold float Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities. 0.5 average Union[str, jax_metrics.metrics.utils.AverageMethod] Defines the reduction that is applied. Should be one of the following: 'micro' [default]: Calculate the metric globally, across all samples and classes. 'macro' : Calculate the metric for each class separately, and average the metrics across classes (with equal weights for each class). 'weighted' : Calculate the metric for each class separately, and average the metrics across classes, weighting each class by its support ( tp + fn ). 'none' or None : Calculate the metric for each class separately, and return the metric for every class. 'samples' : Calculate the metric for each sample, and average the metrics across samples (with equal weights for each sample). .. note:: What is considered a sample in the multi-dimensional multi-class case depends on the value of mdmc_average . .. note:: If 'none' and a given class doesn't occur in the preds or target , the value for the class will be nan . <AverageMethod.MICRO: 1> mdmc_average Union[str, jax_metrics.metrics.utils.MDMCAverageMethod] Defines how averaging is done for multi-dimensional multi-class inputs (on top of the average parameter). Should be one of the following: None [default]: Should be left unchanged if your data is not multi-dimensional multi-class. 'samplewise' : In this case, the statistics are computed separately for each sample on the N axis, and then averaged over samples. The computation for each sample is done by treating the flattened extra axes ... (see :ref: references/modules:input types ) as the N dimension within the sample, and computing the metric for the sample based on that. 'global' : In this case the N and ... dimensions of the inputs (see :ref: references/modules:input types ) are flattened into a new N_X sample axis, i.e. the inputs are treated as if they were (N_X, C) . From here on the average parameter applies as usual. <MDMCAverageMethod.GLOBAL: 1> ignore_index Optional[int] Integer specifying a target class to ignore. If given, this class index does not contribute to the returned score, regardless of reduction method. If an index is ignored, and average=None or 'none' , the score for the ignored class will be returned as nan . None top_k Optional[int] Number of highest probability or logit score predictions considered to find the correct label, relevant only for (multi-dimensional) multi-class inputs. The default value ( None ) will be interpreted as 1 for these inputs. Should be left at default ( None ) for all other types of inputs. None multiclass Optional[bool] Used only in certain special cases, where you want to treat inputs as a different type than what they appear to be. See the parameter's :ref: documentation section <references/modules:using the multiclass parameter> for a more detailed explanation and examples. None subset_accuracy bool Whether to compute subset accuracy for multi-label and multi-dimensional multi-class inputs (has no effect for other input types). For multi-label inputs, if the parameter is set to True , then all target for each sample must be correctly predicted for the sample to count as correct. If it is set to False , then all target are counted separately - this is equivalent to flattening inputs beforehand (i.e. preds = preds.flatten() and same for target ). For multi-dimensional multi-class inputs, if the parameter is set to True , then all sub-sample (on the extra axis) must be correct for the sample to be counted as correct. If it is set to False , then all sub-samples are counter separately - this is equivalent, in the case of label predictions, to flattening the inputs beforehand (i.e. preds = preds.flatten() and same for target ). Note that the top_k parameter still applies in both cases, if set. False Source code in jax_metrics/metrics/accuracy.py class Accuracy ( Metric ): r \"\"\" Computes Accuracy, ported from [torchmetrics](https://github.com/PytorchLightning/metrics). .. math:: \\text{Accuracy} = \\frac{1}{N}\\sum_i^N 1(y_i = \\hat{y}_i) Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. For multi-class and multi-dimensional multi-class data with probability or logits predictions, the parameter `top_k` generalizes this metric to a Top-K accuracy metric: for each sample the top-K highest probability or logit score items are considered to find the correct label. For multi-label and multi-dimensional multi-class inputs, this metric computes the \"glob\" accuracy by default, which counts all target or sub-samples separately. This can be changed to subset accuracy (which requires all target or sub-samples in the sample to be correctly predicted) by setting `subset_accuracy=True`. Arguments: num_classes: Number of classes. Necessary for `'macro'`, `'weighted'` and `None` average methods. threshold: Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities. average: Defines the reduction that is applied. Should be one of the following: - `'micro'` [default]: Calculate the metric globally, across all samples and classes. - `'macro'`: Calculate the metric for each class separately, and average the metrics across classes (with equal weights for each class). - `'weighted'`: Calculate the metric for each class separately, and average the metrics across classes, weighting each class by its support (`tp + fn`). - `'none'` or `None`: Calculate the metric for each class separately, and return the metric for every class. - `'samples'`: Calculate the metric for each sample, and average the metrics across samples (with equal weights for each sample). .. note:: What is considered a sample in the multi-dimensional multi-class case depends on the value of `mdmc_average`. .. note:: If `'none'` and a given class doesn't occur in the `preds` or `target`, the value for the class will be `nan`. mdmc_average: Defines how averaging is done for multi-dimensional multi-class inputs (on top of the `average` parameter). Should be one of the following: - `None` [default]: Should be left unchanged if your data is not multi-dimensional multi-class. - `'samplewise'`: In this case, the statistics are computed separately for each sample on the `N` axis, and then averaged over samples. The computation for each sample is done by treating the flattened extra axes `...` (see :ref:`references/modules:input types`) as the `N` dimension within the sample, and computing the metric for the sample based on that. - `'global'`: In this case the `N` and `...` dimensions of the inputs (see :ref:`references/modules:input types`) are flattened into a new `N_X` sample axis, i.e. the inputs are treated as if they were `(N_X, C)`. From here on the `average` parameter applies as usual. ignore_index: Integer specifying a target class to ignore. If given, this class index does not contribute to the returned score, regardless of reduction method. If an index is ignored, and `average=None` or `'none'`, the score for the ignored class will be returned as `nan`. top_k: Number of highest probability or logit score predictions considered to find the correct label, relevant only for (multi-dimensional) multi-class inputs. The default value (`None`) will be interpreted as 1 for these inputs. Should be left at default (`None`) for all other types of inputs. multiclass: Used only in certain special cases, where you want to treat inputs as a different type than what they appear to be. See the parameter's :ref:`documentation section <references/modules:using the multiclass parameter>` for a more detailed explanation and examples. subset_accuracy: Whether to compute subset accuracy for multi-label and multi-dimensional multi-class inputs (has no effect for other input types). - For multi-label inputs, if the parameter is set to `True`, then all target for each sample must be correctly predicted for the sample to count as correct. If it is set to `False`, then all target are counted separately - this is equivalent to flattening inputs beforehand (i.e. `preds = preds.flatten()` and same for `target`). - For multi-dimensional multi-class inputs, if the parameter is set to `True`, then all sub-sample (on the extra axis) must be correct for the sample to be counted as correct. If it is set to `False`, then all sub-samples are counter separately - this is equivalent, in the case of label predictions, to flattening the inputs beforehand (i.e. `preds = preds.flatten()` and same for `target`). Note that the `top_k` parameter still applies in both cases, if set. Raises: ValueError: If `top_k` is not an `integer` larger than `0`. ValueError: If `average` is none of `\"micro\"`, `\"macro\"`, `\"weighted\"`, `\"samples\"`, `\"none\"`, `None`. ValueError: If two different input modes are provided, eg. using `multi-label` with `multi-class`. ValueError: If `top_k` parameter is set for `multi-label` inputs. \"\"\" tp : typing . Optional [ jnp . ndarray ] = to . node () fp : typing . Optional [ jnp . ndarray ] = to . node () tn : typing . Optional [ jnp . ndarray ] = to . node () fn : typing . Optional [ jnp . ndarray ] = to . node () def __init__ ( self , threshold : float = 0.5 , num_classes : typing . Optional [ int ] = None , average : typing . Union [ str , AverageMethod ] = AverageMethod . MICRO , mdmc_average : typing . Union [ str , MDMCAverageMethod ] = MDMCAverageMethod . GLOBAL , ignore_index : typing . Optional [ int ] = None , top_k : typing . Optional [ int ] = None , multiclass : typing . Optional [ bool ] = None , subset_accuracy : bool = False , # compute_on_step: bool = True, # dist_sync_on_step: bool = False, # process_group: typing.Optional[typing.Any] = None, # dist_sync_fn: typing.Callable = None, mode : DataType = DataType . MULTICLASS , name : typing . Optional [ str ] = None , dtype : typing . Optional [ jnp . dtype ] = None , ): super () . __init__ ( name = name , dtype = dtype ) if isinstance ( average , str ): average = AverageMethod [ average . upper ()] if isinstance ( mdmc_average , str ): mdmc_average = MDMCAverageMethod [ mdmc_average . upper ()] average = ( AverageMethod . MACRO if average in [ AverageMethod . WEIGHTED , AverageMethod . NONE ] else average ) if average not in [ AverageMethod . MICRO , AverageMethod . MACRO , # AverageMethod.SAMPLES, ]: raise ValueError ( f \"The `reduce` { average } is not valid.\" ) if average == AverageMethod . MACRO and ( not num_classes or num_classes < 1 ): raise ValueError ( \"When you set `reduce` as 'macro', you have to provide the number of classes.\" ) if top_k is not None and ( not isinstance ( top_k , int ) or top_k <= 0 ): raise ValueError ( f \"The `top_k` should be an integer larger than 0, got { top_k } \" ) if ( num_classes and ignore_index is not None and ( not 0 <= ignore_index < num_classes or num_classes == 1 ) ): raise ValueError ( f \"The `ignore_index` { ignore_index } is not valid for inputs with { num_classes } classes\" ) # Update states if average == AverageMethod . SAMPLES : raise ValueError ( f \"The `average` method ' { average } ' is not yet supported.\" ) if mdmc_average == MDMCAverageMethod . SAMPLEWISE : raise ValueError ( f \"The `mdmc_average` method ' { mdmc_average } ' is not yet supported.\" ) self . average = average self . mdmc_average = mdmc_average self . num_classes = num_classes self . threshold = threshold self . multiclass = multiclass self . ignore_index = ignore_index self . top_k = top_k self . subset_accuracy = subset_accuracy self . mode = mode self . tp = None self . fp = None self . tn = None self . fn = None def reset ( self ) -> \"Accuracy\" : # nodes if self . average == AverageMethod . MICRO : zeros_shape = [] elif self . average == AverageMethod . MACRO : zeros_shape = [ self . num_classes ] else : raise ValueError ( f 'Wrong reduce=\" { self . average } \"' ) initial_value = jnp . zeros ( zeros_shape , dtype = jnp . uint32 ) return self . replace ( tp = initial_value , fp = initial_value , tn = initial_value , fn = initial_value , ) def update ( self , preds : jnp . ndarray , target : jnp . ndarray , ** _ ) -> \"Accuracy\" : \"\"\"Updates Accuracy metric state. Example: ``python accuracy = Accuracy().reset() accuracy = accuracy.update(preds=preds, target=target) Args: preds: Predictions from model (logits, probabilities, or target) target: Ground truth target Returns: Updated Accuracy instance \"\"\" if self . tp is None or self . fp is None or self . tn is None or self . fn is None : raise ValueError ( \"Accuracy metric has not been initialized, call 'reset()' first.\" ) tp , fp , tn , fn = metric_utils . _stat_scores_update ( preds , target , intended_mode = self . mode , average_method = self . average , mdmc_average_method = self . mdmc_average , threshold = self . threshold , num_classes = self . num_classes , top_k = self . top_k , multiclass = self . multiclass , ) return self . replace ( tp = self . tp + tp , fp = self . fp + fp , tn = self . tn + tn , fn = self . fn + fn , ) def compute ( self ) -> jnp . ndarray : \"\"\" Computes accuracy based on inputs passed in to `update` previously. Returns: Accuracy score \"\"\" # if self.mode is None: # raise RuntimeError(\"You have to have determined mode.\") if self . tp is None or self . fp is None or self . tn is None or self . fn is None : raise ValueError ( \"Accuracy metric has not been initialized, call 'reset()' first.\" ) return metric_utils . _accuracy_compute ( self . tp , self . fp , self . tn , self . fn , self . average , self . mdmc_average , self . mode , )","title":"jax_metrics.metrics.Accuracy"},{"location":"api/metrics/Accuracy/#jax_metrics.metrics.accuracy.Accuracy.compute","text":"Computes accuracy based on inputs passed in to update previously. Returns: Type Description ndarray Accuracy score Source code in jax_metrics/metrics/accuracy.py def compute ( self ) -> jnp . ndarray : \"\"\" Computes accuracy based on inputs passed in to `update` previously. Returns: Accuracy score \"\"\" # if self.mode is None: # raise RuntimeError(\"You have to have determined mode.\") if self . tp is None or self . fp is None or self . tn is None or self . fn is None : raise ValueError ( \"Accuracy metric has not been initialized, call 'reset()' first.\" ) return metric_utils . _accuracy_compute ( self . tp , self . fp , self . tn , self . fn , self . average , self . mdmc_average , self . mode , )","title":"compute()"},{"location":"api/metrics/Accuracy/#jax_metrics.metrics.accuracy.Accuracy.reset","text":"Resets the metric state. Returns: Type Description Accuracy Metric with the initial state Source code in jax_metrics/metrics/accuracy.py def reset ( self ) -> \"Accuracy\" : # nodes if self . average == AverageMethod . MICRO : zeros_shape = [] elif self . average == AverageMethod . MACRO : zeros_shape = [ self . num_classes ] else : raise ValueError ( f 'Wrong reduce=\" { self . average } \"' ) initial_value = jnp . zeros ( zeros_shape , dtype = jnp . uint32 ) return self . replace ( tp = initial_value , fp = initial_value , tn = initial_value , fn = initial_value , )","title":"reset()"},{"location":"api/metrics/Accuracy/#jax_metrics.metrics.accuracy.Accuracy.update","text":"Updates Accuracy metric state. Examples: ``python accuracy = Accuracy().reset() accuracy = accuracy.update(preds=preds, target=target) Parameters: Name Type Description Default preds ndarray Predictions from model (logits, probabilities, or target) required target ndarray Ground truth target required Returns: Type Description Accuracy Updated Accuracy instance Source code in jax_metrics/metrics/accuracy.py def update ( self , preds : jnp . ndarray , target : jnp . ndarray , ** _ ) -> \"Accuracy\" : \"\"\"Updates Accuracy metric state. Example: ``python accuracy = Accuracy().reset() accuracy = accuracy.update(preds=preds, target=target) Args: preds: Predictions from model (logits, probabilities, or target) target: Ground truth target Returns: Updated Accuracy instance \"\"\" if self . tp is None or self . fp is None or self . tn is None or self . fn is None : raise ValueError ( \"Accuracy metric has not been initialized, call 'reset()' first.\" ) tp , fp , tn , fn = metric_utils . _stat_scores_update ( preds , target , intended_mode = self . mode , average_method = self . average , mdmc_average_method = self . mdmc_average , threshold = self . threshold , num_classes = self . num_classes , top_k = self . top_k , multiclass = self . multiclass , ) return self . replace ( tp = self . tp + tp , fp = self . fp + fp , tn = self . tn + tn , fn = self . fn + fn , )","title":"update()"},{"location":"api/metrics/AuxLosses/","text":"jax_metrics.metrics.AuxLosses Source code in jax_metrics/metrics/losses.py class AuxLosses ( AuxMetrics ): def total_loss ( self ) -> jnp . ndarray : return sum ( self . compute () . values (), jnp . array ( 0.0 )) def loss_and_update ( self : A , ** kwargs ) -> tp . Tuple [ jnp . ndarray , A ]: batch_updates = self . batch_updates ( ** kwargs ) loss = batch_updates . total_loss () metrics = self . merge ( batch_updates ) return loss , metrics","title":"AuxLosses"},{"location":"api/metrics/AuxLosses/#jax_metricsmetricsauxlosses","text":"Source code in jax_metrics/metrics/losses.py class AuxLosses ( AuxMetrics ): def total_loss ( self ) -> jnp . ndarray : return sum ( self . compute () . values (), jnp . array ( 0.0 )) def loss_and_update ( self : A , ** kwargs ) -> tp . Tuple [ jnp . ndarray , A ]: batch_updates = self . batch_updates ( ** kwargs ) loss = batch_updates . total_loss () metrics = self . merge ( batch_updates ) return loss , metrics","title":"jax_metrics.metrics.AuxLosses"},{"location":"api/metrics/AuxMetrics/","text":"jax_metrics.metrics.AuxMetrics Source code in jax_metrics/metrics/metrics.py class AuxMetrics ( Metric ): totals : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = to . node () counts : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = to . node () names : tp . Optional [ tp . Tuple [ str , ... ]] = to . static () def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): super () . __init__ ( name = name , dtype = dtype ) self . totals = None self . counts = None self . names = None def init ( self : A , aux_values : tp . Dict [ str , jnp . ndarray ]) -> A : names = tuple ( aux_values . keys ()) return self . replace ( names = names ) . reset () def reset ( self : A ) -> A : if self . names is None : raise ValueError ( \"AuxMetrics not initialized, call `init()` first\" ) totals = { name : jnp . array ( 0.0 , dtype = jnp . float32 ) for name in self . names } counts = { name : jnp . array ( 0 , dtype = jnp . uint32 ) for name in self . names } return self . replace ( totals = totals , counts = counts ) def update ( self : A , aux_values : tp . Dict [ str , jnp . ndarray ], ** _ ) -> A : if self . totals is None or self . counts is None : raise ValueError ( \"AuxMetrics not initialized, call 'reset()' first\" ) totals = { name : ( self . totals [ name ] + aux_values [ name ]) . astype ( self . totals [ name ] . dtype ) for name in self . totals } counts = { name : ( self . counts [ name ] + np . prod ( aux_values [ name ] . shape )) . astype ( self . counts [ name ] . dtype ) for name in self . counts } return self . replace ( totals = totals , counts = counts ) def compute ( self ) -> tp . Dict [ str , jnp . ndarray ]: if self . totals is None or self . counts is None : raise ValueError ( \"AuxMetrics not initialized, call `reset()` first\" ) return { name : self . totals [ name ] / self . counts [ name ] for name in self . totals } def compute_logs ( self ) -> tp . Dict [ str , jnp . ndarray ]: return self . compute () def __call__ ( self : A , aux_values : tp . Any ) -> tp . Tuple [ tp . Dict [ str , jnp . ndarray ], A ]: return super () . __call__ ( aux_values = aux_values ) compute ( self ) Compute the current metric value. Source code in jax_metrics/metrics/metrics.py def compute ( self ) -> tp . Dict [ str , jnp . ndarray ]: if self . totals is None or self . counts is None : raise ValueError ( \"AuxMetrics not initialized, call `reset()` first\" ) return { name : self . totals [ name ] / self . counts [ name ] for name in self . totals } compute_logs ( self ) Compute the current metric value(s) and returns it/them in a {metric_name: metric_value} dictionary. Returns: Type Description Dict[str, jax._src.numpy.lax_numpy.ndarray] A dictionary of metric values Source code in jax_metrics/metrics/metrics.py def compute_logs ( self ) -> tp . Dict [ str , jnp . ndarray ]: return self . compute () init ( self , aux_values ) Initialize the metric's state. Source code in jax_metrics/metrics/metrics.py def init ( self : A , aux_values : tp . Dict [ str , jnp . ndarray ]) -> A : names = tuple ( aux_values . keys ()) return self . replace ( names = names ) . reset () reset ( self ) Resets the metric state. Returns: Type Description ~A Metric with the initial state Source code in jax_metrics/metrics/metrics.py def reset ( self : A ) -> A : if self . names is None : raise ValueError ( \"AuxMetrics not initialized, call `init()` first\" ) totals = { name : jnp . array ( 0.0 , dtype = jnp . float32 ) for name in self . names } counts = { name : jnp . array ( 0 , dtype = jnp . uint32 ) for name in self . names } return self . replace ( totals = totals , counts = counts ) update ( self , aux_values , ** _ ) Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining **kwargs . Parameters: Name Type Description Default **kwargs data to update the metric with required Returns: Type Description ~A Metric with updated state Source code in jax_metrics/metrics/metrics.py def update ( self : A , aux_values : tp . Dict [ str , jnp . ndarray ], ** _ ) -> A : if self . totals is None or self . counts is None : raise ValueError ( \"AuxMetrics not initialized, call 'reset()' first\" ) totals = { name : ( self . totals [ name ] + aux_values [ name ]) . astype ( self . totals [ name ] . dtype ) for name in self . totals } counts = { name : ( self . counts [ name ] + np . prod ( aux_values [ name ] . shape )) . astype ( self . counts [ name ] . dtype ) for name in self . counts } return self . replace ( totals = totals , counts = counts )","title":"AuxMetrics"},{"location":"api/metrics/AuxMetrics/#jax_metricsmetricsauxmetrics","text":"Source code in jax_metrics/metrics/metrics.py class AuxMetrics ( Metric ): totals : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = to . node () counts : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = to . node () names : tp . Optional [ tp . Tuple [ str , ... ]] = to . static () def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): super () . __init__ ( name = name , dtype = dtype ) self . totals = None self . counts = None self . names = None def init ( self : A , aux_values : tp . Dict [ str , jnp . ndarray ]) -> A : names = tuple ( aux_values . keys ()) return self . replace ( names = names ) . reset () def reset ( self : A ) -> A : if self . names is None : raise ValueError ( \"AuxMetrics not initialized, call `init()` first\" ) totals = { name : jnp . array ( 0.0 , dtype = jnp . float32 ) for name in self . names } counts = { name : jnp . array ( 0 , dtype = jnp . uint32 ) for name in self . names } return self . replace ( totals = totals , counts = counts ) def update ( self : A , aux_values : tp . Dict [ str , jnp . ndarray ], ** _ ) -> A : if self . totals is None or self . counts is None : raise ValueError ( \"AuxMetrics not initialized, call 'reset()' first\" ) totals = { name : ( self . totals [ name ] + aux_values [ name ]) . astype ( self . totals [ name ] . dtype ) for name in self . totals } counts = { name : ( self . counts [ name ] + np . prod ( aux_values [ name ] . shape )) . astype ( self . counts [ name ] . dtype ) for name in self . counts } return self . replace ( totals = totals , counts = counts ) def compute ( self ) -> tp . Dict [ str , jnp . ndarray ]: if self . totals is None or self . counts is None : raise ValueError ( \"AuxMetrics not initialized, call `reset()` first\" ) return { name : self . totals [ name ] / self . counts [ name ] for name in self . totals } def compute_logs ( self ) -> tp . Dict [ str , jnp . ndarray ]: return self . compute () def __call__ ( self : A , aux_values : tp . Any ) -> tp . Tuple [ tp . Dict [ str , jnp . ndarray ], A ]: return super () . __call__ ( aux_values = aux_values )","title":"jax_metrics.metrics.AuxMetrics"},{"location":"api/metrics/AuxMetrics/#jax_metrics.metrics.metrics.AuxMetrics.compute","text":"Compute the current metric value. Source code in jax_metrics/metrics/metrics.py def compute ( self ) -> tp . Dict [ str , jnp . ndarray ]: if self . totals is None or self . counts is None : raise ValueError ( \"AuxMetrics not initialized, call `reset()` first\" ) return { name : self . totals [ name ] / self . counts [ name ] for name in self . totals }","title":"compute()"},{"location":"api/metrics/AuxMetrics/#jax_metrics.metrics.metrics.AuxMetrics.compute_logs","text":"Compute the current metric value(s) and returns it/them in a {metric_name: metric_value} dictionary. Returns: Type Description Dict[str, jax._src.numpy.lax_numpy.ndarray] A dictionary of metric values Source code in jax_metrics/metrics/metrics.py def compute_logs ( self ) -> tp . Dict [ str , jnp . ndarray ]: return self . compute ()","title":"compute_logs()"},{"location":"api/metrics/AuxMetrics/#jax_metrics.metrics.metrics.AuxMetrics.init","text":"Initialize the metric's state. Source code in jax_metrics/metrics/metrics.py def init ( self : A , aux_values : tp . Dict [ str , jnp . ndarray ]) -> A : names = tuple ( aux_values . keys ()) return self . replace ( names = names ) . reset ()","title":"init()"},{"location":"api/metrics/AuxMetrics/#jax_metrics.metrics.metrics.AuxMetrics.reset","text":"Resets the metric state. Returns: Type Description ~A Metric with the initial state Source code in jax_metrics/metrics/metrics.py def reset ( self : A ) -> A : if self . names is None : raise ValueError ( \"AuxMetrics not initialized, call `init()` first\" ) totals = { name : jnp . array ( 0.0 , dtype = jnp . float32 ) for name in self . names } counts = { name : jnp . array ( 0 , dtype = jnp . uint32 ) for name in self . names } return self . replace ( totals = totals , counts = counts )","title":"reset()"},{"location":"api/metrics/AuxMetrics/#jax_metrics.metrics.metrics.AuxMetrics.update","text":"Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining **kwargs . Parameters: Name Type Description Default **kwargs data to update the metric with required Returns: Type Description ~A Metric with updated state Source code in jax_metrics/metrics/metrics.py def update ( self : A , aux_values : tp . Dict [ str , jnp . ndarray ], ** _ ) -> A : if self . totals is None or self . counts is None : raise ValueError ( \"AuxMetrics not initialized, call 'reset()' first\" ) totals = { name : ( self . totals [ name ] + aux_values [ name ]) . astype ( self . totals [ name ] . dtype ) for name in self . totals } counts = { name : ( self . counts [ name ] + np . prod ( aux_values [ name ] . shape )) . astype ( self . counts [ name ] . dtype ) for name in self . counts } return self . replace ( totals = totals , counts = counts )","title":"update()"},{"location":"api/metrics/Losses/","text":"jax_metrics.metrics.Losses Source code in jax_metrics/metrics/losses.py class Losses ( Metric ): losses : tp . Dict [ str , Loss ] = to . static () totals : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = to . node () counts : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = to . node () def __init__ ( self , losses : tp . Any , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): super () . __init__ ( name = name , dtype = dtype ) names : tp . Set [ str ] = set () def get_name ( path , metric , parent_iterable ): name = utils . _get_name ( metric ) if path : if parent_iterable : return f \" { path } / { name } \" else : return path else : return name self . losses = { utils . _unique_name ( names , get_name ( path , loss , parent_iterable )): loss for path , loss , parent_iterable in utils . _flatten_names ( losses ) } self . totals = None self . counts = None def reset ( self : M ) -> M : totals = { name : jnp . array ( 0.0 , dtype = jnp . float32 ) for name in self . losses } counts = { name : jnp . array ( 0 , dtype = jnp . uint32 ) for name in self . losses } return self . replace ( totals = totals , counts = counts ) def update ( self : M , ** kwargs ) -> M : if self . totals is None or self . counts is None : raise ValueError ( \"Losses not initialized, call 'reset()' first\" ) totals = { name : self . totals [ name ] + loss ( ** kwargs ) for name , loss in self . losses . items () } counts = { name : self . counts [ name ] + 1 for name in self . losses . keys ()} return self . replace ( totals = totals , counts = counts ) def compute ( self ) -> tp . Dict [ str , jnp . ndarray ]: if self . totals is None or self . counts is None : raise ValueError ( \"Losses not initialized, call 'reset()' first\" ) outputs = {} names = set () for name in self . losses . keys (): value = self . totals [ name ] / self . counts [ name ] for path , value , parent_iterable in utils . _flatten_names ( value ): name = utils . _unique_name ( names , name ) if path : if parent_iterable : name = f \" { path } / { name } \" else : name = path outputs [ name ] = value return outputs def compute_logs ( self ) -> tp . Dict [ str , jnp . ndarray ]: return self . compute () def __call__ ( self : M , ** kwargs ) -> tp . Tuple [ tp . Dict [ str , jnp . ndarray ], M ]: return super () . __call__ ( ** kwargs ) def total_loss ( self ) -> jnp . ndarray : return sum ( self . compute () . values (), jnp . array ( 0.0 )) def slice ( self , ** kwargs : types . IndexLike ) -> \"Losses\" : losses = { name : loss . index_into ( ** kwargs ) for name , loss in self . losses . items ()} return self . replace ( losses = losses ) def loss_and_update ( self : M , ** kwargs ) -> tp . Tuple [ jnp . ndarray , M ]: batch_updates = self . batch_updates ( ** kwargs ) loss = batch_updates . total_loss () metrics = self . merge ( batch_updates ) return loss , metrics compute ( self ) Compute the current metric value. Source code in jax_metrics/metrics/losses.py def compute ( self ) -> tp . Dict [ str , jnp . ndarray ]: if self . totals is None or self . counts is None : raise ValueError ( \"Losses not initialized, call 'reset()' first\" ) outputs = {} names = set () for name in self . losses . keys (): value = self . totals [ name ] / self . counts [ name ] for path , value , parent_iterable in utils . _flatten_names ( value ): name = utils . _unique_name ( names , name ) if path : if parent_iterable : name = f \" { path } / { name } \" else : name = path outputs [ name ] = value return outputs compute_logs ( self ) Compute the current metric value(s) and returns it/them in a {metric_name: metric_value} dictionary. Returns: Type Description Dict[str, jax._src.numpy.lax_numpy.ndarray] A dictionary of metric values Source code in jax_metrics/metrics/losses.py def compute_logs ( self ) -> tp . Dict [ str , jnp . ndarray ]: return self . compute () reset ( self ) Resets the metric state. Returns: Type Description ~M Metric with the initial state Source code in jax_metrics/metrics/losses.py def reset ( self : M ) -> M : totals = { name : jnp . array ( 0.0 , dtype = jnp . float32 ) for name in self . losses } counts = { name : jnp . array ( 0 , dtype = jnp . uint32 ) for name in self . losses } return self . replace ( totals = totals , counts = counts ) update ( self , ** kwargs ) Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining **kwargs . Parameters: Name Type Description Default **kwargs data to update the metric with {} Returns: Type Description ~M Metric with updated state Source code in jax_metrics/metrics/losses.py def update ( self : M , ** kwargs ) -> M : if self . totals is None or self . counts is None : raise ValueError ( \"Losses not initialized, call 'reset()' first\" ) totals = { name : self . totals [ name ] + loss ( ** kwargs ) for name , loss in self . losses . items () } counts = { name : self . counts [ name ] + 1 for name in self . losses . keys ()} return self . replace ( totals = totals , counts = counts )","title":"Losses"},{"location":"api/metrics/Losses/#jax_metricsmetricslosses","text":"Source code in jax_metrics/metrics/losses.py class Losses ( Metric ): losses : tp . Dict [ str , Loss ] = to . static () totals : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = to . node () counts : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = to . node () def __init__ ( self , losses : tp . Any , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): super () . __init__ ( name = name , dtype = dtype ) names : tp . Set [ str ] = set () def get_name ( path , metric , parent_iterable ): name = utils . _get_name ( metric ) if path : if parent_iterable : return f \" { path } / { name } \" else : return path else : return name self . losses = { utils . _unique_name ( names , get_name ( path , loss , parent_iterable )): loss for path , loss , parent_iterable in utils . _flatten_names ( losses ) } self . totals = None self . counts = None def reset ( self : M ) -> M : totals = { name : jnp . array ( 0.0 , dtype = jnp . float32 ) for name in self . losses } counts = { name : jnp . array ( 0 , dtype = jnp . uint32 ) for name in self . losses } return self . replace ( totals = totals , counts = counts ) def update ( self : M , ** kwargs ) -> M : if self . totals is None or self . counts is None : raise ValueError ( \"Losses not initialized, call 'reset()' first\" ) totals = { name : self . totals [ name ] + loss ( ** kwargs ) for name , loss in self . losses . items () } counts = { name : self . counts [ name ] + 1 for name in self . losses . keys ()} return self . replace ( totals = totals , counts = counts ) def compute ( self ) -> tp . Dict [ str , jnp . ndarray ]: if self . totals is None or self . counts is None : raise ValueError ( \"Losses not initialized, call 'reset()' first\" ) outputs = {} names = set () for name in self . losses . keys (): value = self . totals [ name ] / self . counts [ name ] for path , value , parent_iterable in utils . _flatten_names ( value ): name = utils . _unique_name ( names , name ) if path : if parent_iterable : name = f \" { path } / { name } \" else : name = path outputs [ name ] = value return outputs def compute_logs ( self ) -> tp . Dict [ str , jnp . ndarray ]: return self . compute () def __call__ ( self : M , ** kwargs ) -> tp . Tuple [ tp . Dict [ str , jnp . ndarray ], M ]: return super () . __call__ ( ** kwargs ) def total_loss ( self ) -> jnp . ndarray : return sum ( self . compute () . values (), jnp . array ( 0.0 )) def slice ( self , ** kwargs : types . IndexLike ) -> \"Losses\" : losses = { name : loss . index_into ( ** kwargs ) for name , loss in self . losses . items ()} return self . replace ( losses = losses ) def loss_and_update ( self : M , ** kwargs ) -> tp . Tuple [ jnp . ndarray , M ]: batch_updates = self . batch_updates ( ** kwargs ) loss = batch_updates . total_loss () metrics = self . merge ( batch_updates ) return loss , metrics","title":"jax_metrics.metrics.Losses"},{"location":"api/metrics/Losses/#jax_metrics.metrics.losses.Losses.compute","text":"Compute the current metric value. Source code in jax_metrics/metrics/losses.py def compute ( self ) -> tp . Dict [ str , jnp . ndarray ]: if self . totals is None or self . counts is None : raise ValueError ( \"Losses not initialized, call 'reset()' first\" ) outputs = {} names = set () for name in self . losses . keys (): value = self . totals [ name ] / self . counts [ name ] for path , value , parent_iterable in utils . _flatten_names ( value ): name = utils . _unique_name ( names , name ) if path : if parent_iterable : name = f \" { path } / { name } \" else : name = path outputs [ name ] = value return outputs","title":"compute()"},{"location":"api/metrics/Losses/#jax_metrics.metrics.losses.Losses.compute_logs","text":"Compute the current metric value(s) and returns it/them in a {metric_name: metric_value} dictionary. Returns: Type Description Dict[str, jax._src.numpy.lax_numpy.ndarray] A dictionary of metric values Source code in jax_metrics/metrics/losses.py def compute_logs ( self ) -> tp . Dict [ str , jnp . ndarray ]: return self . compute ()","title":"compute_logs()"},{"location":"api/metrics/Losses/#jax_metrics.metrics.losses.Losses.reset","text":"Resets the metric state. Returns: Type Description ~M Metric with the initial state Source code in jax_metrics/metrics/losses.py def reset ( self : M ) -> M : totals = { name : jnp . array ( 0.0 , dtype = jnp . float32 ) for name in self . losses } counts = { name : jnp . array ( 0 , dtype = jnp . uint32 ) for name in self . losses } return self . replace ( totals = totals , counts = counts )","title":"reset()"},{"location":"api/metrics/Losses/#jax_metrics.metrics.losses.Losses.update","text":"Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining **kwargs . Parameters: Name Type Description Default **kwargs data to update the metric with {} Returns: Type Description ~M Metric with updated state Source code in jax_metrics/metrics/losses.py def update ( self : M , ** kwargs ) -> M : if self . totals is None or self . counts is None : raise ValueError ( \"Losses not initialized, call 'reset()' first\" ) totals = { name : self . totals [ name ] + loss ( ** kwargs ) for name , loss in self . losses . items () } counts = { name : self . counts [ name ] + 1 for name in self . losses . keys ()} return self . replace ( totals = totals , counts = counts )","title":"update()"},{"location":"api/metrics/LossesAndMetrics/","text":"jax_metrics.metrics.LossesAndMetrics Source code in jax_metrics/metrics/losses_and_metrics.py class LossesAndMetrics ( Metric ): losses : tp . Optional [ Losses ] metrics : tp . Optional [ Metrics ] aux_losses : tp . Optional [ AuxLosses ] aux_metrics : tp . Optional [ AuxMetrics ] def __init__ ( self , losses : tp . Optional [ tp . Union [ Losses , tp . Any ]] = None , metrics : tp . Optional [ tp . Union [ Metrics , tp . Any ]] = None , aux_losses : tp . Optional [ tp . Union [ AuxLosses , tp . Any ]] = None , aux_metrics : tp . Optional [ tp . Union [ AuxMetrics , tp . Any ]] = None , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): super () . __init__ ( name = name , dtype = dtype ) self . losses = ( losses if isinstance ( losses , Losses ) else Losses ( losses ) if losses is not None else None ) self . metrics = ( metrics if isinstance ( metrics , Metrics ) else Metrics ( metrics ) if metrics is not None else None ) self . aux_losses = ( aux_losses if isinstance ( aux_losses , AuxLosses ) else AuxLosses ( aux_losses ) if aux_losses is not None else None ) self . aux_metrics = ( aux_metrics if isinstance ( aux_metrics , AuxMetrics ) else AuxMetrics ( aux_metrics ) if aux_metrics is not None else None ) def init ( self : M , * , aux_losses : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , aux_metrics : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , ) -> M : if self . aux_losses is not None : if aux_losses is None : raise ValueError ( \"`aux_losses` are expected, got None.\" ) aux_losses_ = self . aux_losses . init ( aux_losses ) else : if aux_losses is not None : raise ValueError ( f \"`aux_losses` are not expected, got { aux_losses } .\" ) aux_losses_ = None if self . aux_metrics is not None : if aux_metrics is None : raise ValueError ( \"`aux_metrics` are expected, got None.\" ) aux_metrics_ = self . aux_metrics . init ( aux_metrics ) else : if aux_metrics is not None : raise ValueError ( f \"`aux_metrics` are not expected, got { aux_metrics } .\" ) aux_metrics_ = None return self . replace ( aux_losses = aux_losses_ , aux_metrics = aux_metrics_ , ) . reset () def reset ( self : M ) -> M : if self . losses is not None : losses = self . losses . reset () else : losses = None if self . metrics is not None : metrics = self . metrics . reset () else : metrics = None if self . aux_losses is not None : aux_losses_ = self . aux_losses . reset () else : aux_losses_ = None if self . aux_metrics is not None : aux_metrics_ = self . aux_metrics . reset () else : aux_metrics_ = None return self . replace ( losses = losses , metrics = metrics , aux_losses = aux_losses_ , aux_metrics = aux_metrics_ , ) def update ( self : M , aux_losses : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , aux_metrics : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , ** kwargs , ) -> M : if self . losses is not None : losses = self . losses . update ( ** kwargs ) else : losses = None if self . metrics is not None : metrics = self . metrics . update ( ** kwargs ) else : metrics = None if self . aux_losses is not None : if aux_losses is None : raise ValueError ( \"`aux_losses` are expected, got None.\" ) aux_losses_ = self . aux_losses . update ( aux_values = aux_losses ) else : aux_losses_ = None if self . aux_metrics is not None : if aux_metrics is None : raise ValueError ( \"`aux_metrics` are expected, got None.\" ) aux_metrics_ = self . aux_metrics . update ( aux_values = aux_metrics ) else : aux_metrics_ = None return self . replace ( losses = losses , metrics = metrics , aux_losses = aux_losses_ , aux_metrics = aux_metrics_ , ) def compute ( self ) -> tp . Dict [ str , jnp . ndarray ]: if self . losses is not None : losses_logs = self . losses . compute () else : losses_logs = {} if self . metrics is not None : metrics_logs = self . metrics . compute () else : metrics_logs = {} if self . aux_losses is not None : aux_losses_logs = self . aux_losses . compute () else : aux_losses_logs = {} if self . aux_metrics is not None : aux_metrics_logs = self . aux_metrics . compute () else : aux_metrics_logs = {} loss = self . total_loss () return { \"loss\" : loss , ** losses_logs , ** metrics_logs , ** aux_losses_logs , ** aux_metrics_logs , } def compute_logs ( self ) -> tp . Dict [ str , jnp . ndarray ]: return self . compute () def __call__ ( self : M , aux_losses : tp . Optional [ tp . Any ] = None , aux_metrics : tp . Optional [ tp . Any ] = None , ** kwargs , ) -> tp . Tuple [ tp . Dict [ str , jnp . ndarray ], M ]: return super () . __call__ ( aux_losses = aux_losses , aux_metrics = aux_metrics , ** kwargs , ) def total_loss ( self ) -> jnp . ndarray : loss = jnp . array ( 0.0 , dtype = jnp . float32 ) if self . losses is not None : loss += self . losses . total_loss () if self . aux_losses is not None : loss += self . aux_losses . total_loss () return loss def loss_and_update ( self : M , ** kwargs ) -> tp . Tuple [ jnp . ndarray , M ]: batch_updates = self . batch_updates ( ** kwargs ) loss = batch_updates . total_loss () metrics = self . merge ( batch_updates ) return loss , metrics compute ( self ) Compute the current metric value. Source code in jax_metrics/metrics/losses_and_metrics.py def compute ( self ) -> tp . Dict [ str , jnp . ndarray ]: if self . losses is not None : losses_logs = self . losses . compute () else : losses_logs = {} if self . metrics is not None : metrics_logs = self . metrics . compute () else : metrics_logs = {} if self . aux_losses is not None : aux_losses_logs = self . aux_losses . compute () else : aux_losses_logs = {} if self . aux_metrics is not None : aux_metrics_logs = self . aux_metrics . compute () else : aux_metrics_logs = {} loss = self . total_loss () return { \"loss\" : loss , ** losses_logs , ** metrics_logs , ** aux_losses_logs , ** aux_metrics_logs , } compute_logs ( self ) Compute the current metric value(s) and returns it/them in a {metric_name: metric_value} dictionary. Returns: Type Description Dict[str, jax._src.numpy.lax_numpy.ndarray] A dictionary of metric values Source code in jax_metrics/metrics/losses_and_metrics.py def compute_logs ( self ) -> tp . Dict [ str , jnp . ndarray ]: return self . compute () init ( self , * , aux_losses = None , aux_metrics = None ) Initialize the metric's state. Source code in jax_metrics/metrics/losses_and_metrics.py def init ( self : M , * , aux_losses : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , aux_metrics : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , ) -> M : if self . aux_losses is not None : if aux_losses is None : raise ValueError ( \"`aux_losses` are expected, got None.\" ) aux_losses_ = self . aux_losses . init ( aux_losses ) else : if aux_losses is not None : raise ValueError ( f \"`aux_losses` are not expected, got { aux_losses } .\" ) aux_losses_ = None if self . aux_metrics is not None : if aux_metrics is None : raise ValueError ( \"`aux_metrics` are expected, got None.\" ) aux_metrics_ = self . aux_metrics . init ( aux_metrics ) else : if aux_metrics is not None : raise ValueError ( f \"`aux_metrics` are not expected, got { aux_metrics } .\" ) aux_metrics_ = None return self . replace ( aux_losses = aux_losses_ , aux_metrics = aux_metrics_ , ) . reset () reset ( self ) Resets the metric state. Returns: Type Description ~M Metric with the initial state Source code in jax_metrics/metrics/losses_and_metrics.py def reset ( self : M ) -> M : if self . losses is not None : losses = self . losses . reset () else : losses = None if self . metrics is not None : metrics = self . metrics . reset () else : metrics = None if self . aux_losses is not None : aux_losses_ = self . aux_losses . reset () else : aux_losses_ = None if self . aux_metrics is not None : aux_metrics_ = self . aux_metrics . reset () else : aux_metrics_ = None return self . replace ( losses = losses , metrics = metrics , aux_losses = aux_losses_ , aux_metrics = aux_metrics_ , ) update ( self , aux_losses = None , aux_metrics = None , ** kwargs ) Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining **kwargs . Parameters: Name Type Description Default **kwargs data to update the metric with {} Returns: Type Description ~M Metric with updated state Source code in jax_metrics/metrics/losses_and_metrics.py def update ( self : M , aux_losses : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , aux_metrics : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , ** kwargs , ) -> M : if self . losses is not None : losses = self . losses . update ( ** kwargs ) else : losses = None if self . metrics is not None : metrics = self . metrics . update ( ** kwargs ) else : metrics = None if self . aux_losses is not None : if aux_losses is None : raise ValueError ( \"`aux_losses` are expected, got None.\" ) aux_losses_ = self . aux_losses . update ( aux_values = aux_losses ) else : aux_losses_ = None if self . aux_metrics is not None : if aux_metrics is None : raise ValueError ( \"`aux_metrics` are expected, got None.\" ) aux_metrics_ = self . aux_metrics . update ( aux_values = aux_metrics ) else : aux_metrics_ = None return self . replace ( losses = losses , metrics = metrics , aux_losses = aux_losses_ , aux_metrics = aux_metrics_ , )","title":"LossesAndMetrics"},{"location":"api/metrics/LossesAndMetrics/#jax_metricsmetricslossesandmetrics","text":"Source code in jax_metrics/metrics/losses_and_metrics.py class LossesAndMetrics ( Metric ): losses : tp . Optional [ Losses ] metrics : tp . Optional [ Metrics ] aux_losses : tp . Optional [ AuxLosses ] aux_metrics : tp . Optional [ AuxMetrics ] def __init__ ( self , losses : tp . Optional [ tp . Union [ Losses , tp . Any ]] = None , metrics : tp . Optional [ tp . Union [ Metrics , tp . Any ]] = None , aux_losses : tp . Optional [ tp . Union [ AuxLosses , tp . Any ]] = None , aux_metrics : tp . Optional [ tp . Union [ AuxMetrics , tp . Any ]] = None , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): super () . __init__ ( name = name , dtype = dtype ) self . losses = ( losses if isinstance ( losses , Losses ) else Losses ( losses ) if losses is not None else None ) self . metrics = ( metrics if isinstance ( metrics , Metrics ) else Metrics ( metrics ) if metrics is not None else None ) self . aux_losses = ( aux_losses if isinstance ( aux_losses , AuxLosses ) else AuxLosses ( aux_losses ) if aux_losses is not None else None ) self . aux_metrics = ( aux_metrics if isinstance ( aux_metrics , AuxMetrics ) else AuxMetrics ( aux_metrics ) if aux_metrics is not None else None ) def init ( self : M , * , aux_losses : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , aux_metrics : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , ) -> M : if self . aux_losses is not None : if aux_losses is None : raise ValueError ( \"`aux_losses` are expected, got None.\" ) aux_losses_ = self . aux_losses . init ( aux_losses ) else : if aux_losses is not None : raise ValueError ( f \"`aux_losses` are not expected, got { aux_losses } .\" ) aux_losses_ = None if self . aux_metrics is not None : if aux_metrics is None : raise ValueError ( \"`aux_metrics` are expected, got None.\" ) aux_metrics_ = self . aux_metrics . init ( aux_metrics ) else : if aux_metrics is not None : raise ValueError ( f \"`aux_metrics` are not expected, got { aux_metrics } .\" ) aux_metrics_ = None return self . replace ( aux_losses = aux_losses_ , aux_metrics = aux_metrics_ , ) . reset () def reset ( self : M ) -> M : if self . losses is not None : losses = self . losses . reset () else : losses = None if self . metrics is not None : metrics = self . metrics . reset () else : metrics = None if self . aux_losses is not None : aux_losses_ = self . aux_losses . reset () else : aux_losses_ = None if self . aux_metrics is not None : aux_metrics_ = self . aux_metrics . reset () else : aux_metrics_ = None return self . replace ( losses = losses , metrics = metrics , aux_losses = aux_losses_ , aux_metrics = aux_metrics_ , ) def update ( self : M , aux_losses : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , aux_metrics : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , ** kwargs , ) -> M : if self . losses is not None : losses = self . losses . update ( ** kwargs ) else : losses = None if self . metrics is not None : metrics = self . metrics . update ( ** kwargs ) else : metrics = None if self . aux_losses is not None : if aux_losses is None : raise ValueError ( \"`aux_losses` are expected, got None.\" ) aux_losses_ = self . aux_losses . update ( aux_values = aux_losses ) else : aux_losses_ = None if self . aux_metrics is not None : if aux_metrics is None : raise ValueError ( \"`aux_metrics` are expected, got None.\" ) aux_metrics_ = self . aux_metrics . update ( aux_values = aux_metrics ) else : aux_metrics_ = None return self . replace ( losses = losses , metrics = metrics , aux_losses = aux_losses_ , aux_metrics = aux_metrics_ , ) def compute ( self ) -> tp . Dict [ str , jnp . ndarray ]: if self . losses is not None : losses_logs = self . losses . compute () else : losses_logs = {} if self . metrics is not None : metrics_logs = self . metrics . compute () else : metrics_logs = {} if self . aux_losses is not None : aux_losses_logs = self . aux_losses . compute () else : aux_losses_logs = {} if self . aux_metrics is not None : aux_metrics_logs = self . aux_metrics . compute () else : aux_metrics_logs = {} loss = self . total_loss () return { \"loss\" : loss , ** losses_logs , ** metrics_logs , ** aux_losses_logs , ** aux_metrics_logs , } def compute_logs ( self ) -> tp . Dict [ str , jnp . ndarray ]: return self . compute () def __call__ ( self : M , aux_losses : tp . Optional [ tp . Any ] = None , aux_metrics : tp . Optional [ tp . Any ] = None , ** kwargs , ) -> tp . Tuple [ tp . Dict [ str , jnp . ndarray ], M ]: return super () . __call__ ( aux_losses = aux_losses , aux_metrics = aux_metrics , ** kwargs , ) def total_loss ( self ) -> jnp . ndarray : loss = jnp . array ( 0.0 , dtype = jnp . float32 ) if self . losses is not None : loss += self . losses . total_loss () if self . aux_losses is not None : loss += self . aux_losses . total_loss () return loss def loss_and_update ( self : M , ** kwargs ) -> tp . Tuple [ jnp . ndarray , M ]: batch_updates = self . batch_updates ( ** kwargs ) loss = batch_updates . total_loss () metrics = self . merge ( batch_updates ) return loss , metrics","title":"jax_metrics.metrics.LossesAndMetrics"},{"location":"api/metrics/LossesAndMetrics/#jax_metrics.metrics.losses_and_metrics.LossesAndMetrics.compute","text":"Compute the current metric value. Source code in jax_metrics/metrics/losses_and_metrics.py def compute ( self ) -> tp . Dict [ str , jnp . ndarray ]: if self . losses is not None : losses_logs = self . losses . compute () else : losses_logs = {} if self . metrics is not None : metrics_logs = self . metrics . compute () else : metrics_logs = {} if self . aux_losses is not None : aux_losses_logs = self . aux_losses . compute () else : aux_losses_logs = {} if self . aux_metrics is not None : aux_metrics_logs = self . aux_metrics . compute () else : aux_metrics_logs = {} loss = self . total_loss () return { \"loss\" : loss , ** losses_logs , ** metrics_logs , ** aux_losses_logs , ** aux_metrics_logs , }","title":"compute()"},{"location":"api/metrics/LossesAndMetrics/#jax_metrics.metrics.losses_and_metrics.LossesAndMetrics.compute_logs","text":"Compute the current metric value(s) and returns it/them in a {metric_name: metric_value} dictionary. Returns: Type Description Dict[str, jax._src.numpy.lax_numpy.ndarray] A dictionary of metric values Source code in jax_metrics/metrics/losses_and_metrics.py def compute_logs ( self ) -> tp . Dict [ str , jnp . ndarray ]: return self . compute ()","title":"compute_logs()"},{"location":"api/metrics/LossesAndMetrics/#jax_metrics.metrics.losses_and_metrics.LossesAndMetrics.init","text":"Initialize the metric's state. Source code in jax_metrics/metrics/losses_and_metrics.py def init ( self : M , * , aux_losses : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , aux_metrics : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , ) -> M : if self . aux_losses is not None : if aux_losses is None : raise ValueError ( \"`aux_losses` are expected, got None.\" ) aux_losses_ = self . aux_losses . init ( aux_losses ) else : if aux_losses is not None : raise ValueError ( f \"`aux_losses` are not expected, got { aux_losses } .\" ) aux_losses_ = None if self . aux_metrics is not None : if aux_metrics is None : raise ValueError ( \"`aux_metrics` are expected, got None.\" ) aux_metrics_ = self . aux_metrics . init ( aux_metrics ) else : if aux_metrics is not None : raise ValueError ( f \"`aux_metrics` are not expected, got { aux_metrics } .\" ) aux_metrics_ = None return self . replace ( aux_losses = aux_losses_ , aux_metrics = aux_metrics_ , ) . reset ()","title":"init()"},{"location":"api/metrics/LossesAndMetrics/#jax_metrics.metrics.losses_and_metrics.LossesAndMetrics.reset","text":"Resets the metric state. Returns: Type Description ~M Metric with the initial state Source code in jax_metrics/metrics/losses_and_metrics.py def reset ( self : M ) -> M : if self . losses is not None : losses = self . losses . reset () else : losses = None if self . metrics is not None : metrics = self . metrics . reset () else : metrics = None if self . aux_losses is not None : aux_losses_ = self . aux_losses . reset () else : aux_losses_ = None if self . aux_metrics is not None : aux_metrics_ = self . aux_metrics . reset () else : aux_metrics_ = None return self . replace ( losses = losses , metrics = metrics , aux_losses = aux_losses_ , aux_metrics = aux_metrics_ , )","title":"reset()"},{"location":"api/metrics/LossesAndMetrics/#jax_metrics.metrics.losses_and_metrics.LossesAndMetrics.update","text":"Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining **kwargs . Parameters: Name Type Description Default **kwargs data to update the metric with {} Returns: Type Description ~M Metric with updated state Source code in jax_metrics/metrics/losses_and_metrics.py def update ( self : M , aux_losses : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , aux_metrics : tp . Optional [ tp . Dict [ str , jnp . ndarray ]] = None , ** kwargs , ) -> M : if self . losses is not None : losses = self . losses . update ( ** kwargs ) else : losses = None if self . metrics is not None : metrics = self . metrics . update ( ** kwargs ) else : metrics = None if self . aux_losses is not None : if aux_losses is None : raise ValueError ( \"`aux_losses` are expected, got None.\" ) aux_losses_ = self . aux_losses . update ( aux_values = aux_losses ) else : aux_losses_ = None if self . aux_metrics is not None : if aux_metrics is None : raise ValueError ( \"`aux_metrics` are expected, got None.\" ) aux_metrics_ = self . aux_metrics . update ( aux_values = aux_metrics ) else : aux_metrics_ = None return self . replace ( losses = losses , metrics = metrics , aux_losses = aux_losses_ , aux_metrics = aux_metrics_ , )","title":"update()"},{"location":"api/metrics/MAE/","text":"jax_metrics.metrics.MAE Source code in jax_metrics/metrics/mean_absolute_error.py class MeanAbsoluteError ( Mean ): def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Absolute Error`_ (MAE): .. math:: \\text{MAE} = \\frac{1}{N}\\sum_i^N | y_i - \\hat{y_i} | Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from jax_metrics.metrics.mean_absolute_error import MeanAbsolutError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mae = MeanAbsolutError() >>> mae(preds, target) \"\"\" super () . __init__ ( name = name , dtype = dtype ) def update ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> tp . Any : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: MeanAbsoluteError instance with updated state \"\"\" values = _mean_absolute_error ( preds , target ) return super () . update ( values , sample_weight ) __init__ ( self , name = None , dtype = None ) special Computes Mean Absolute Error _ (MAE): .. math:: ext{MAE} = rac{1}{N}\\sum_i^N | y_i - \\hat{y_i} | Where :math: y is a tensor of target values, and :math: \\hat{y} is a tensor of predictions. Parameters: Name Type Description Default name Optional[str] Module name None dtype Optional[numpy.dtype] Metrics states initialization dtype None Examples: import jax.numpy as jnp from jax_metrics.metrics.mean_absolute_error import MeanAbsolutError target = jnp.array([3.0, -0.5, 2.0, 7.0]) preds = jnp.array([3.0, -0.5, 2.0, 7.0]) mae = MeanAbsolutError() mae(preds, target) Source code in jax_metrics/metrics/mean_absolute_error.py def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Absolute Error`_ (MAE): .. math:: \\text{MAE} = \\frac{1}{N}\\sum_i^N | y_i - \\hat{y_i} | Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from jax_metrics.metrics.mean_absolute_error import MeanAbsolutError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mae = MeanAbsolutError() >>> mae(preds, target) \"\"\" super () . __init__ ( name = name , dtype = dtype ) update ( self , target , preds , sample_weight = None , ** _ ) Accumulates metric statistics. target and preds should have the same shape. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. shape = [batch_size, d0, .. dN] None Returns: Type Description Any MeanAbsoluteError instance with updated state Source code in jax_metrics/metrics/mean_absolute_error.py def update ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> tp . Any : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: MeanAbsoluteError instance with updated state \"\"\" values = _mean_absolute_error ( preds , target ) return super () . update ( values , sample_weight )","title":"MAE"},{"location":"api/metrics/MAE/#jax_metricsmetricsmae","text":"Source code in jax_metrics/metrics/mean_absolute_error.py class MeanAbsoluteError ( Mean ): def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Absolute Error`_ (MAE): .. math:: \\text{MAE} = \\frac{1}{N}\\sum_i^N | y_i - \\hat{y_i} | Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from jax_metrics.metrics.mean_absolute_error import MeanAbsolutError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mae = MeanAbsolutError() >>> mae(preds, target) \"\"\" super () . __init__ ( name = name , dtype = dtype ) def update ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> tp . Any : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: MeanAbsoluteError instance with updated state \"\"\" values = _mean_absolute_error ( preds , target ) return super () . update ( values , sample_weight )","title":"jax_metrics.metrics.MAE"},{"location":"api/metrics/MAE/#jax_metrics.metrics.mean_absolute_error.MAE.__init__","text":"Computes Mean Absolute Error _ (MAE): .. math:: ext{MAE} = rac{1}{N}\\sum_i^N | y_i - \\hat{y_i} | Where :math: y is a tensor of target values, and :math: \\hat{y} is a tensor of predictions. Parameters: Name Type Description Default name Optional[str] Module name None dtype Optional[numpy.dtype] Metrics states initialization dtype None Examples: import jax.numpy as jnp from jax_metrics.metrics.mean_absolute_error import MeanAbsolutError target = jnp.array([3.0, -0.5, 2.0, 7.0]) preds = jnp.array([3.0, -0.5, 2.0, 7.0]) mae = MeanAbsolutError() mae(preds, target) Source code in jax_metrics/metrics/mean_absolute_error.py def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Absolute Error`_ (MAE): .. math:: \\text{MAE} = \\frac{1}{N}\\sum_i^N | y_i - \\hat{y_i} | Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from jax_metrics.metrics.mean_absolute_error import MeanAbsolutError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mae = MeanAbsolutError() >>> mae(preds, target) \"\"\" super () . __init__ ( name = name , dtype = dtype )","title":"__init__()"},{"location":"api/metrics/MAE/#jax_metrics.metrics.mean_absolute_error.MAE.update","text":"Accumulates metric statistics. target and preds should have the same shape. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. shape = [batch_size, d0, .. dN] None Returns: Type Description Any MeanAbsoluteError instance with updated state Source code in jax_metrics/metrics/mean_absolute_error.py def update ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> tp . Any : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: MeanAbsoluteError instance with updated state \"\"\" values = _mean_absolute_error ( preds , target ) return super () . update ( values , sample_weight )","title":"update()"},{"location":"api/metrics/MSE/","text":"jax_metrics.metrics.MSE Source code in jax_metrics/metrics/mean_square_error.py class MeanSquareError ( Mean ): def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Square Error`_ (MSE): .. math:: \\text{MSE} = \\frac{1}{N}\\sum_i^N(y_i - \\hat{y_i})^2 Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from jax_metrics.metrics.mean_square_error import MeanSquareError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mse = MeanSquareError() >>> mse(preds, target) \"\"\" super () . __init__ ( name = name , dtype = dtype ) def update ( self : M , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> M : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: MeanSquareError instance with updated state \"\"\" values = _mean_square_error ( preds , target ) return super () . update ( values , sample_weight ) __init__ ( self , name = None , dtype = None ) special Computes Mean Square Error _ (MSE): .. math:: ext{MSE} = rac{1}{N}\\sum_i^N(y_i - \\hat{y_i})^2 Where :math: y is a tensor of target values, and :math: \\hat{y} is a tensor of predictions. Parameters: Name Type Description Default name Optional[str] Module name None dtype Optional[numpy.dtype] Metrics states initialization dtype None Examples: import jax.numpy as jnp from jax_metrics.metrics.mean_square_error import MeanSquareError target = jnp.array([3.0, -0.5, 2.0, 7.0]) preds = jnp.array([3.0, -0.5, 2.0, 7.0]) mse = MeanSquareError() mse(preds, target) Source code in jax_metrics/metrics/mean_square_error.py def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Square Error`_ (MSE): .. math:: \\text{MSE} = \\frac{1}{N}\\sum_i^N(y_i - \\hat{y_i})^2 Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from jax_metrics.metrics.mean_square_error import MeanSquareError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mse = MeanSquareError() >>> mse(preds, target) \"\"\" super () . __init__ ( name = name , dtype = dtype ) update ( self , target , preds , sample_weight = None , ** _ ) Accumulates metric statistics. target and preds should have the same shape. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. shape = [batch_size, d0, .. dN] None Returns: Type Description ~M MeanSquareError instance with updated state Source code in jax_metrics/metrics/mean_square_error.py def update ( self : M , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> M : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: MeanSquareError instance with updated state \"\"\" values = _mean_square_error ( preds , target ) return super () . update ( values , sample_weight )","title":"MSE"},{"location":"api/metrics/MSE/#jax_metricsmetricsmse","text":"Source code in jax_metrics/metrics/mean_square_error.py class MeanSquareError ( Mean ): def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Square Error`_ (MSE): .. math:: \\text{MSE} = \\frac{1}{N}\\sum_i^N(y_i - \\hat{y_i})^2 Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from jax_metrics.metrics.mean_square_error import MeanSquareError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mse = MeanSquareError() >>> mse(preds, target) \"\"\" super () . __init__ ( name = name , dtype = dtype ) def update ( self : M , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> M : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: MeanSquareError instance with updated state \"\"\" values = _mean_square_error ( preds , target ) return super () . update ( values , sample_weight )","title":"jax_metrics.metrics.MSE"},{"location":"api/metrics/MSE/#jax_metrics.metrics.mean_square_error.MSE.__init__","text":"Computes Mean Square Error _ (MSE): .. math:: ext{MSE} = rac{1}{N}\\sum_i^N(y_i - \\hat{y_i})^2 Where :math: y is a tensor of target values, and :math: \\hat{y} is a tensor of predictions. Parameters: Name Type Description Default name Optional[str] Module name None dtype Optional[numpy.dtype] Metrics states initialization dtype None Examples: import jax.numpy as jnp from jax_metrics.metrics.mean_square_error import MeanSquareError target = jnp.array([3.0, -0.5, 2.0, 7.0]) preds = jnp.array([3.0, -0.5, 2.0, 7.0]) mse = MeanSquareError() mse(preds, target) Source code in jax_metrics/metrics/mean_square_error.py def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Square Error`_ (MSE): .. math:: \\text{MSE} = \\frac{1}{N}\\sum_i^N(y_i - \\hat{y_i})^2 Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from jax_metrics.metrics.mean_square_error import MeanSquareError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mse = MeanSquareError() >>> mse(preds, target) \"\"\" super () . __init__ ( name = name , dtype = dtype )","title":"__init__()"},{"location":"api/metrics/MSE/#jax_metrics.metrics.mean_square_error.MSE.update","text":"Accumulates metric statistics. target and preds should have the same shape. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. shape = [batch_size, d0, .. dN] None Returns: Type Description ~M MeanSquareError instance with updated state Source code in jax_metrics/metrics/mean_square_error.py def update ( self : M , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> M : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: MeanSquareError instance with updated state \"\"\" values = _mean_square_error ( preds , target ) return super () . update ( values , sample_weight )","title":"update()"},{"location":"api/metrics/Mean/","text":"jax_metrics.metrics.Mean Computes the (weighted) mean of the given values. For example, if values is [1, 3, 5, 7] then the mean is 4 . If the weights were specified as [1, 1, 0, 0] then the mean would be 2 . This metric creates two variables, total and count that are used to compute the average of values . This average is ultimately returned as mean which is an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: mean = elegy . metrics . Mean () result = mean ([ 1 , 3 , 5 , 7 ]) # 16 / 4 assert result == 4.0 result = mean ([ 4 , 10 ]) # 30 / 6 assert result == 5.0 Usage with elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanSquaredError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/metrics/mean.py class Mean ( Reduce ): \"\"\" Computes the (weighted) mean of the given values. For example, if values is `[1, 3, 5, 7]` then the mean is `4`. If the weights were specified as `[1, 1, 0, 0]` then the mean would be `2`. This metric creates two variables, `total` and `count` that are used to compute the average of `values`. This average is ultimately returned as `mean` which is an idempotent operation that simply divides `total` by `count`. If `sample_weight` is `None`, weights default to 1. Use `sample_weight` of 0 to mask values. Usage: ```python mean = elegy.metrics.Mean() result = mean([1, 3, 5, 7]) # 16 / 4 assert result == 4.0 result = mean([4, 10]) # 30 / 6 assert result == 5.0 ``` Usage with elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanSquaredError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\"Creates a `Mean` instance. Arguments: kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( reduction = Reduction . weighted_mean , name = name , dtype = dtype , ) def update ( self : M , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> M : \"\"\" Accumulates the mean statistic over various batches. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Returns: Mean instance with updated state. \"\"\" return super () . update ( values = values , sample_weight = sample_weight , ) __init__ ( self , name = None , dtype = None ) special Creates a Mean instance. Parameters: Name Type Description Default kwargs Additional keyword arguments passed to Module. required Source code in jax_metrics/metrics/mean.py def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\"Creates a `Mean` instance. Arguments: kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( reduction = Reduction . weighted_mean , name = name , dtype = dtype , ) update ( self , values , sample_weight = None , ** _ ) Accumulates the mean statistic over various batches. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. None Returns: Type Description ~M Mean instance with updated state. Source code in jax_metrics/metrics/mean.py def update ( self : M , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> M : \"\"\" Accumulates the mean statistic over various batches. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Returns: Mean instance with updated state. \"\"\" return super () . update ( values = values , sample_weight = sample_weight , )","title":"Mean"},{"location":"api/metrics/Mean/#jax_metricsmetricsmean","text":"Computes the (weighted) mean of the given values. For example, if values is [1, 3, 5, 7] then the mean is 4 . If the weights were specified as [1, 1, 0, 0] then the mean would be 2 . This metric creates two variables, total and count that are used to compute the average of values . This average is ultimately returned as mean which is an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: mean = elegy . metrics . Mean () result = mean ([ 1 , 3 , 5 , 7 ]) # 16 / 4 assert result == 4.0 result = mean ([ 4 , 10 ]) # 30 / 6 assert result == 5.0 Usage with elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanSquaredError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/metrics/mean.py class Mean ( Reduce ): \"\"\" Computes the (weighted) mean of the given values. For example, if values is `[1, 3, 5, 7]` then the mean is `4`. If the weights were specified as `[1, 1, 0, 0]` then the mean would be `2`. This metric creates two variables, `total` and `count` that are used to compute the average of `values`. This average is ultimately returned as `mean` which is an idempotent operation that simply divides `total` by `count`. If `sample_weight` is `None`, weights default to 1. Use `sample_weight` of 0 to mask values. Usage: ```python mean = elegy.metrics.Mean() result = mean([1, 3, 5, 7]) # 16 / 4 assert result == 4.0 result = mean([4, 10]) # 30 / 6 assert result == 5.0 ``` Usage with elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanSquaredError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\"Creates a `Mean` instance. Arguments: kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( reduction = Reduction . weighted_mean , name = name , dtype = dtype , ) def update ( self : M , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> M : \"\"\" Accumulates the mean statistic over various batches. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Returns: Mean instance with updated state. \"\"\" return super () . update ( values = values , sample_weight = sample_weight , )","title":"jax_metrics.metrics.Mean"},{"location":"api/metrics/Mean/#jax_metrics.metrics.mean.Mean.__init__","text":"Creates a Mean instance. Parameters: Name Type Description Default kwargs Additional keyword arguments passed to Module. required Source code in jax_metrics/metrics/mean.py def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\"Creates a `Mean` instance. Arguments: kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( reduction = Reduction . weighted_mean , name = name , dtype = dtype , )","title":"__init__()"},{"location":"api/metrics/Mean/#jax_metrics.metrics.mean.Mean.update","text":"Accumulates the mean statistic over various batches. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. None Returns: Type Description ~M Mean instance with updated state. Source code in jax_metrics/metrics/mean.py def update ( self : M , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> M : \"\"\" Accumulates the mean statistic over various batches. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Returns: Mean instance with updated state. \"\"\" return super () . update ( values = values , sample_weight = sample_weight , )","title":"update()"},{"location":"api/metrics/MeanAbsoluteError/","text":"jax_metrics.metrics.MeanAbsoluteError Source code in jax_metrics/metrics/mean_absolute_error.py class MeanAbsoluteError ( Mean ): def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Absolute Error`_ (MAE): .. math:: \\text{MAE} = \\frac{1}{N}\\sum_i^N | y_i - \\hat{y_i} | Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from jax_metrics.metrics.mean_absolute_error import MeanAbsolutError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mae = MeanAbsolutError() >>> mae(preds, target) \"\"\" super () . __init__ ( name = name , dtype = dtype ) def update ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> tp . Any : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: MeanAbsoluteError instance with updated state \"\"\" values = _mean_absolute_error ( preds , target ) return super () . update ( values , sample_weight ) __init__ ( self , name = None , dtype = None ) special Computes Mean Absolute Error _ (MAE): .. math:: ext{MAE} = rac{1}{N}\\sum_i^N | y_i - \\hat{y_i} | Where :math: y is a tensor of target values, and :math: \\hat{y} is a tensor of predictions. Parameters: Name Type Description Default name Optional[str] Module name None dtype Optional[numpy.dtype] Metrics states initialization dtype None Examples: import jax.numpy as jnp from jax_metrics.metrics.mean_absolute_error import MeanAbsolutError target = jnp.array([3.0, -0.5, 2.0, 7.0]) preds = jnp.array([3.0, -0.5, 2.0, 7.0]) mae = MeanAbsolutError() mae(preds, target) Source code in jax_metrics/metrics/mean_absolute_error.py def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Absolute Error`_ (MAE): .. math:: \\text{MAE} = \\frac{1}{N}\\sum_i^N | y_i - \\hat{y_i} | Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from jax_metrics.metrics.mean_absolute_error import MeanAbsolutError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mae = MeanAbsolutError() >>> mae(preds, target) \"\"\" super () . __init__ ( name = name , dtype = dtype ) update ( self , target , preds , sample_weight = None , ** _ ) Accumulates metric statistics. target and preds should have the same shape. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. shape = [batch_size, d0, .. dN] None Returns: Type Description Any MeanAbsoluteError instance with updated state Source code in jax_metrics/metrics/mean_absolute_error.py def update ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> tp . Any : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: MeanAbsoluteError instance with updated state \"\"\" values = _mean_absolute_error ( preds , target ) return super () . update ( values , sample_weight )","title":"MeanAbsoluteError"},{"location":"api/metrics/MeanAbsoluteError/#jax_metricsmetricsmeanabsoluteerror","text":"Source code in jax_metrics/metrics/mean_absolute_error.py class MeanAbsoluteError ( Mean ): def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Absolute Error`_ (MAE): .. math:: \\text{MAE} = \\frac{1}{N}\\sum_i^N | y_i - \\hat{y_i} | Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from jax_metrics.metrics.mean_absolute_error import MeanAbsolutError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mae = MeanAbsolutError() >>> mae(preds, target) \"\"\" super () . __init__ ( name = name , dtype = dtype ) def update ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> tp . Any : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: MeanAbsoluteError instance with updated state \"\"\" values = _mean_absolute_error ( preds , target ) return super () . update ( values , sample_weight )","title":"jax_metrics.metrics.MeanAbsoluteError"},{"location":"api/metrics/MeanAbsoluteError/#jax_metrics.metrics.mean_absolute_error.MeanAbsoluteError.__init__","text":"Computes Mean Absolute Error _ (MAE): .. math:: ext{MAE} = rac{1}{N}\\sum_i^N | y_i - \\hat{y_i} | Where :math: y is a tensor of target values, and :math: \\hat{y} is a tensor of predictions. Parameters: Name Type Description Default name Optional[str] Module name None dtype Optional[numpy.dtype] Metrics states initialization dtype None Examples: import jax.numpy as jnp from jax_metrics.metrics.mean_absolute_error import MeanAbsolutError target = jnp.array([3.0, -0.5, 2.0, 7.0]) preds = jnp.array([3.0, -0.5, 2.0, 7.0]) mae = MeanAbsolutError() mae(preds, target) Source code in jax_metrics/metrics/mean_absolute_error.py def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Absolute Error`_ (MAE): .. math:: \\text{MAE} = \\frac{1}{N}\\sum_i^N | y_i - \\hat{y_i} | Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from jax_metrics.metrics.mean_absolute_error import MeanAbsolutError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mae = MeanAbsolutError() >>> mae(preds, target) \"\"\" super () . __init__ ( name = name , dtype = dtype )","title":"__init__()"},{"location":"api/metrics/MeanAbsoluteError/#jax_metrics.metrics.mean_absolute_error.MeanAbsoluteError.update","text":"Accumulates metric statistics. target and preds should have the same shape. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. shape = [batch_size, d0, .. dN] None Returns: Type Description Any MeanAbsoluteError instance with updated state Source code in jax_metrics/metrics/mean_absolute_error.py def update ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> tp . Any : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: MeanAbsoluteError instance with updated state \"\"\" values = _mean_absolute_error ( preds , target ) return super () . update ( values , sample_weight )","title":"update()"},{"location":"api/metrics/MeanSquareError/","text":"jax_metrics.metrics.MeanSquareError Source code in jax_metrics/metrics/mean_square_error.py class MeanSquareError ( Mean ): def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Square Error`_ (MSE): .. math:: \\text{MSE} = \\frac{1}{N}\\sum_i^N(y_i - \\hat{y_i})^2 Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from jax_metrics.metrics.mean_square_error import MeanSquareError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mse = MeanSquareError() >>> mse(preds, target) \"\"\" super () . __init__ ( name = name , dtype = dtype ) def update ( self : M , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> M : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: MeanSquareError instance with updated state \"\"\" values = _mean_square_error ( preds , target ) return super () . update ( values , sample_weight ) __init__ ( self , name = None , dtype = None ) special Computes Mean Square Error _ (MSE): .. math:: ext{MSE} = rac{1}{N}\\sum_i^N(y_i - \\hat{y_i})^2 Where :math: y is a tensor of target values, and :math: \\hat{y} is a tensor of predictions. Parameters: Name Type Description Default name Optional[str] Module name None dtype Optional[numpy.dtype] Metrics states initialization dtype None Examples: import jax.numpy as jnp from jax_metrics.metrics.mean_square_error import MeanSquareError target = jnp.array([3.0, -0.5, 2.0, 7.0]) preds = jnp.array([3.0, -0.5, 2.0, 7.0]) mse = MeanSquareError() mse(preds, target) Source code in jax_metrics/metrics/mean_square_error.py def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Square Error`_ (MSE): .. math:: \\text{MSE} = \\frac{1}{N}\\sum_i^N(y_i - \\hat{y_i})^2 Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from jax_metrics.metrics.mean_square_error import MeanSquareError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mse = MeanSquareError() >>> mse(preds, target) \"\"\" super () . __init__ ( name = name , dtype = dtype ) update ( self , target , preds , sample_weight = None , ** _ ) Accumulates metric statistics. target and preds should have the same shape. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. shape = [batch_size, d0, .. dN] None Returns: Type Description ~M MeanSquareError instance with updated state Source code in jax_metrics/metrics/mean_square_error.py def update ( self : M , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> M : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: MeanSquareError instance with updated state \"\"\" values = _mean_square_error ( preds , target ) return super () . update ( values , sample_weight )","title":"MeanSquareError"},{"location":"api/metrics/MeanSquareError/#jax_metricsmetricsmeansquareerror","text":"Source code in jax_metrics/metrics/mean_square_error.py class MeanSquareError ( Mean ): def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Square Error`_ (MSE): .. math:: \\text{MSE} = \\frac{1}{N}\\sum_i^N(y_i - \\hat{y_i})^2 Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from jax_metrics.metrics.mean_square_error import MeanSquareError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mse = MeanSquareError() >>> mse(preds, target) \"\"\" super () . __init__ ( name = name , dtype = dtype ) def update ( self : M , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> M : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: MeanSquareError instance with updated state \"\"\" values = _mean_square_error ( preds , target ) return super () . update ( values , sample_weight )","title":"jax_metrics.metrics.MeanSquareError"},{"location":"api/metrics/MeanSquareError/#jax_metrics.metrics.mean_square_error.MeanSquareError.__init__","text":"Computes Mean Square Error _ (MSE): .. math:: ext{MSE} = rac{1}{N}\\sum_i^N(y_i - \\hat{y_i})^2 Where :math: y is a tensor of target values, and :math: \\hat{y} is a tensor of predictions. Parameters: Name Type Description Default name Optional[str] Module name None dtype Optional[numpy.dtype] Metrics states initialization dtype None Examples: import jax.numpy as jnp from jax_metrics.metrics.mean_square_error import MeanSquareError target = jnp.array([3.0, -0.5, 2.0, 7.0]) preds = jnp.array([3.0, -0.5, 2.0, 7.0]) mse = MeanSquareError() mse(preds, target) Source code in jax_metrics/metrics/mean_square_error.py def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Square Error`_ (MSE): .. math:: \\text{MSE} = \\frac{1}{N}\\sum_i^N(y_i - \\hat{y_i})^2 Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from jax_metrics.metrics.mean_square_error import MeanSquareError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mse = MeanSquareError() >>> mse(preds, target) \"\"\" super () . __init__ ( name = name , dtype = dtype )","title":"__init__()"},{"location":"api/metrics/MeanSquareError/#jax_metrics.metrics.mean_square_error.MeanSquareError.update","text":"Accumulates metric statistics. target and preds should have the same shape. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. shape = [batch_size, d0, .. dN] None Returns: Type Description ~M MeanSquareError instance with updated state Source code in jax_metrics/metrics/mean_square_error.py def update ( self : M , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> M : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: MeanSquareError instance with updated state \"\"\" values = _mean_square_error ( preds , target ) return super () . update ( values , sample_weight )","title":"update()"},{"location":"api/metrics/Metric/","text":"jax_metrics.metrics.Metric Encapsulates metric logic and state. Metrics accumulate state between calls such that their output value reflect the metric as if calculated on the whole data given up to that point. Source code in jax_metrics/metrics/metric.py class Metric ( to . Tree , to . Copy , to . ToString , to . ToDict , to . Repr , to . Map , to . Immutable ): \"\"\" Encapsulates metric logic and state. Metrics accumulate state between calls such that their output value reflect the metric as if calculated on the whole data given up to that point. \"\"\" def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" name: name of the metric dtype: dtype of the metric \"\"\" self . name = name if name is not None else utils . _get_name ( self ) self . dtype = dtype if dtype is not None else jnp . float32 def __call__ ( self : M , ** kwargs ) -> tp . Tuple [ tp . Any , M ]: metric : M = self batch_updates = metric . batch_updates ( ** kwargs ) batch_values = batch_updates . compute () metric = metric . merge ( batch_updates ) return batch_values , metric def init ( self : M ) -> M : \"\"\" Initialize the metric's state. \"\"\" return self . reset () @abstractmethod def reset ( self : M ) -> M : \"\"\" Resets the metric state. Returns: Metric with the initial state \"\"\" ... @abstractmethod def update ( self : M , ** kwargs ) -> M : \"\"\" Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining `**kwargs`. Arguments: **kwargs: data to update the metric with Returns: Metric with updated state \"\"\" ... @abstractmethod def compute ( self ) -> tp . Any : \"\"\" Compute the current metric value. \"\"\" ... def compute_logs ( self ) -> tp . Dict [ str , jnp . ndarray ]: \"\"\" Compute the current metric value(s) and returns it/them in a `{metric_name: metric_value}` dictionary. Returns: A dictionary of metric values \"\"\" return { self . name : self . compute ()} def batch_updates ( self : M , ** kwargs ) -> M : \"\"\" Compute metric updates for a batch of data. Equivalent to `.reset().update(**kwargs)`. Arguments: kwargs: data to update the metric with Returns: Metric with updated state \"\"\" return self . reset () . update ( ** kwargs ) def aggregate ( self : M ) -> M : \"\"\" Aggregate metric state. It assumes the metric's internal state has an additional 'device' dimension on the 0th axis. Example: ```python batch_updates = metric.batch_updates(**kwargs) batch_updates = jax.lax.all_gather(batch_updates, axis_name=\"device\") batch_updates = batch_updates.aggregate() metric = metric.merge(batch_updates) ``` Returns: Metric with aggregated state \"\"\" return jax . tree_map ( lambda x : jnp . sum ( x , axis = 0 ), self ) def merge ( self : M , other : M ) -> M : \"\"\" Merge the state of two metrics of the same type. Usually used to merge a metric with its batch_updates. Example: ```python batch_updates = metric.batch_updates(**kwargs) metric = metric.merge(batch_updates) ``` \"\"\" stacked = jax . tree_map ( lambda * xs : jnp . stack ( xs ), self , other ) return stacked . aggregate () def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedMetric\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python metrics = jm.Metrics([ jm.metrics.Mean().index_into(values=[\"a\"]), jm.metrics.Mean().index_into(values=[\"b\"]), ]).reset() metrics = metrics.update(values={ \"a\": loss0, \"b\": loss1, }) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. This also works with Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedMetric instance \"\"\" return IndexedMetric ( self , kwargs ) def map_arg ( self , ** kwargs : str ) -> \"MapArgsMetric\" : \"\"\" Returns a metric that renames the keyword arguments expected by `.update()`. Example: ```python mean = jm.metrics.Mean().map_arg(values=\"loss\").reset() ... loss = loss_fn(x, y) mean = mean.update(loss=loss) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A MapArgsMetric instance \"\"\" return MapArgsMetric ( self , kwargs ) def _not_initialized_error ( self ): return ValueError ( f \"Metric ' { self . name } ' has not been initialized, call 'reset()' first\" ) __init__ ( self , name = None , dtype = None ) special name: name of the metric dtype: dtype of the metric Source code in jax_metrics/metrics/metric.py def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" name: name of the metric dtype: dtype of the metric \"\"\" self . name = name if name is not None else utils . _get_name ( self ) self . dtype = dtype if dtype is not None else jnp . float32 aggregate ( self ) Aggregate metric state. It assumes the metric's internal state has an additional 'device' dimension on the 0th axis. Examples: batch_updates = metric . batch_updates ( ** kwargs ) batch_updates = jax . lax . all_gather ( batch_updates , axis_name = \"device\" ) batch_updates = batch_updates . aggregate () metric = metric . merge ( batch_updates ) Returns: Type Description ~M Metric with aggregated state Source code in jax_metrics/metrics/metric.py def aggregate ( self : M ) -> M : \"\"\" Aggregate metric state. It assumes the metric's internal state has an additional 'device' dimension on the 0th axis. Example: ```python batch_updates = metric.batch_updates(**kwargs) batch_updates = jax.lax.all_gather(batch_updates, axis_name=\"device\") batch_updates = batch_updates.aggregate() metric = metric.merge(batch_updates) ``` Returns: Metric with aggregated state \"\"\" return jax . tree_map ( lambda x : jnp . sum ( x , axis = 0 ), self ) batch_updates ( self , ** kwargs ) Compute metric updates for a batch of data. Equivalent to .reset().update(**kwargs) . Parameters: Name Type Description Default kwargs data to update the metric with {} Returns: Type Description ~M Metric with updated state Source code in jax_metrics/metrics/metric.py def batch_updates ( self : M , ** kwargs ) -> M : \"\"\" Compute metric updates for a batch of data. Equivalent to `.reset().update(**kwargs)`. Arguments: kwargs: data to update the metric with Returns: Metric with updated state \"\"\" return self . reset () . update ( ** kwargs ) compute ( self ) Compute the current metric value. Source code in jax_metrics/metrics/metric.py @abstractmethod def compute ( self ) -> tp . Any : \"\"\" Compute the current metric value. \"\"\" ... compute_logs ( self ) Compute the current metric value(s) and returns it/them in a {metric_name: metric_value} dictionary. Returns: Type Description Dict[str, jax._src.numpy.lax_numpy.ndarray] A dictionary of metric values Source code in jax_metrics/metrics/metric.py def compute_logs ( self ) -> tp . Dict [ str , jnp . ndarray ]: \"\"\" Compute the current metric value(s) and returns it/them in a `{metric_name: metric_value}` dictionary. Returns: A dictionary of metric values \"\"\" return { self . name : self . compute ()} index_into ( self , ** kwargs ) Returns a metric that \"indexes\" the specified keyword arguments expected by .update() . You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing ( __getitem__ ). Examples: metrics = jm . Metrics ([ jm . metrics . Mean () . index_into ( values = [ \"a\" ]), jm . metrics . Mean () . index_into ( values = [ \"b\" ]), ]) . reset () metrics = metrics . update ( values = { \"a\" : loss0 , \"b\" : loss1 , }) Here values is set to a dict of arrays, but thanks to .index_into() each loss can index into its correspoding array. This also works with Parameters: Name Type Description Default **kwargs Union[str, int, Sequence[Union[str, int]]] keyword arguments to be indexed {} Returns: Type Description IndexedMetric A IndexedMetric instance Source code in jax_metrics/metrics/metric.py def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedMetric\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python metrics = jm.Metrics([ jm.metrics.Mean().index_into(values=[\"a\"]), jm.metrics.Mean().index_into(values=[\"b\"]), ]).reset() metrics = metrics.update(values={ \"a\": loss0, \"b\": loss1, }) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. This also works with Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedMetric instance \"\"\" return IndexedMetric ( self , kwargs ) init ( self ) Initialize the metric's state. Source code in jax_metrics/metrics/metric.py def init ( self : M ) -> M : \"\"\" Initialize the metric's state. \"\"\" return self . reset () map_arg ( self , ** kwargs ) Returns a metric that renames the keyword arguments expected by .update() . Examples: mean = jm . metrics . Mean () . map_arg ( values = \"loss\" ) . reset () ... loss = loss_fn ( x , y ) mean = mean . update ( loss = loss ) Parameters: Name Type Description Default **kwargs str keyword arguments to be renamed {} Returns: Type Description MapArgsMetric A MapArgsMetric instance Source code in jax_metrics/metrics/metric.py def map_arg ( self , ** kwargs : str ) -> \"MapArgsMetric\" : \"\"\" Returns a metric that renames the keyword arguments expected by `.update()`. Example: ```python mean = jm.metrics.Mean().map_arg(values=\"loss\").reset() ... loss = loss_fn(x, y) mean = mean.update(loss=loss) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A MapArgsMetric instance \"\"\" return MapArgsMetric ( self , kwargs ) merge ( self , other ) Merge the state of two metrics of the same type. Usually used to merge a metric with its batch_updates. Examples: batch_updates = metric . batch_updates ( ** kwargs ) metric = metric . merge ( batch_updates ) Source code in jax_metrics/metrics/metric.py def merge ( self : M , other : M ) -> M : \"\"\" Merge the state of two metrics of the same type. Usually used to merge a metric with its batch_updates. Example: ```python batch_updates = metric.batch_updates(**kwargs) metric = metric.merge(batch_updates) ``` \"\"\" stacked = jax . tree_map ( lambda * xs : jnp . stack ( xs ), self , other ) return stacked . aggregate () reset ( self ) Resets the metric state. Returns: Type Description ~M Metric with the initial state Source code in jax_metrics/metrics/metric.py @abstractmethod def reset ( self : M ) -> M : \"\"\" Resets the metric state. Returns: Metric with the initial state \"\"\" ... update ( self , ** kwargs ) Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining **kwargs . Parameters: Name Type Description Default **kwargs data to update the metric with {} Returns: Type Description ~M Metric with updated state Source code in jax_metrics/metrics/metric.py @abstractmethod def update ( self : M , ** kwargs ) -> M : \"\"\" Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining `**kwargs`. Arguments: **kwargs: data to update the metric with Returns: Metric with updated state \"\"\" ...","title":"Metric"},{"location":"api/metrics/Metric/#jax_metricsmetricsmetric","text":"Encapsulates metric logic and state. Metrics accumulate state between calls such that their output value reflect the metric as if calculated on the whole data given up to that point. Source code in jax_metrics/metrics/metric.py class Metric ( to . Tree , to . Copy , to . ToString , to . ToDict , to . Repr , to . Map , to . Immutable ): \"\"\" Encapsulates metric logic and state. Metrics accumulate state between calls such that their output value reflect the metric as if calculated on the whole data given up to that point. \"\"\" def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" name: name of the metric dtype: dtype of the metric \"\"\" self . name = name if name is not None else utils . _get_name ( self ) self . dtype = dtype if dtype is not None else jnp . float32 def __call__ ( self : M , ** kwargs ) -> tp . Tuple [ tp . Any , M ]: metric : M = self batch_updates = metric . batch_updates ( ** kwargs ) batch_values = batch_updates . compute () metric = metric . merge ( batch_updates ) return batch_values , metric def init ( self : M ) -> M : \"\"\" Initialize the metric's state. \"\"\" return self . reset () @abstractmethod def reset ( self : M ) -> M : \"\"\" Resets the metric state. Returns: Metric with the initial state \"\"\" ... @abstractmethod def update ( self : M , ** kwargs ) -> M : \"\"\" Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining `**kwargs`. Arguments: **kwargs: data to update the metric with Returns: Metric with updated state \"\"\" ... @abstractmethod def compute ( self ) -> tp . Any : \"\"\" Compute the current metric value. \"\"\" ... def compute_logs ( self ) -> tp . Dict [ str , jnp . ndarray ]: \"\"\" Compute the current metric value(s) and returns it/them in a `{metric_name: metric_value}` dictionary. Returns: A dictionary of metric values \"\"\" return { self . name : self . compute ()} def batch_updates ( self : M , ** kwargs ) -> M : \"\"\" Compute metric updates for a batch of data. Equivalent to `.reset().update(**kwargs)`. Arguments: kwargs: data to update the metric with Returns: Metric with updated state \"\"\" return self . reset () . update ( ** kwargs ) def aggregate ( self : M ) -> M : \"\"\" Aggregate metric state. It assumes the metric's internal state has an additional 'device' dimension on the 0th axis. Example: ```python batch_updates = metric.batch_updates(**kwargs) batch_updates = jax.lax.all_gather(batch_updates, axis_name=\"device\") batch_updates = batch_updates.aggregate() metric = metric.merge(batch_updates) ``` Returns: Metric with aggregated state \"\"\" return jax . tree_map ( lambda x : jnp . sum ( x , axis = 0 ), self ) def merge ( self : M , other : M ) -> M : \"\"\" Merge the state of two metrics of the same type. Usually used to merge a metric with its batch_updates. Example: ```python batch_updates = metric.batch_updates(**kwargs) metric = metric.merge(batch_updates) ``` \"\"\" stacked = jax . tree_map ( lambda * xs : jnp . stack ( xs ), self , other ) return stacked . aggregate () def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedMetric\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python metrics = jm.Metrics([ jm.metrics.Mean().index_into(values=[\"a\"]), jm.metrics.Mean().index_into(values=[\"b\"]), ]).reset() metrics = metrics.update(values={ \"a\": loss0, \"b\": loss1, }) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. This also works with Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedMetric instance \"\"\" return IndexedMetric ( self , kwargs ) def map_arg ( self , ** kwargs : str ) -> \"MapArgsMetric\" : \"\"\" Returns a metric that renames the keyword arguments expected by `.update()`. Example: ```python mean = jm.metrics.Mean().map_arg(values=\"loss\").reset() ... loss = loss_fn(x, y) mean = mean.update(loss=loss) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A MapArgsMetric instance \"\"\" return MapArgsMetric ( self , kwargs ) def _not_initialized_error ( self ): return ValueError ( f \"Metric ' { self . name } ' has not been initialized, call 'reset()' first\" )","title":"jax_metrics.metrics.Metric"},{"location":"api/metrics/Metric/#jax_metrics.metrics.metric.Metric.__init__","text":"name: name of the metric dtype: dtype of the metric Source code in jax_metrics/metrics/metric.py def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" name: name of the metric dtype: dtype of the metric \"\"\" self . name = name if name is not None else utils . _get_name ( self ) self . dtype = dtype if dtype is not None else jnp . float32","title":"__init__()"},{"location":"api/metrics/Metric/#jax_metrics.metrics.metric.Metric.aggregate","text":"Aggregate metric state. It assumes the metric's internal state has an additional 'device' dimension on the 0th axis. Examples: batch_updates = metric . batch_updates ( ** kwargs ) batch_updates = jax . lax . all_gather ( batch_updates , axis_name = \"device\" ) batch_updates = batch_updates . aggregate () metric = metric . merge ( batch_updates ) Returns: Type Description ~M Metric with aggregated state Source code in jax_metrics/metrics/metric.py def aggregate ( self : M ) -> M : \"\"\" Aggregate metric state. It assumes the metric's internal state has an additional 'device' dimension on the 0th axis. Example: ```python batch_updates = metric.batch_updates(**kwargs) batch_updates = jax.lax.all_gather(batch_updates, axis_name=\"device\") batch_updates = batch_updates.aggregate() metric = metric.merge(batch_updates) ``` Returns: Metric with aggregated state \"\"\" return jax . tree_map ( lambda x : jnp . sum ( x , axis = 0 ), self )","title":"aggregate()"},{"location":"api/metrics/Metric/#jax_metrics.metrics.metric.Metric.batch_updates","text":"Compute metric updates for a batch of data. Equivalent to .reset().update(**kwargs) . Parameters: Name Type Description Default kwargs data to update the metric with {} Returns: Type Description ~M Metric with updated state Source code in jax_metrics/metrics/metric.py def batch_updates ( self : M , ** kwargs ) -> M : \"\"\" Compute metric updates for a batch of data. Equivalent to `.reset().update(**kwargs)`. Arguments: kwargs: data to update the metric with Returns: Metric with updated state \"\"\" return self . reset () . update ( ** kwargs )","title":"batch_updates()"},{"location":"api/metrics/Metric/#jax_metrics.metrics.metric.Metric.compute","text":"Compute the current metric value. Source code in jax_metrics/metrics/metric.py @abstractmethod def compute ( self ) -> tp . Any : \"\"\" Compute the current metric value. \"\"\" ...","title":"compute()"},{"location":"api/metrics/Metric/#jax_metrics.metrics.metric.Metric.compute_logs","text":"Compute the current metric value(s) and returns it/them in a {metric_name: metric_value} dictionary. Returns: Type Description Dict[str, jax._src.numpy.lax_numpy.ndarray] A dictionary of metric values Source code in jax_metrics/metrics/metric.py def compute_logs ( self ) -> tp . Dict [ str , jnp . ndarray ]: \"\"\" Compute the current metric value(s) and returns it/them in a `{metric_name: metric_value}` dictionary. Returns: A dictionary of metric values \"\"\" return { self . name : self . compute ()}","title":"compute_logs()"},{"location":"api/metrics/Metric/#jax_metrics.metrics.metric.Metric.index_into","text":"Returns a metric that \"indexes\" the specified keyword arguments expected by .update() . You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing ( __getitem__ ). Examples: metrics = jm . Metrics ([ jm . metrics . Mean () . index_into ( values = [ \"a\" ]), jm . metrics . Mean () . index_into ( values = [ \"b\" ]), ]) . reset () metrics = metrics . update ( values = { \"a\" : loss0 , \"b\" : loss1 , }) Here values is set to a dict of arrays, but thanks to .index_into() each loss can index into its correspoding array. This also works with Parameters: Name Type Description Default **kwargs Union[str, int, Sequence[Union[str, int]]] keyword arguments to be indexed {} Returns: Type Description IndexedMetric A IndexedMetric instance Source code in jax_metrics/metrics/metric.py def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedMetric\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python metrics = jm.Metrics([ jm.metrics.Mean().index_into(values=[\"a\"]), jm.metrics.Mean().index_into(values=[\"b\"]), ]).reset() metrics = metrics.update(values={ \"a\": loss0, \"b\": loss1, }) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. This also works with Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedMetric instance \"\"\" return IndexedMetric ( self , kwargs )","title":"index_into()"},{"location":"api/metrics/Metric/#jax_metrics.metrics.metric.Metric.init","text":"Initialize the metric's state. Source code in jax_metrics/metrics/metric.py def init ( self : M ) -> M : \"\"\" Initialize the metric's state. \"\"\" return self . reset ()","title":"init()"},{"location":"api/metrics/Metric/#jax_metrics.metrics.metric.Metric.map_arg","text":"Returns a metric that renames the keyword arguments expected by .update() . Examples: mean = jm . metrics . Mean () . map_arg ( values = \"loss\" ) . reset () ... loss = loss_fn ( x , y ) mean = mean . update ( loss = loss ) Parameters: Name Type Description Default **kwargs str keyword arguments to be renamed {} Returns: Type Description MapArgsMetric A MapArgsMetric instance Source code in jax_metrics/metrics/metric.py def map_arg ( self , ** kwargs : str ) -> \"MapArgsMetric\" : \"\"\" Returns a metric that renames the keyword arguments expected by `.update()`. Example: ```python mean = jm.metrics.Mean().map_arg(values=\"loss\").reset() ... loss = loss_fn(x, y) mean = mean.update(loss=loss) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A MapArgsMetric instance \"\"\" return MapArgsMetric ( self , kwargs )","title":"map_arg()"},{"location":"api/metrics/Metric/#jax_metrics.metrics.metric.Metric.merge","text":"Merge the state of two metrics of the same type. Usually used to merge a metric with its batch_updates. Examples: batch_updates = metric . batch_updates ( ** kwargs ) metric = metric . merge ( batch_updates ) Source code in jax_metrics/metrics/metric.py def merge ( self : M , other : M ) -> M : \"\"\" Merge the state of two metrics of the same type. Usually used to merge a metric with its batch_updates. Example: ```python batch_updates = metric.batch_updates(**kwargs) metric = metric.merge(batch_updates) ``` \"\"\" stacked = jax . tree_map ( lambda * xs : jnp . stack ( xs ), self , other ) return stacked . aggregate ()","title":"merge()"},{"location":"api/metrics/Metric/#jax_metrics.metrics.metric.Metric.reset","text":"Resets the metric state. Returns: Type Description ~M Metric with the initial state Source code in jax_metrics/metrics/metric.py @abstractmethod def reset ( self : M ) -> M : \"\"\" Resets the metric state. Returns: Metric with the initial state \"\"\" ...","title":"reset()"},{"location":"api/metrics/Metric/#jax_metrics.metrics.metric.Metric.update","text":"Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining **kwargs . Parameters: Name Type Description Default **kwargs data to update the metric with {} Returns: Type Description ~M Metric with updated state Source code in jax_metrics/metrics/metric.py @abstractmethod def update ( self : M , ** kwargs ) -> M : \"\"\" Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining `**kwargs`. Arguments: **kwargs: data to update the metric with Returns: Metric with updated state \"\"\" ...","title":"update()"},{"location":"api/metrics/Metrics/","text":"jax_metrics.metrics.Metrics Source code in jax_metrics/metrics/metrics.py class Metrics ( Metric ): metrics : tp . Dict [ str , Metric ] def __init__ ( self , metrics : tp . Any , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): super () . __init__ ( name = name , dtype = dtype ) names : tp . Set [ str ] = set () def get_name ( path , metric , parent_iterable ): name = utils . _get_name ( metric ) if path : if parent_iterable : return f \" { path } / { name } \" else : return path else : return name self . metrics = { utils . _unique_name ( names , get_name ( path , metric , parent_iterable )): metric for path , metric , parent_iterable in utils . _flatten_names ( metrics ) } def reset ( self : M ) -> M : metrics = { name : metric . reset () for name , metric in self . metrics . items ()} return self . replace ( metrics = metrics ) def update ( self : M , ** kwargs ) -> M : \"\"\" Update all metrics with the given values. Each metric will receive the same keyword arguments but each can internally select the values to use. If a required value is not provided, the metric will raise a TypeError. Arguments: **kwargs: Keyword arguments to pass to each metric. Returns: Metrics instance with updated state. \"\"\" metrics = { name : metric . update ( ** kwargs ) for name , metric in self . metrics . items () } return self . replace ( metrics = metrics ) def compute ( self ) -> tp . Dict [ str , jnp . ndarray ]: outputs = {} names = set () for name , metric in self . metrics . items (): value = metric . compute () for path , value , parent_iterable in utils . _flatten_names ( value ): name = utils . _unique_name ( names , name ) if path : if parent_iterable : name = f \" { path } / { name } \" else : name = path outputs [ name ] = value return outputs def __call__ ( self : M , ** kwargs ) -> tp . Tuple [ tp . Dict [ str , jnp . ndarray ], M ]: return super () . __call__ ( ** kwargs ) def slice ( self , ** kwargs : types . IndexLike ) -> \"Metrics\" : metrics = { name : metric . index_into ( ** kwargs ) for name , metric in self . metrics . items () } return self . replace ( metrics = metrics ) compute ( self ) Compute the current metric value. Source code in jax_metrics/metrics/metrics.py def compute ( self ) -> tp . Dict [ str , jnp . ndarray ]: outputs = {} names = set () for name , metric in self . metrics . items (): value = metric . compute () for path , value , parent_iterable in utils . _flatten_names ( value ): name = utils . _unique_name ( names , name ) if path : if parent_iterable : name = f \" { path } / { name } \" else : name = path outputs [ name ] = value return outputs reset ( self ) Resets the metric state. Returns: Type Description ~M Metric with the initial state Source code in jax_metrics/metrics/metrics.py def reset ( self : M ) -> M : metrics = { name : metric . reset () for name , metric in self . metrics . items ()} return self . replace ( metrics = metrics ) update ( self , ** kwargs ) Update all metrics with the given values. Each metric will receive the same keyword arguments but each can internally select the values to use. If a required value is not provided, the metric will raise a TypeError. Parameters: Name Type Description Default **kwargs Keyword arguments to pass to each metric. {} Returns: Type Description ~M Metrics instance with updated state. Source code in jax_metrics/metrics/metrics.py def update ( self : M , ** kwargs ) -> M : \"\"\" Update all metrics with the given values. Each metric will receive the same keyword arguments but each can internally select the values to use. If a required value is not provided, the metric will raise a TypeError. Arguments: **kwargs: Keyword arguments to pass to each metric. Returns: Metrics instance with updated state. \"\"\" metrics = { name : metric . update ( ** kwargs ) for name , metric in self . metrics . items () } return self . replace ( metrics = metrics )","title":"Metrics"},{"location":"api/metrics/Metrics/#jax_metricsmetricsmetrics","text":"Source code in jax_metrics/metrics/metrics.py class Metrics ( Metric ): metrics : tp . Dict [ str , Metric ] def __init__ ( self , metrics : tp . Any , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): super () . __init__ ( name = name , dtype = dtype ) names : tp . Set [ str ] = set () def get_name ( path , metric , parent_iterable ): name = utils . _get_name ( metric ) if path : if parent_iterable : return f \" { path } / { name } \" else : return path else : return name self . metrics = { utils . _unique_name ( names , get_name ( path , metric , parent_iterable )): metric for path , metric , parent_iterable in utils . _flatten_names ( metrics ) } def reset ( self : M ) -> M : metrics = { name : metric . reset () for name , metric in self . metrics . items ()} return self . replace ( metrics = metrics ) def update ( self : M , ** kwargs ) -> M : \"\"\" Update all metrics with the given values. Each metric will receive the same keyword arguments but each can internally select the values to use. If a required value is not provided, the metric will raise a TypeError. Arguments: **kwargs: Keyword arguments to pass to each metric. Returns: Metrics instance with updated state. \"\"\" metrics = { name : metric . update ( ** kwargs ) for name , metric in self . metrics . items () } return self . replace ( metrics = metrics ) def compute ( self ) -> tp . Dict [ str , jnp . ndarray ]: outputs = {} names = set () for name , metric in self . metrics . items (): value = metric . compute () for path , value , parent_iterable in utils . _flatten_names ( value ): name = utils . _unique_name ( names , name ) if path : if parent_iterable : name = f \" { path } / { name } \" else : name = path outputs [ name ] = value return outputs def __call__ ( self : M , ** kwargs ) -> tp . Tuple [ tp . Dict [ str , jnp . ndarray ], M ]: return super () . __call__ ( ** kwargs ) def slice ( self , ** kwargs : types . IndexLike ) -> \"Metrics\" : metrics = { name : metric . index_into ( ** kwargs ) for name , metric in self . metrics . items () } return self . replace ( metrics = metrics )","title":"jax_metrics.metrics.Metrics"},{"location":"api/metrics/Metrics/#jax_metrics.metrics.metrics.Metrics.compute","text":"Compute the current metric value. Source code in jax_metrics/metrics/metrics.py def compute ( self ) -> tp . Dict [ str , jnp . ndarray ]: outputs = {} names = set () for name , metric in self . metrics . items (): value = metric . compute () for path , value , parent_iterable in utils . _flatten_names ( value ): name = utils . _unique_name ( names , name ) if path : if parent_iterable : name = f \" { path } / { name } \" else : name = path outputs [ name ] = value return outputs","title":"compute()"},{"location":"api/metrics/Metrics/#jax_metrics.metrics.metrics.Metrics.reset","text":"Resets the metric state. Returns: Type Description ~M Metric with the initial state Source code in jax_metrics/metrics/metrics.py def reset ( self : M ) -> M : metrics = { name : metric . reset () for name , metric in self . metrics . items ()} return self . replace ( metrics = metrics )","title":"reset()"},{"location":"api/metrics/Metrics/#jax_metrics.metrics.metrics.Metrics.update","text":"Update all metrics with the given values. Each metric will receive the same keyword arguments but each can internally select the values to use. If a required value is not provided, the metric will raise a TypeError. Parameters: Name Type Description Default **kwargs Keyword arguments to pass to each metric. {} Returns: Type Description ~M Metrics instance with updated state. Source code in jax_metrics/metrics/metrics.py def update ( self : M , ** kwargs ) -> M : \"\"\" Update all metrics with the given values. Each metric will receive the same keyword arguments but each can internally select the values to use. If a required value is not provided, the metric will raise a TypeError. Arguments: **kwargs: Keyword arguments to pass to each metric. Returns: Metrics instance with updated state. \"\"\" metrics = { name : metric . update ( ** kwargs ) for name , metric in self . metrics . items () } return self . replace ( metrics = metrics )","title":"update()"},{"location":"api/metrics/Reduce/","text":"jax_metrics.metrics.Reduce Encapsulates metrics that perform a reduce operation on the values. Source code in jax_metrics/metrics/reduce.py class Reduce ( Metric ): \"\"\"Encapsulates metrics that perform a reduce operation on the values.\"\"\" total : tp . Optional [ jnp . ndarray ] = to . node () count : tp . Optional [ jnp . ndarray ] = to . node () def __init__ ( self , reduction : tp . Union [ Reduction , str ], name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): super () . __init__ ( name = name , dtype = dtype ) self . reduction = ( reduction if isinstance ( reduction , Reduction ) else Reduction [ reduction ] ) self . total = None self . count = None def reset ( self : M ) -> M : # initialize states total = jnp . array ( 0.0 , dtype = self . dtype ) if self . reduction in ( Reduction . sum_over_batch_size , Reduction . weighted_mean , ): count = jnp . array ( 0 , dtype = jnp . uint32 ) else : count = None return self . replace ( total = total , count = count ) def update ( self : M , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> M : \"\"\" Accumulates statistics for computing the reduction metric. For example, if `values` is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of `result()` is 4. If the `sample_weight` is specified as [1, 1, 0, 0] then value of `result()` would be 2. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Defaults to 1. Returns: Array with the cumulative reduce. \"\"\" if self . total is None : raise self . _not_initialized_error () # perform update if sample_weight is not None : if sample_weight . ndim > values . ndim : raise Exception ( f \"sample_weight dimention is higher than values, when masking values sample_weight dimention needs to be equal or lower than values dimension, currently values have shape equal to { values . shape } \" ) try : # Broadcast weights if possible. sample_weight = jnp . broadcast_to ( sample_weight , values . shape ) except ValueError : # Reduce values to same ndim as weight array values_ndim , weight_ndim = values . ndim , sample_weight . ndim if self . reduction == Reduction . sum : values = jnp . sum ( values , axis = list ( range ( weight_ndim , values_ndim ))) else : values = jnp . mean ( values , axis = list ( range ( weight_ndim , values_ndim )) ) values = values * sample_weight value_sum = jnp . sum ( values ) total = ( self . total + value_sum ) . astype ( self . total . dtype ) # Exit early if the reduction doesn't have a denominator. if self . reduction == Reduction . sum : num_values = None # Update `count` for reductions that require a denominator. elif self . reduction == Reduction . sum_over_batch_size : num_values = np . prod ( values . shape ) else : if sample_weight is None : num_values = np . prod ( values . shape ) else : num_values = jnp . sum ( sample_weight ) if self . count is not None : assert num_values is not None count = ( self . count + num_values ) . astype ( self . count . dtype ) else : count = None return self . replace ( total = total , count = count ) def compute ( self ) -> jnp . ndarray : if self . total is None : raise self . _not_initialized_error () if self . reduction == Reduction . sum : return self . total else : return self . total / self . count compute ( self ) Compute the current metric value. Source code in jax_metrics/metrics/reduce.py def compute ( self ) -> jnp . ndarray : if self . total is None : raise self . _not_initialized_error () if self . reduction == Reduction . sum : return self . total else : return self . total / self . count reset ( self ) Resets the metric state. Returns: Type Description ~M Metric with the initial state Source code in jax_metrics/metrics/reduce.py def reset ( self : M ) -> M : # initialize states total = jnp . array ( 0.0 , dtype = self . dtype ) if self . reduction in ( Reduction . sum_over_batch_size , Reduction . weighted_mean , ): count = jnp . array ( 0 , dtype = jnp . uint32 ) else : count = None return self . replace ( total = total , count = count ) update ( self , values , sample_weight = None , ** _ ) Accumulates statistics for computing the reduction metric. For example, if values is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of result() is 4. If the sample_weight is specified as [1, 1, 0, 0] then value of result() would be 2. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. None Returns: Type Description ~M Array with the cumulative reduce. Source code in jax_metrics/metrics/reduce.py def update ( self : M , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> M : \"\"\" Accumulates statistics for computing the reduction metric. For example, if `values` is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of `result()` is 4. If the `sample_weight` is specified as [1, 1, 0, 0] then value of `result()` would be 2. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Defaults to 1. Returns: Array with the cumulative reduce. \"\"\" if self . total is None : raise self . _not_initialized_error () # perform update if sample_weight is not None : if sample_weight . ndim > values . ndim : raise Exception ( f \"sample_weight dimention is higher than values, when masking values sample_weight dimention needs to be equal or lower than values dimension, currently values have shape equal to { values . shape } \" ) try : # Broadcast weights if possible. sample_weight = jnp . broadcast_to ( sample_weight , values . shape ) except ValueError : # Reduce values to same ndim as weight array values_ndim , weight_ndim = values . ndim , sample_weight . ndim if self . reduction == Reduction . sum : values = jnp . sum ( values , axis = list ( range ( weight_ndim , values_ndim ))) else : values = jnp . mean ( values , axis = list ( range ( weight_ndim , values_ndim )) ) values = values * sample_weight value_sum = jnp . sum ( values ) total = ( self . total + value_sum ) . astype ( self . total . dtype ) # Exit early if the reduction doesn't have a denominator. if self . reduction == Reduction . sum : num_values = None # Update `count` for reductions that require a denominator. elif self . reduction == Reduction . sum_over_batch_size : num_values = np . prod ( values . shape ) else : if sample_weight is None : num_values = np . prod ( values . shape ) else : num_values = jnp . sum ( sample_weight ) if self . count is not None : assert num_values is not None count = ( self . count + num_values ) . astype ( self . count . dtype ) else : count = None return self . replace ( total = total , count = count )","title":"Reduce"},{"location":"api/metrics/Reduce/#jax_metricsmetricsreduce","text":"Encapsulates metrics that perform a reduce operation on the values. Source code in jax_metrics/metrics/reduce.py class Reduce ( Metric ): \"\"\"Encapsulates metrics that perform a reduce operation on the values.\"\"\" total : tp . Optional [ jnp . ndarray ] = to . node () count : tp . Optional [ jnp . ndarray ] = to . node () def __init__ ( self , reduction : tp . Union [ Reduction , str ], name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): super () . __init__ ( name = name , dtype = dtype ) self . reduction = ( reduction if isinstance ( reduction , Reduction ) else Reduction [ reduction ] ) self . total = None self . count = None def reset ( self : M ) -> M : # initialize states total = jnp . array ( 0.0 , dtype = self . dtype ) if self . reduction in ( Reduction . sum_over_batch_size , Reduction . weighted_mean , ): count = jnp . array ( 0 , dtype = jnp . uint32 ) else : count = None return self . replace ( total = total , count = count ) def update ( self : M , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> M : \"\"\" Accumulates statistics for computing the reduction metric. For example, if `values` is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of `result()` is 4. If the `sample_weight` is specified as [1, 1, 0, 0] then value of `result()` would be 2. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Defaults to 1. Returns: Array with the cumulative reduce. \"\"\" if self . total is None : raise self . _not_initialized_error () # perform update if sample_weight is not None : if sample_weight . ndim > values . ndim : raise Exception ( f \"sample_weight dimention is higher than values, when masking values sample_weight dimention needs to be equal or lower than values dimension, currently values have shape equal to { values . shape } \" ) try : # Broadcast weights if possible. sample_weight = jnp . broadcast_to ( sample_weight , values . shape ) except ValueError : # Reduce values to same ndim as weight array values_ndim , weight_ndim = values . ndim , sample_weight . ndim if self . reduction == Reduction . sum : values = jnp . sum ( values , axis = list ( range ( weight_ndim , values_ndim ))) else : values = jnp . mean ( values , axis = list ( range ( weight_ndim , values_ndim )) ) values = values * sample_weight value_sum = jnp . sum ( values ) total = ( self . total + value_sum ) . astype ( self . total . dtype ) # Exit early if the reduction doesn't have a denominator. if self . reduction == Reduction . sum : num_values = None # Update `count` for reductions that require a denominator. elif self . reduction == Reduction . sum_over_batch_size : num_values = np . prod ( values . shape ) else : if sample_weight is None : num_values = np . prod ( values . shape ) else : num_values = jnp . sum ( sample_weight ) if self . count is not None : assert num_values is not None count = ( self . count + num_values ) . astype ( self . count . dtype ) else : count = None return self . replace ( total = total , count = count ) def compute ( self ) -> jnp . ndarray : if self . total is None : raise self . _not_initialized_error () if self . reduction == Reduction . sum : return self . total else : return self . total / self . count","title":"jax_metrics.metrics.Reduce"},{"location":"api/metrics/Reduce/#jax_metrics.metrics.reduce.Reduce.compute","text":"Compute the current metric value. Source code in jax_metrics/metrics/reduce.py def compute ( self ) -> jnp . ndarray : if self . total is None : raise self . _not_initialized_error () if self . reduction == Reduction . sum : return self . total else : return self . total / self . count","title":"compute()"},{"location":"api/metrics/Reduce/#jax_metrics.metrics.reduce.Reduce.reset","text":"Resets the metric state. Returns: Type Description ~M Metric with the initial state Source code in jax_metrics/metrics/reduce.py def reset ( self : M ) -> M : # initialize states total = jnp . array ( 0.0 , dtype = self . dtype ) if self . reduction in ( Reduction . sum_over_batch_size , Reduction . weighted_mean , ): count = jnp . array ( 0 , dtype = jnp . uint32 ) else : count = None return self . replace ( total = total , count = count )","title":"reset()"},{"location":"api/metrics/Reduce/#jax_metrics.metrics.reduce.Reduce.update","text":"Accumulates statistics for computing the reduction metric. For example, if values is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of result() is 4. If the sample_weight is specified as [1, 1, 0, 0] then value of result() would be 2. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. None Returns: Type Description ~M Array with the cumulative reduce. Source code in jax_metrics/metrics/reduce.py def update ( self : M , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ** _ , ) -> M : \"\"\" Accumulates statistics for computing the reduction metric. For example, if `values` is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of `result()` is 4. If the `sample_weight` is specified as [1, 1, 0, 0] then value of `result()` would be 2. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Defaults to 1. Returns: Array with the cumulative reduce. \"\"\" if self . total is None : raise self . _not_initialized_error () # perform update if sample_weight is not None : if sample_weight . ndim > values . ndim : raise Exception ( f \"sample_weight dimention is higher than values, when masking values sample_weight dimention needs to be equal or lower than values dimension, currently values have shape equal to { values . shape } \" ) try : # Broadcast weights if possible. sample_weight = jnp . broadcast_to ( sample_weight , values . shape ) except ValueError : # Reduce values to same ndim as weight array values_ndim , weight_ndim = values . ndim , sample_weight . ndim if self . reduction == Reduction . sum : values = jnp . sum ( values , axis = list ( range ( weight_ndim , values_ndim ))) else : values = jnp . mean ( values , axis = list ( range ( weight_ndim , values_ndim )) ) values = values * sample_weight value_sum = jnp . sum ( values ) total = ( self . total + value_sum ) . astype ( self . total . dtype ) # Exit early if the reduction doesn't have a denominator. if self . reduction == Reduction . sum : num_values = None # Update `count` for reductions that require a denominator. elif self . reduction == Reduction . sum_over_batch_size : num_values = np . prod ( values . shape ) else : if sample_weight is None : num_values = np . prod ( values . shape ) else : num_values = jnp . sum ( sample_weight ) if self . count is not None : assert num_values is not None count = ( self . count + num_values ) . astype ( self . count . dtype ) else : count = None return self . replace ( total = total , count = count )","title":"update()"},{"location":"api/metrics/Reduction/","text":"jax_metrics.metrics.Reduction An enumeration. Source code in jax_metrics/metrics/reduce.py class Reduction ( enum . Enum ): sum = enum . auto () sum_over_batch_size = enum . auto () weighted_mean = enum . auto ()","title":"Reduction"},{"location":"api/metrics/Reduction/#jax_metricsmetricsreduction","text":"An enumeration. Source code in jax_metrics/metrics/reduce.py class Reduction ( enum . Enum ): sum = enum . auto () sum_over_batch_size = enum . auto () weighted_mean = enum . auto ()","title":"jax_metrics.metrics.Reduction"},{"location":"api/regularizers/L1/","text":"jax_metrics.regularizers.L1 Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| Source code in jax_metrics/regularizers/l1l2.py class L1 ( L1L2 ): r \"\"\" Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: $$\\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i|$$ \"\"\" def __init__ ( self , l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , name : tp . Optional [ str ] = None , ): super () . __init__ ( l1 = l , reduction = reduction , weight = weight , name = name )","title":"L1"},{"location":"api/regularizers/L1/#jax_metricsregularizersl1","text":"Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| Source code in jax_metrics/regularizers/l1l2.py class L1 ( L1L2 ): r \"\"\" Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: $$\\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i|$$ \"\"\" def __init__ ( self , l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , name : tp . Optional [ str ] = None , ): super () . __init__ ( l1 = l , reduction = reduction , weight = weight , name = name )","title":"jax_metrics.regularizers.L1"},{"location":"api/regularizers/L1L2/","text":"jax_metrics.regularizers.L1L2 A regularizer that applies both L1 and L2 regularization penalties. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| The L2 regularization penalty is computed as \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 Source code in jax_metrics/regularizers/l1l2.py class L1L2 ( Loss ): r \"\"\" A regularizer that applies both L1 and L2 regularization penalties. The L1 regularization penalty is computed as: $$ \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| $$ The L2 regularization penalty is computed as $$\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2$$ \"\"\" def __init__ ( self , l1 = 0.0 , l2 = 0.0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , name : tp . Optional [ str ] = None , ): # pylint: disable=redefined-outer-name super () . __init__ ( reduction = reduction , weight = weight , name = name ) self . l1 = l1 self . l2 = l2 def call ( self , parameters : tp . Any , ** _ ) -> jnp . ndarray : \"\"\" Computes the L1 and L2 regularization penalty simultaneously. Arguments: net_params: A structure with all the parameters of the model. \"\"\" regularization : jnp . ndarray = jnp . array ( 0.0 ) if not self . l1 and not self . l2 : return regularization leaves = jax . tree_leaves ( parameters ) if self . l1 : regularization += self . l1 * sum ( jnp . sum ( jnp . abs ( p )) for p in leaves ) if self . l2 : regularization += self . l2 * sum ( jnp . sum ( jnp . square ( p )) for p in leaves ) return regularization call ( self , parameters , ** _ ) Computes the L1 and L2 regularization penalty simultaneously. Parameters: Name Type Description Default net_params A structure with all the parameters of the model. required Source code in jax_metrics/regularizers/l1l2.py def call ( self , parameters : tp . Any , ** _ ) -> jnp . ndarray : \"\"\" Computes the L1 and L2 regularization penalty simultaneously. Arguments: net_params: A structure with all the parameters of the model. \"\"\" regularization : jnp . ndarray = jnp . array ( 0.0 ) if not self . l1 and not self . l2 : return regularization leaves = jax . tree_leaves ( parameters ) if self . l1 : regularization += self . l1 * sum ( jnp . sum ( jnp . abs ( p )) for p in leaves ) if self . l2 : regularization += self . l2 * sum ( jnp . sum ( jnp . square ( p )) for p in leaves ) return regularization","title":"L1L2"},{"location":"api/regularizers/L1L2/#jax_metricsregularizersl1l2","text":"A regularizer that applies both L1 and L2 regularization penalties. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| The L2 regularization penalty is computed as \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 Source code in jax_metrics/regularizers/l1l2.py class L1L2 ( Loss ): r \"\"\" A regularizer that applies both L1 and L2 regularization penalties. The L1 regularization penalty is computed as: $$ \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| $$ The L2 regularization penalty is computed as $$\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2$$ \"\"\" def __init__ ( self , l1 = 0.0 , l2 = 0.0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , name : tp . Optional [ str ] = None , ): # pylint: disable=redefined-outer-name super () . __init__ ( reduction = reduction , weight = weight , name = name ) self . l1 = l1 self . l2 = l2 def call ( self , parameters : tp . Any , ** _ ) -> jnp . ndarray : \"\"\" Computes the L1 and L2 regularization penalty simultaneously. Arguments: net_params: A structure with all the parameters of the model. \"\"\" regularization : jnp . ndarray = jnp . array ( 0.0 ) if not self . l1 and not self . l2 : return regularization leaves = jax . tree_leaves ( parameters ) if self . l1 : regularization += self . l1 * sum ( jnp . sum ( jnp . abs ( p )) for p in leaves ) if self . l2 : regularization += self . l2 * sum ( jnp . sum ( jnp . square ( p )) for p in leaves ) return regularization","title":"jax_metrics.regularizers.L1L2"},{"location":"api/regularizers/L1L2/#jax_metrics.regularizers.l1l2.L1L2.call","text":"Computes the L1 and L2 regularization penalty simultaneously. Parameters: Name Type Description Default net_params A structure with all the parameters of the model. required Source code in jax_metrics/regularizers/l1l2.py def call ( self , parameters : tp . Any , ** _ ) -> jnp . ndarray : \"\"\" Computes the L1 and L2 regularization penalty simultaneously. Arguments: net_params: A structure with all the parameters of the model. \"\"\" regularization : jnp . ndarray = jnp . array ( 0.0 ) if not self . l1 and not self . l2 : return regularization leaves = jax . tree_leaves ( parameters ) if self . l1 : regularization += self . l1 * sum ( jnp . sum ( jnp . abs ( p )) for p in leaves ) if self . l2 : regularization += self . l2 * sum ( jnp . sum ( jnp . square ( p )) for p in leaves ) return regularization","title":"call()"},{"location":"api/regularizers/L2/","text":"jax_metrics.regularizers.L2 Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 <span class=\"arithmatex\"><span class=\"MathJax_Preview\">\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2</span><script type=\"math/tex\">\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 ``` Source code in jax_metrics/regularizers/l1l2.py class L2 ( L1L2 ): r \"\"\" Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: $$\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2$$ ``` \"\"\" def __init__ ( self , l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , name : tp . Optional [ str ] = None , ): super () . __init__ ( l2 = l , reduction = reduction , weight = weight , name = name )","title":"L2"},{"location":"api/regularizers/L2/#jax_metricsregularizersl2","text":"Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 <span class=\"arithmatex\"><span class=\"MathJax_Preview\">\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2</span><script type=\"math/tex\">\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 ``` Source code in jax_metrics/regularizers/l1l2.py class L2 ( L1L2 ): r \"\"\" Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: $$\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2$$ ``` \"\"\" def __init__ ( self , l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , name : tp . Optional [ str ] = None , ): super () . __init__ ( l2 = l , reduction = reduction , weight = weight , name = name )","title":"jax_metrics.regularizers.L2"}]}