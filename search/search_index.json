{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"JAX Metrics A Metrics library for the JAX ecosystem Main Features Standard metrics that can be used in any JAX project. Pytree abstractions that can natively integrate with all JAX APIs and pytree-supporting frameworks (flax.struct, equinox, treex, etc). Distributed-friendly APIs that make it super easy to synchronize metrics across devices. Automatic accumulation over epochs. JAX Metrics is implemented on top of Treeo . What is included? The Keras-like Loss and Metric abstractions. A metrics module containing popular metrics. The losses and regularizers modules containing popular losses. The Metrics and Losses combinators. Installation Install using pip: pip install jax_metrics Status Metrics on this library are usually tested against their Keras or Torchmetrics counterparts for numerical equivalence. This code base comes from Treex and Elegy so it's already in use. Getting Started Metric The Metric API consists of 3 basic methods: reset : Used to both initialize and reset a metric. update : Takes in new data and updates the metric state. compute : Returns the current value of the metric. Simple usage looks like this: import jax_metrics as jm metric = jm . metrics . Accuracy () # Update the metric with a batch of predictions and labels metric = metric . update ( target = y , preds = logits ) # Get the current value of the metric acc = metric . compute () # 0.95 # alternatively, produce a logs dict logs = metric . compute_logs () # {'accuracy': 0.95} # Reset the metric metric = metric . reset () Note that update enforces the use of keyword arguments. Also the Metric.name property is used as the key in the returned dict, by default this is the name of the class in lowercase but can be overridden in the constructor via the name argument. Tipical Training Setup Because Metrics are pytrees they can be used with jit , pmap , etc. On a more realistic scenario you will proably want to use them inside some of your JAX functions in a setup similar to this: import jax_metrics as jm metric = jm . metrics . Accuracy () def loss_fn ( params , metric , x , y ): ... metric = metric . update ( target = y , preds = logits ) ... return loss , metric @jax . jit def train_step ( params , metric , x , y ): grads , metric = jax . grad ( loss_fn , has_aux = True )( params , metric , x , y ) ... return params , metric @jax . jit def reset_step ( metric : jm . Metric ) -> jm . Metric : return metric . reset () Since the loss function usually has access to the predictions and labels, its usually where you would call metric.update , and the new metric state can be returned as an auxiliary output. Distributed Training JAX Metrics has a distributed friendly API via the batch_updates and reduce methods. A simple example of a loss function inside a data parallel setup could look like this: def loss_fn ( params , metric , x , y ): ... # compuate batch update batch_updates = metric . batch_updates ( target = y , preds = logits ) # gather over all devices and reduce batch_updates = jax . lax . all_gather ( batch_updates , \"device\" ) . reduce () # update metric metric = metric . merge ( batch_updates ) ... The batch_updates method behaves similar to update but returns a new metric state with only information about that batch, jax.lax.all_gather \"gathers\" the metric state over all devices plus adds a new axis to the metric state, and reduce reduces the metric state over all devices (first axis). Finally, merge combines the accumulated metric state over the previous batches with the batch updates. Loss The Loss API just consists of a __call__ method. Simple usage looks like this: import jax_metrics as jm crossentropy = jm . losses . Crossentropy () # get reduced loss value loss = crossentropy ( target = y , preds = logits ) # 0.23 Note that losses are not pytrees so they should be marked as static. Similar to Keras, all losses have a reduction strategy that can be specified in the constructor and (usually) makes sure that the output is a scalar. Why have losses in a metrics library? There are a few reasons for having losses in a metrics library: 1. Most code from this library was originally written for and will still be consumed by Elegy. Since Elegy needs support for calculating cumulative losses, as you will see later, a Metric abstraction called `Losses` was created for this. 2. A couple of API design decisions are shared between the `Loss` and `Metric` APIs. This includes: * `__call__` and `update` both accept any number keyword only arguments. This is used to facilitate composition (see [Combinators](#combinators) section). * Both classes have the `index_into` and `rename_arguments` methods that allow them to modify how arguments are consumed. * Argument names are standardized to be consistent when ever possible, e.g. both `metrics.Accuracy` and `losses.Crossentropy` use the `target` and `preds` arguments. Combinators Combinators enable you to group together multiple metrics while also being instances of Metric and thus maintaining the same API. Metrics The Metrics combinator lets you combine multiple metrics into a single Metric object. metrics = jm . Metrics ([ jm . metrics . Accuracy (), jm . metrics . F1 (), # not yet implemented \ud83d\ude05, coming soon? ]) # same API metrics = metrics . update ( target = y , preds = logits ) # compute now returns a dict metrics . compute () # {'accuracy': 0.95, 'f1': 0.87} # same as compute_logs in the case metrics . compute_logs () # {'accuracy': 0.95, 'f1': 0.87} # Reset the metrics metrics = metrics . reset () As you can see the Metrics.update method accepts and forwards all the arguments required by the individual metrics. In this example they use the same arguments, but in practice they may consume different subsets of the arguments. Also, if names are repeated then unique names are generated for each metric by appending a number to the metric name. If a dictionary is used instead of a list, the keys are used instead of the name property of the metrics to determine the key in the returned dict. metrics = jm . Metrics ({ \"acc\" : jm . metrics . Accuracy (), \"f_one\" : jm . metrics . F1 (), # not yet implemented \ud83d\ude05, coming soon? }) # same API metrics = metrics . update ( target = y , preds = logits ) # compute new returns a dict metrics . compute () # {'acc': 0.95, 'f_one': 0.87} # same as compute_logs in the case metrics . compute_logs () # {'acc': 0.95, 'f_one': 0.87} # Reset the metrics metrics = metrics . reset () You can use nested structures of dicts and lists to group metrics, the keys of the dicts are used to determine group names. Group names and metrics names are concatenated using \"/\" e.g. \"{group_name}/{metric_name}\" . Losses Losses is a Metric combinator that behaves very similarly to Metrics but contains Loss instances. Losses calculates the cumulative mean value of each loss over the batches. losses = jm . Losses ([ jm . losses . Crossentropy (), jm . regularizers . L2 ( 1e-4 ), ]) # same API losses = losses . update ( target = y , preds = logits , parameters = params ) # compute new returns a dict losses . compute () # {'crossentropy': 0.23, 'l2': 0.005} # same as compute_logs in the case losses . compute_logs () # {'crossentropy': 0.23, 'l2': 0.005} # you can also compute the total loss loss = losses . total_loss () # 0.235 # Reset the losses losses = losses . reset () As with Metrics , the update method accepts and forwards all the arguments required by the individual losses. In this example target and preds are used by the Crossentropy , while parameters is used by the L2 . The total_loss method returns the sum of all values returned by compute . If a dictionary is used instead of a list, the keys are used instead of the name property of the losses to determine the key in the returned dict. losses = jm . Losses ({ \"xent\" : jm . losses . Crossentropy (), \"l2\" : jm . regularizers . L2 ( 1e-4 ), }) # same API losses = losses . update ( target = y , preds = logits , parameters = params ) # compute new returns a dict losses . compute () # {'xent': 0.23, 'l2': 0.005} # same as compute_logs in the case losses . compute_logs () # {'xent': 0.23, 'l2': 0.005} # you can also compute the total loss loss = losses . total_loss () # 0.235 # Reset the losses losses = losses . reset () If you want to use Losses to calculate the loss of a model, you should use batch_updates followed by total_loss to get the correct batch loss. For example, a loss function could be written as: def loss_fn ( ... , losses ): ... batch_updates = losses . batch_updates ( target = y , preds = logits , parameters = params ) loss = batch_updates . total_loss () losses = losses . merge ( batch_updates ) ... return loss , losses For convenience, the previous pattern can be simplified to a single line using the loss_and_update method: def loss_fn ( ... ): ... loss , lossses = losses . loss_and_update ( target = y , preds = logits , parameters = params ) ... return loss , losses","title":"JAX Metrics"},{"location":"#jax-metrics","text":"A Metrics library for the JAX ecosystem","title":"JAX Metrics"},{"location":"#main-features","text":"Standard metrics that can be used in any JAX project. Pytree abstractions that can natively integrate with all JAX APIs and pytree-supporting frameworks (flax.struct, equinox, treex, etc). Distributed-friendly APIs that make it super easy to synchronize metrics across devices. Automatic accumulation over epochs. JAX Metrics is implemented on top of Treeo .","title":"Main Features"},{"location":"#what-is-included","text":"The Keras-like Loss and Metric abstractions. A metrics module containing popular metrics. The losses and regularizers modules containing popular losses. The Metrics and Losses combinators.","title":"What is included?"},{"location":"#installation","text":"Install using pip: pip install jax_metrics","title":"Installation"},{"location":"#status","text":"Metrics on this library are usually tested against their Keras or Torchmetrics counterparts for numerical equivalence. This code base comes from Treex and Elegy so it's already in use.","title":"Status"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#metric","text":"The Metric API consists of 3 basic methods: reset : Used to both initialize and reset a metric. update : Takes in new data and updates the metric state. compute : Returns the current value of the metric. Simple usage looks like this: import jax_metrics as jm metric = jm . metrics . Accuracy () # Update the metric with a batch of predictions and labels metric = metric . update ( target = y , preds = logits ) # Get the current value of the metric acc = metric . compute () # 0.95 # alternatively, produce a logs dict logs = metric . compute_logs () # {'accuracy': 0.95} # Reset the metric metric = metric . reset () Note that update enforces the use of keyword arguments. Also the Metric.name property is used as the key in the returned dict, by default this is the name of the class in lowercase but can be overridden in the constructor via the name argument.","title":"Metric"},{"location":"#tipical-training-setup","text":"Because Metrics are pytrees they can be used with jit , pmap , etc. On a more realistic scenario you will proably want to use them inside some of your JAX functions in a setup similar to this: import jax_metrics as jm metric = jm . metrics . Accuracy () def loss_fn ( params , metric , x , y ): ... metric = metric . update ( target = y , preds = logits ) ... return loss , metric @jax . jit def train_step ( params , metric , x , y ): grads , metric = jax . grad ( loss_fn , has_aux = True )( params , metric , x , y ) ... return params , metric @jax . jit def reset_step ( metric : jm . Metric ) -> jm . Metric : return metric . reset () Since the loss function usually has access to the predictions and labels, its usually where you would call metric.update , and the new metric state can be returned as an auxiliary output.","title":"Tipical Training Setup"},{"location":"#distributed-training","text":"JAX Metrics has a distributed friendly API via the batch_updates and reduce methods. A simple example of a loss function inside a data parallel setup could look like this: def loss_fn ( params , metric , x , y ): ... # compuate batch update batch_updates = metric . batch_updates ( target = y , preds = logits ) # gather over all devices and reduce batch_updates = jax . lax . all_gather ( batch_updates , \"device\" ) . reduce () # update metric metric = metric . merge ( batch_updates ) ... The batch_updates method behaves similar to update but returns a new metric state with only information about that batch, jax.lax.all_gather \"gathers\" the metric state over all devices plus adds a new axis to the metric state, and reduce reduces the metric state over all devices (first axis). Finally, merge combines the accumulated metric state over the previous batches with the batch updates.","title":"Distributed Training"},{"location":"#loss","text":"The Loss API just consists of a __call__ method. Simple usage looks like this: import jax_metrics as jm crossentropy = jm . losses . Crossentropy () # get reduced loss value loss = crossentropy ( target = y , preds = logits ) # 0.23 Note that losses are not pytrees so they should be marked as static. Similar to Keras, all losses have a reduction strategy that can be specified in the constructor and (usually) makes sure that the output is a scalar. Why have losses in a metrics library? There are a few reasons for having losses in a metrics library: 1. Most code from this library was originally written for and will still be consumed by Elegy. Since Elegy needs support for calculating cumulative losses, as you will see later, a Metric abstraction called `Losses` was created for this. 2. A couple of API design decisions are shared between the `Loss` and `Metric` APIs. This includes: * `__call__` and `update` both accept any number keyword only arguments. This is used to facilitate composition (see [Combinators](#combinators) section). * Both classes have the `index_into` and `rename_arguments` methods that allow them to modify how arguments are consumed. * Argument names are standardized to be consistent when ever possible, e.g. both `metrics.Accuracy` and `losses.Crossentropy` use the `target` and `preds` arguments.","title":"Loss"},{"location":"#combinators","text":"Combinators enable you to group together multiple metrics while also being instances of Metric and thus maintaining the same API.","title":"Combinators"},{"location":"#metrics","text":"The Metrics combinator lets you combine multiple metrics into a single Metric object. metrics = jm . Metrics ([ jm . metrics . Accuracy (), jm . metrics . F1 (), # not yet implemented \ud83d\ude05, coming soon? ]) # same API metrics = metrics . update ( target = y , preds = logits ) # compute now returns a dict metrics . compute () # {'accuracy': 0.95, 'f1': 0.87} # same as compute_logs in the case metrics . compute_logs () # {'accuracy': 0.95, 'f1': 0.87} # Reset the metrics metrics = metrics . reset () As you can see the Metrics.update method accepts and forwards all the arguments required by the individual metrics. In this example they use the same arguments, but in practice they may consume different subsets of the arguments. Also, if names are repeated then unique names are generated for each metric by appending a number to the metric name. If a dictionary is used instead of a list, the keys are used instead of the name property of the metrics to determine the key in the returned dict. metrics = jm . Metrics ({ \"acc\" : jm . metrics . Accuracy (), \"f_one\" : jm . metrics . F1 (), # not yet implemented \ud83d\ude05, coming soon? }) # same API metrics = metrics . update ( target = y , preds = logits ) # compute new returns a dict metrics . compute () # {'acc': 0.95, 'f_one': 0.87} # same as compute_logs in the case metrics . compute_logs () # {'acc': 0.95, 'f_one': 0.87} # Reset the metrics metrics = metrics . reset () You can use nested structures of dicts and lists to group metrics, the keys of the dicts are used to determine group names. Group names and metrics names are concatenated using \"/\" e.g. \"{group_name}/{metric_name}\" .","title":"Metrics"},{"location":"#losses","text":"Losses is a Metric combinator that behaves very similarly to Metrics but contains Loss instances. Losses calculates the cumulative mean value of each loss over the batches. losses = jm . Losses ([ jm . losses . Crossentropy (), jm . regularizers . L2 ( 1e-4 ), ]) # same API losses = losses . update ( target = y , preds = logits , parameters = params ) # compute new returns a dict losses . compute () # {'crossentropy': 0.23, 'l2': 0.005} # same as compute_logs in the case losses . compute_logs () # {'crossentropy': 0.23, 'l2': 0.005} # you can also compute the total loss loss = losses . total_loss () # 0.235 # Reset the losses losses = losses . reset () As with Metrics , the update method accepts and forwards all the arguments required by the individual losses. In this example target and preds are used by the Crossentropy , while parameters is used by the L2 . The total_loss method returns the sum of all values returned by compute . If a dictionary is used instead of a list, the keys are used instead of the name property of the losses to determine the key in the returned dict. losses = jm . Losses ({ \"xent\" : jm . losses . Crossentropy (), \"l2\" : jm . regularizers . L2 ( 1e-4 ), }) # same API losses = losses . update ( target = y , preds = logits , parameters = params ) # compute new returns a dict losses . compute () # {'xent': 0.23, 'l2': 0.005} # same as compute_logs in the case losses . compute_logs () # {'xent': 0.23, 'l2': 0.005} # you can also compute the total loss loss = losses . total_loss () # 0.235 # Reset the losses losses = losses . reset () If you want to use Losses to calculate the loss of a model, you should use batch_updates followed by total_loss to get the correct batch loss. For example, a loss function could be written as: def loss_fn ( ... , losses ): ... batch_updates = losses . batch_updates ( target = y , preds = logits , parameters = params ) loss = batch_updates . total_loss () losses = losses . merge ( batch_updates ) ... return loss , losses For convenience, the previous pattern can be simplified to a single line using the loss_and_update method: def loss_fn ( ... ): ... loss , lossses = losses . loss_and_update ( target = y , preds = logits , parameters = params ) ... return loss , losses","title":"Losses"},{"location":"api/Loss/","text":"jax_metrics.Loss Bases: ABC Loss base class. To be implemented by subclasses: call() : Contains the logic for loss calculation. Example subclass implementation: class MeanSquaredError ( Loss ): def call ( self , target , preds ): return jnp . mean ( jnp . square ( preds - target ), axis =- 1 ) Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this. Source code in jax_metrics/losses/loss.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 class Loss ( ABC ): \"\"\" Loss base class. To be implemented by subclasses: * `call()`: Contains the logic for loss calculation. Example subclass implementation: ```python class MeanSquaredError(Loss): def call(self, target, preds): return jnp.mean(jnp.square(preds - target), axis=-1) ``` Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this. \"\"\" def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ types . ScalarLike ] = None , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . weight = ( jnp . asarray ( weight , dtype = jnp . float32 ) if weight is not None else jnp . array ( 1.0 , dtype = jnp . float32 ) ) self . reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) def __call__ ( self , ** kwargs , ) -> jax . Array : sample_weight : tp . Optional [ jax . Array ] = kwargs . pop ( \"sample_weight\" , None ) values = self . call ( ** kwargs ) return reduce_loss ( values , sample_weight , self . weight , self . reduction ) @abstractmethod def call ( self , ** kwargs ) -> jax . Array : ... def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedLoss\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python losses = jm.Losses([ jm.losses.Crossentropy().index_into(target=[\"a\"]), jm.losses.Crossentropy().index_into(target=[\"b\"]), ]).reset() losses = losses.update( target={ \"a\": y0, \"b\": y1, }, preds=logits, ) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedLoss instance \"\"\" return IndexedLoss ( self , kwargs ) def rename_arguments ( self , ** kwargs : str ) -> \"MapArgsLoss\" : \"\"\" Returns a loss that renames the keyword arguments expected by `__call__`. Example: ```python crossentropy_loss = jm.losses.Crossentropy().rename_arguments(target=\"y_true\", preds=\"y_pred\") ... loss = crossentropy_loss(y_true=y, y_pred=logits) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A MapArgsLoss instance \"\"\" return MapArgsLoss ( self , kwargs ) __init__ ( reduction = None , weight = None ) Initializes Loss class. Parameters: Name Type Description Default reduction tp . Optional [ Reduction ] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight tp . Optional [ types . ScalarLike ] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/loss.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ types . ScalarLike ] = None , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . weight = ( jnp . asarray ( weight , dtype = jnp . float32 ) if weight is not None else jnp . array ( 1.0 , dtype = jnp . float32 ) ) self . reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) index_into ( ** kwargs ) Returns a metric that \"indexes\" the specified keyword arguments expected by .update() . You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing ( __getitem__ ). Example: losses = jm . Losses ([ jm . losses . Crossentropy () . index_into ( target = [ \"a\" ]), jm . losses . Crossentropy () . index_into ( target = [ \"b\" ]), ]) . reset () losses = losses . update ( target = { \"a\" : y0 , \"b\" : y1 , }, preds = logits , ) Here values is set to a dict of arrays, but thanks to .index_into() each loss can index into its correspoding array. Parameters: Name Type Description Default **kwargs types . IndexLike keyword arguments to be indexed {} Returns: Type Description IndexedLoss A IndexedLoss instance Source code in jax_metrics/losses/loss.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedLoss\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python losses = jm.Losses([ jm.losses.Crossentropy().index_into(target=[\"a\"]), jm.losses.Crossentropy().index_into(target=[\"b\"]), ]).reset() losses = losses.update( target={ \"a\": y0, \"b\": y1, }, preds=logits, ) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedLoss instance \"\"\" return IndexedLoss ( self , kwargs ) rename_arguments ( ** kwargs ) Returns a loss that renames the keyword arguments expected by __call__ . Example: crossentropy_loss = jm . losses . Crossentropy () . rename_arguments ( target = \"y_true\" , preds = \"y_pred\" ) ... loss = crossentropy_loss ( y_true = y , y_pred = logits ) Parameters: Name Type Description Default **kwargs str keyword arguments to be renamed {} Returns: Type Description MapArgsLoss A MapArgsLoss instance Source code in jax_metrics/losses/loss.py 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def rename_arguments ( self , ** kwargs : str ) -> \"MapArgsLoss\" : \"\"\" Returns a loss that renames the keyword arguments expected by `__call__`. Example: ```python crossentropy_loss = jm.losses.Crossentropy().rename_arguments(target=\"y_true\", preds=\"y_pred\") ... loss = crossentropy_loss(y_true=y, y_pred=logits) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A MapArgsLoss instance \"\"\" return MapArgsLoss ( self , kwargs )","title":"Loss"},{"location":"api/Loss/#jax_metricsloss","text":"Bases: ABC Loss base class. To be implemented by subclasses: call() : Contains the logic for loss calculation. Example subclass implementation: class MeanSquaredError ( Loss ): def call ( self , target , preds ): return jnp . mean ( jnp . square ( preds - target ), axis =- 1 ) Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this. Source code in jax_metrics/losses/loss.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 class Loss ( ABC ): \"\"\" Loss base class. To be implemented by subclasses: * `call()`: Contains the logic for loss calculation. Example subclass implementation: ```python class MeanSquaredError(Loss): def call(self, target, preds): return jnp.mean(jnp.square(preds - target), axis=-1) ``` Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this. \"\"\" def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ types . ScalarLike ] = None , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . weight = ( jnp . asarray ( weight , dtype = jnp . float32 ) if weight is not None else jnp . array ( 1.0 , dtype = jnp . float32 ) ) self . reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) def __call__ ( self , ** kwargs , ) -> jax . Array : sample_weight : tp . Optional [ jax . Array ] = kwargs . pop ( \"sample_weight\" , None ) values = self . call ( ** kwargs ) return reduce_loss ( values , sample_weight , self . weight , self . reduction ) @abstractmethod def call ( self , ** kwargs ) -> jax . Array : ... def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedLoss\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python losses = jm.Losses([ jm.losses.Crossentropy().index_into(target=[\"a\"]), jm.losses.Crossentropy().index_into(target=[\"b\"]), ]).reset() losses = losses.update( target={ \"a\": y0, \"b\": y1, }, preds=logits, ) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedLoss instance \"\"\" return IndexedLoss ( self , kwargs ) def rename_arguments ( self , ** kwargs : str ) -> \"MapArgsLoss\" : \"\"\" Returns a loss that renames the keyword arguments expected by `__call__`. Example: ```python crossentropy_loss = jm.losses.Crossentropy().rename_arguments(target=\"y_true\", preds=\"y_pred\") ... loss = crossentropy_loss(y_true=y, y_pred=logits) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A MapArgsLoss instance \"\"\" return MapArgsLoss ( self , kwargs )","title":"jax_metrics.Loss"},{"location":"api/Loss/#jax_metrics.losses.loss.Loss.__init__","text":"Initializes Loss class. Parameters: Name Type Description Default reduction tp . Optional [ Reduction ] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight tp . Optional [ types . ScalarLike ] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/loss.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ types . ScalarLike ] = None , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . weight = ( jnp . asarray ( weight , dtype = jnp . float32 ) if weight is not None else jnp . array ( 1.0 , dtype = jnp . float32 ) ) self . reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE )","title":"__init__()"},{"location":"api/Loss/#jax_metrics.losses.loss.Loss.index_into","text":"Returns a metric that \"indexes\" the specified keyword arguments expected by .update() . You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing ( __getitem__ ). Example: losses = jm . Losses ([ jm . losses . Crossentropy () . index_into ( target = [ \"a\" ]), jm . losses . Crossentropy () . index_into ( target = [ \"b\" ]), ]) . reset () losses = losses . update ( target = { \"a\" : y0 , \"b\" : y1 , }, preds = logits , ) Here values is set to a dict of arrays, but thanks to .index_into() each loss can index into its correspoding array. Parameters: Name Type Description Default **kwargs types . IndexLike keyword arguments to be indexed {} Returns: Type Description IndexedLoss A IndexedLoss instance Source code in jax_metrics/losses/loss.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedLoss\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python losses = jm.Losses([ jm.losses.Crossentropy().index_into(target=[\"a\"]), jm.losses.Crossentropy().index_into(target=[\"b\"]), ]).reset() losses = losses.update( target={ \"a\": y0, \"b\": y1, }, preds=logits, ) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedLoss instance \"\"\" return IndexedLoss ( self , kwargs )","title":"index_into()"},{"location":"api/Loss/#jax_metrics.losses.loss.Loss.rename_arguments","text":"Returns a loss that renames the keyword arguments expected by __call__ . Example: crossentropy_loss = jm . losses . Crossentropy () . rename_arguments ( target = \"y_true\" , preds = \"y_pred\" ) ... loss = crossentropy_loss ( y_true = y , y_pred = logits ) Parameters: Name Type Description Default **kwargs str keyword arguments to be renamed {} Returns: Type Description MapArgsLoss A MapArgsLoss instance Source code in jax_metrics/losses/loss.py 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def rename_arguments ( self , ** kwargs : str ) -> \"MapArgsLoss\" : \"\"\" Returns a loss that renames the keyword arguments expected by `__call__`. Example: ```python crossentropy_loss = jm.losses.Crossentropy().rename_arguments(target=\"y_true\", preds=\"y_pred\") ... loss = crossentropy_loss(y_true=y, y_pred=logits) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A MapArgsLoss instance \"\"\" return MapArgsLoss ( self , kwargs )","title":"rename_arguments()"},{"location":"api/Losses/","text":"jax_metrics.Losses Bases: SumMetric Source code in jax_metrics/metrics/losses.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 class Losses ( SumMetric ): totals : tp . Dict [ str , jax . Array ] counts : tp . Dict [ str , jax . Array ] losses : tp . Dict [ str , LossFn ] = static_field () def __init__ ( self , losses : tp . Dict [ str , LossFn ], ): self . losses = losses self . totals = { name : jnp . array ( 0.0 , dtype = jnp . float32 ) for name in self . losses } self . counts = { name : jnp . array ( 0 , dtype = jnp . uint32 ) for name in self . losses } def reset ( self : M ) -> M : return jax . tree_map ( jnp . zeros_like , self ) def update ( self : M , ** kwargs ) -> M : if self . totals is None or self . counts is None : raise ValueError ( \"Losses not initialized, call 'reset()' first\" ) totals = { name : self . totals [ name ] + loss ( ** kwargs ) for name , loss in self . losses . items () } counts = { name : value + 1 for name , value in self . counts . items ()} return self . replace ( totals = totals , counts = counts ) def compute ( self ) -> tp . Dict [ str , jax . Array ]: return { name : self . totals [ name ] / self . counts [ name ] for name in self . losses } def total_loss ( self ) -> jax . Array : return sum ( self . compute () . values (), jnp . array ( 0.0 )) def loss_and_update ( self : M , ** kwargs ) -> tp . Tuple [ jax . Array , M ]: batch_updates = self . batch_updates ( ** kwargs ) loss = batch_updates . total_loss () metrics = self . merge ( batch_updates ) return loss , metrics","title":"Losses"},{"location":"api/Losses/#jax_metricslosses","text":"Bases: SumMetric Source code in jax_metrics/metrics/losses.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 class Losses ( SumMetric ): totals : tp . Dict [ str , jax . Array ] counts : tp . Dict [ str , jax . Array ] losses : tp . Dict [ str , LossFn ] = static_field () def __init__ ( self , losses : tp . Dict [ str , LossFn ], ): self . losses = losses self . totals = { name : jnp . array ( 0.0 , dtype = jnp . float32 ) for name in self . losses } self . counts = { name : jnp . array ( 0 , dtype = jnp . uint32 ) for name in self . losses } def reset ( self : M ) -> M : return jax . tree_map ( jnp . zeros_like , self ) def update ( self : M , ** kwargs ) -> M : if self . totals is None or self . counts is None : raise ValueError ( \"Losses not initialized, call 'reset()' first\" ) totals = { name : self . totals [ name ] + loss ( ** kwargs ) for name , loss in self . losses . items () } counts = { name : value + 1 for name , value in self . counts . items ()} return self . replace ( totals = totals , counts = counts ) def compute ( self ) -> tp . Dict [ str , jax . Array ]: return { name : self . totals [ name ] / self . counts [ name ] for name in self . losses } def total_loss ( self ) -> jax . Array : return sum ( self . compute () . values (), jnp . array ( 0.0 )) def loss_and_update ( self : M , ** kwargs ) -> tp . Tuple [ jax . Array , M ]: batch_updates = self . batch_updates ( ** kwargs ) loss = batch_updates . total_loss () metrics = self . merge ( batch_updates ) return loss , metrics","title":"jax_metrics.Losses"},{"location":"api/Metric/","text":"jax_metrics.Metric Bases: Pytree Encapsulates metric logic and state. Metrics accumulate state between calls such that their output value reflect the metric as if calculated on the whole data given up to that point. Source code in jax_metrics/metrics/metric.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 class Metric ( Pytree ): \"\"\" Encapsulates metric logic and state. Metrics accumulate state between calls such that their output value reflect the metric as if calculated on the whole data given up to that point. \"\"\" def __call__ ( self : M , ** kwargs : tp . Any ) -> tp . Tuple [ tp . Any , M ]: metric : M = self batch_updates = metric . batch_updates ( ** kwargs ) batch_values = batch_updates . compute () metric = metric . merge ( batch_updates ) return batch_values , metric @abstractmethod def reset ( self : M ) -> M : \"\"\" Resets the metric state. Returns: Metric with the initial state \"\"\" ... @abstractmethod def update ( self : M , ** kwargs : tp . Any ) -> M : \"\"\" Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining `**kwargs`. Arguments: **kwargs: data to update the metric with Returns: Metric with updated state \"\"\" ... @abstractmethod def compute ( self ) -> tp . Any : \"\"\" Compute the current metric value. \"\"\" ... @abstractmethod def reduce ( self : M ) -> M : \"\"\" Aggregate metric state. It assumes the metric's internal state has an additional 'device' dimension on the 0th axis. Example: ```python batch_updates = metric.batch_updates(**kwargs) batch_updates = jax.lax.all_gather(batch_updates, axis_name=\"device\") batch_updates = batch_updates.reduce() metric = metric.merge(batch_updates) ``` Returns: Metric with aggregated state \"\"\" # return jax.tree_map(lambda x: jnp.sum(x, axis=0), self) ... @abstractmethod def merge ( self : M , other : M ) -> M : \"\"\" Merge the state of two metrics of the same type. Usually used to merge a metric with its batch_updates. Example: ```python batch_updates = metric.batch_updates(**kwargs) metric = metric.merge(batch_updates) ``` \"\"\" # return jax.tree_map(lambda x, y: x + y, self, other) ... def batch_updates ( self : M , ** kwargs : tp . Any ) -> M : \"\"\" Compute metric updates for a batch of data. Equivalent to `.reset().update(**kwargs)`. Arguments: kwargs: data to update the metric with Returns: Metric with updated state \"\"\" return self . reset () . update ( ** kwargs ) def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedMetric\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python metrics = jm.Metrics([ jm.metrics.Mean().index_into(values=[\"a\"]), jm.metrics.Mean().index_into(values=[\"b\"]), ]).reset() metrics = metrics.update(values={ \"a\": loss0, \"b\": loss1, }) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. This also works with Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedMetric instance \"\"\" return IndexedMetric ( self , kwargs ) def rename_arguments ( self , ** kwargs : str ) -> \"RenameArguments\" : \"\"\" Returns a metric that renames the keyword arguments expected by `.update()`. Example: ```python mean = jm.metrics.Mean().rename_arguments(values=\"loss\") ... loss = loss_fn(x, y) mean = mean.update(loss=loss) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A RenameArguments instance \"\"\" return RenameArguments ( self , kwargs ) batch_updates ( ** kwargs ) Compute metric updates for a batch of data. Equivalent to .reset().update(**kwargs) . Parameters: Name Type Description Default kwargs tp . Any data to update the metric with {} Returns: Type Description M Metric with updated state Source code in jax_metrics/metrics/metric.py 103 104 105 106 107 108 109 110 111 112 113 114 def batch_updates ( self : M , ** kwargs : tp . Any ) -> M : \"\"\" Compute metric updates for a batch of data. Equivalent to `.reset().update(**kwargs)`. Arguments: kwargs: data to update the metric with Returns: Metric with updated state \"\"\" return self . reset () . update ( ** kwargs ) compute () abstractmethod Compute the current metric value. Source code in jax_metrics/metrics/metric.py 58 59 60 61 62 63 @abstractmethod def compute ( self ) -> tp . Any : \"\"\" Compute the current metric value. \"\"\" ... index_into ( ** kwargs ) Returns a metric that \"indexes\" the specified keyword arguments expected by .update() . You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing ( __getitem__ ). Example: metrics = jm . Metrics ([ jm . metrics . Mean () . index_into ( values = [ \"a\" ]), jm . metrics . Mean () . index_into ( values = [ \"b\" ]), ]) . reset () metrics = metrics . update ( values = { \"a\" : loss0 , \"b\" : loss1 , }) Here values is set to a dict of arrays, but thanks to .index_into() each loss can index into its correspoding array. This also works with Parameters: Name Type Description Default **kwargs types . IndexLike keyword arguments to be indexed {} Returns: Type Description IndexedMetric A IndexedMetric instance Source code in jax_metrics/metrics/metric.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedMetric\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python metrics = jm.Metrics([ jm.metrics.Mean().index_into(values=[\"a\"]), jm.metrics.Mean().index_into(values=[\"b\"]), ]).reset() metrics = metrics.update(values={ \"a\": loss0, \"b\": loss1, }) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. This also works with Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedMetric instance \"\"\" return IndexedMetric ( self , kwargs ) merge ( other ) abstractmethod Merge the state of two metrics of the same type. Usually used to merge a metric with its batch_updates. Example: batch_updates = metric . batch_updates ( ** kwargs ) metric = metric . merge ( batch_updates ) Source code in jax_metrics/metrics/metric.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 @abstractmethod def merge ( self : M , other : M ) -> M : \"\"\" Merge the state of two metrics of the same type. Usually used to merge a metric with its batch_updates. Example: ```python batch_updates = metric.batch_updates(**kwargs) metric = metric.merge(batch_updates) ``` \"\"\" # return jax.tree_map(lambda x, y: x + y, self, other) ... reduce () abstractmethod Aggregate metric state. It assumes the metric's internal state has an additional 'device' dimension on the 0th axis. Example: batch_updates = metric . batch_updates ( ** kwargs ) batch_updates = jax . lax . all_gather ( batch_updates , axis_name = \"device\" ) batch_updates = batch_updates . reduce () metric = metric . merge ( batch_updates ) Returns: Type Description M Metric with aggregated state Source code in jax_metrics/metrics/metric.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 @abstractmethod def reduce ( self : M ) -> M : \"\"\" Aggregate metric state. It assumes the metric's internal state has an additional 'device' dimension on the 0th axis. Example: ```python batch_updates = metric.batch_updates(**kwargs) batch_updates = jax.lax.all_gather(batch_updates, axis_name=\"device\") batch_updates = batch_updates.reduce() metric = metric.merge(batch_updates) ``` Returns: Metric with aggregated state \"\"\" # return jax.tree_map(lambda x: jnp.sum(x, axis=0), self) ... rename_arguments ( ** kwargs ) Returns a metric that renames the keyword arguments expected by .update() . Example: mean = jm . metrics . Mean () . rename_arguments ( values = \"loss\" ) ... loss = loss_fn ( x , y ) mean = mean . update ( loss = loss ) Parameters: Name Type Description Default **kwargs str keyword arguments to be renamed {} Returns: Type Description RenameArguments A RenameArguments instance Source code in jax_metrics/metrics/metric.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def rename_arguments ( self , ** kwargs : str ) -> \"RenameArguments\" : \"\"\" Returns a metric that renames the keyword arguments expected by `.update()`. Example: ```python mean = jm.metrics.Mean().rename_arguments(values=\"loss\") ... loss = loss_fn(x, y) mean = mean.update(loss=loss) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A RenameArguments instance \"\"\" return RenameArguments ( self , kwargs ) reset () abstractmethod Resets the metric state. Returns: Type Description M Metric with the initial state Source code in jax_metrics/metrics/metric.py 33 34 35 36 37 38 39 40 41 @abstractmethod def reset ( self : M ) -> M : \"\"\" Resets the metric state. Returns: Metric with the initial state \"\"\" ... update ( ** kwargs ) abstractmethod Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining **kwargs . Parameters: Name Type Description Default **kwargs tp . Any data to update the metric with {} Returns: Type Description M Metric with updated state Source code in jax_metrics/metrics/metric.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 @abstractmethod def update ( self : M , ** kwargs : tp . Any ) -> M : \"\"\" Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining `**kwargs`. Arguments: **kwargs: data to update the metric with Returns: Metric with updated state \"\"\" ...","title":"Metric"},{"location":"api/Metric/#jax_metricsmetric","text":"Bases: Pytree Encapsulates metric logic and state. Metrics accumulate state between calls such that their output value reflect the metric as if calculated on the whole data given up to that point. Source code in jax_metrics/metrics/metric.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 class Metric ( Pytree ): \"\"\" Encapsulates metric logic and state. Metrics accumulate state between calls such that their output value reflect the metric as if calculated on the whole data given up to that point. \"\"\" def __call__ ( self : M , ** kwargs : tp . Any ) -> tp . Tuple [ tp . Any , M ]: metric : M = self batch_updates = metric . batch_updates ( ** kwargs ) batch_values = batch_updates . compute () metric = metric . merge ( batch_updates ) return batch_values , metric @abstractmethod def reset ( self : M ) -> M : \"\"\" Resets the metric state. Returns: Metric with the initial state \"\"\" ... @abstractmethod def update ( self : M , ** kwargs : tp . Any ) -> M : \"\"\" Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining `**kwargs`. Arguments: **kwargs: data to update the metric with Returns: Metric with updated state \"\"\" ... @abstractmethod def compute ( self ) -> tp . Any : \"\"\" Compute the current metric value. \"\"\" ... @abstractmethod def reduce ( self : M ) -> M : \"\"\" Aggregate metric state. It assumes the metric's internal state has an additional 'device' dimension on the 0th axis. Example: ```python batch_updates = metric.batch_updates(**kwargs) batch_updates = jax.lax.all_gather(batch_updates, axis_name=\"device\") batch_updates = batch_updates.reduce() metric = metric.merge(batch_updates) ``` Returns: Metric with aggregated state \"\"\" # return jax.tree_map(lambda x: jnp.sum(x, axis=0), self) ... @abstractmethod def merge ( self : M , other : M ) -> M : \"\"\" Merge the state of two metrics of the same type. Usually used to merge a metric with its batch_updates. Example: ```python batch_updates = metric.batch_updates(**kwargs) metric = metric.merge(batch_updates) ``` \"\"\" # return jax.tree_map(lambda x, y: x + y, self, other) ... def batch_updates ( self : M , ** kwargs : tp . Any ) -> M : \"\"\" Compute metric updates for a batch of data. Equivalent to `.reset().update(**kwargs)`. Arguments: kwargs: data to update the metric with Returns: Metric with updated state \"\"\" return self . reset () . update ( ** kwargs ) def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedMetric\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python metrics = jm.Metrics([ jm.metrics.Mean().index_into(values=[\"a\"]), jm.metrics.Mean().index_into(values=[\"b\"]), ]).reset() metrics = metrics.update(values={ \"a\": loss0, \"b\": loss1, }) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. This also works with Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedMetric instance \"\"\" return IndexedMetric ( self , kwargs ) def rename_arguments ( self , ** kwargs : str ) -> \"RenameArguments\" : \"\"\" Returns a metric that renames the keyword arguments expected by `.update()`. Example: ```python mean = jm.metrics.Mean().rename_arguments(values=\"loss\") ... loss = loss_fn(x, y) mean = mean.update(loss=loss) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A RenameArguments instance \"\"\" return RenameArguments ( self , kwargs )","title":"jax_metrics.Metric"},{"location":"api/Metric/#jax_metrics.metrics.metric.Metric.batch_updates","text":"Compute metric updates for a batch of data. Equivalent to .reset().update(**kwargs) . Parameters: Name Type Description Default kwargs tp . Any data to update the metric with {} Returns: Type Description M Metric with updated state Source code in jax_metrics/metrics/metric.py 103 104 105 106 107 108 109 110 111 112 113 114 def batch_updates ( self : M , ** kwargs : tp . Any ) -> M : \"\"\" Compute metric updates for a batch of data. Equivalent to `.reset().update(**kwargs)`. Arguments: kwargs: data to update the metric with Returns: Metric with updated state \"\"\" return self . reset () . update ( ** kwargs )","title":"batch_updates()"},{"location":"api/Metric/#jax_metrics.metrics.metric.Metric.compute","text":"Compute the current metric value. Source code in jax_metrics/metrics/metric.py 58 59 60 61 62 63 @abstractmethod def compute ( self ) -> tp . Any : \"\"\" Compute the current metric value. \"\"\" ...","title":"compute()"},{"location":"api/Metric/#jax_metrics.metrics.metric.Metric.index_into","text":"Returns a metric that \"indexes\" the specified keyword arguments expected by .update() . You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing ( __getitem__ ). Example: metrics = jm . Metrics ([ jm . metrics . Mean () . index_into ( values = [ \"a\" ]), jm . metrics . Mean () . index_into ( values = [ \"b\" ]), ]) . reset () metrics = metrics . update ( values = { \"a\" : loss0 , \"b\" : loss1 , }) Here values is set to a dict of arrays, but thanks to .index_into() each loss can index into its correspoding array. This also works with Parameters: Name Type Description Default **kwargs types . IndexLike keyword arguments to be indexed {} Returns: Type Description IndexedMetric A IndexedMetric instance Source code in jax_metrics/metrics/metric.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedMetric\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python metrics = jm.Metrics([ jm.metrics.Mean().index_into(values=[\"a\"]), jm.metrics.Mean().index_into(values=[\"b\"]), ]).reset() metrics = metrics.update(values={ \"a\": loss0, \"b\": loss1, }) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. This also works with Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedMetric instance \"\"\" return IndexedMetric ( self , kwargs )","title":"index_into()"},{"location":"api/Metric/#jax_metrics.metrics.metric.Metric.merge","text":"Merge the state of two metrics of the same type. Usually used to merge a metric with its batch_updates. Example: batch_updates = metric . batch_updates ( ** kwargs ) metric = metric . merge ( batch_updates ) Source code in jax_metrics/metrics/metric.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 @abstractmethod def merge ( self : M , other : M ) -> M : \"\"\" Merge the state of two metrics of the same type. Usually used to merge a metric with its batch_updates. Example: ```python batch_updates = metric.batch_updates(**kwargs) metric = metric.merge(batch_updates) ``` \"\"\" # return jax.tree_map(lambda x, y: x + y, self, other) ...","title":"merge()"},{"location":"api/Metric/#jax_metrics.metrics.metric.Metric.reduce","text":"Aggregate metric state. It assumes the metric's internal state has an additional 'device' dimension on the 0th axis. Example: batch_updates = metric . batch_updates ( ** kwargs ) batch_updates = jax . lax . all_gather ( batch_updates , axis_name = \"device\" ) batch_updates = batch_updates . reduce () metric = metric . merge ( batch_updates ) Returns: Type Description M Metric with aggregated state Source code in jax_metrics/metrics/metric.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 @abstractmethod def reduce ( self : M ) -> M : \"\"\" Aggregate metric state. It assumes the metric's internal state has an additional 'device' dimension on the 0th axis. Example: ```python batch_updates = metric.batch_updates(**kwargs) batch_updates = jax.lax.all_gather(batch_updates, axis_name=\"device\") batch_updates = batch_updates.reduce() metric = metric.merge(batch_updates) ``` Returns: Metric with aggregated state \"\"\" # return jax.tree_map(lambda x: jnp.sum(x, axis=0), self) ...","title":"reduce()"},{"location":"api/Metric/#jax_metrics.metrics.metric.Metric.rename_arguments","text":"Returns a metric that renames the keyword arguments expected by .update() . Example: mean = jm . metrics . Mean () . rename_arguments ( values = \"loss\" ) ... loss = loss_fn ( x , y ) mean = mean . update ( loss = loss ) Parameters: Name Type Description Default **kwargs str keyword arguments to be renamed {} Returns: Type Description RenameArguments A RenameArguments instance Source code in jax_metrics/metrics/metric.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def rename_arguments ( self , ** kwargs : str ) -> \"RenameArguments\" : \"\"\" Returns a metric that renames the keyword arguments expected by `.update()`. Example: ```python mean = jm.metrics.Mean().rename_arguments(values=\"loss\") ... loss = loss_fn(x, y) mean = mean.update(loss=loss) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A RenameArguments instance \"\"\" return RenameArguments ( self , kwargs )","title":"rename_arguments()"},{"location":"api/Metric/#jax_metrics.metrics.metric.Metric.reset","text":"Resets the metric state. Returns: Type Description M Metric with the initial state Source code in jax_metrics/metrics/metric.py 33 34 35 36 37 38 39 40 41 @abstractmethod def reset ( self : M ) -> M : \"\"\" Resets the metric state. Returns: Metric with the initial state \"\"\" ...","title":"reset()"},{"location":"api/Metric/#jax_metrics.metrics.metric.Metric.update","text":"Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining **kwargs . Parameters: Name Type Description Default **kwargs tp . Any data to update the metric with {} Returns: Type Description M Metric with updated state Source code in jax_metrics/metrics/metric.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 @abstractmethod def update ( self : M , ** kwargs : tp . Any ) -> M : \"\"\" Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining `**kwargs`. Arguments: **kwargs: data to update the metric with Returns: Metric with updated state \"\"\" ...","title":"update()"},{"location":"api/Metrics/","text":"jax_metrics.Metrics Bases: Metric Source code in jax_metrics/metrics/metrics.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 @dataclasses . dataclass class Metrics ( Metric ): metrics : tp . Dict [ str , Metric ] def reset ( self : M ) -> M : metrics = { name : metric . reset () for name , metric in self . metrics . items ()} return self . replace ( metrics = metrics ) def update ( self : M , ** kwargs ) -> M : \"\"\" Update all metrics with the given values. Each metric will receive the same keyword arguments but each can internally select the values to use. If a required value is not provided, the metric will raise a TypeError. Arguments: **kwargs: Keyword arguments to pass to each metric. Returns: Metrics instance with updated state. \"\"\" metrics = { name : metric . update ( ** kwargs ) for name , metric in self . metrics . items () } return self . replace ( metrics = metrics ) def compute ( self ) -> tp . Dict [ str , jax . Array ]: outputs = {} names = set () for name , metric in self . metrics . items (): value = metric . compute () for path , value , parent_iterable in utils . _flatten_names ( value ): name = utils . _unique_name ( names , name ) if path : if parent_iterable : name = f \" { path } / { name } \" else : name = path outputs [ name ] = value return outputs def slice ( self , ** kwargs : types . IndexLike ) -> \"Metrics\" : metrics = { name : metric . index_into ( ** kwargs ) for name , metric in self . metrics . items () } return self . replace ( metrics = metrics ) def merge ( self : M , other : M ) -> M : return type ( self )( metrics = { name : metric . merge ( other . metrics [ name ]) for name , metric in self . metrics . items () } ) def reduce ( self : M ) -> M : return type ( self )( metrics = { name : metric . reduce () for name , metric in self . metrics . items ()} ) update ( ** kwargs ) Update all metrics with the given values. Each metric will receive the same keyword arguments but each can internally select the values to use. If a required value is not provided, the metric will raise a TypeError. Parameters: Name Type Description Default **kwargs Keyword arguments to pass to each metric. {} Returns: Type Description M Metrics instance with updated state. Source code in jax_metrics/metrics/metrics.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def update ( self : M , ** kwargs ) -> M : \"\"\" Update all metrics with the given values. Each metric will receive the same keyword arguments but each can internally select the values to use. If a required value is not provided, the metric will raise a TypeError. Arguments: **kwargs: Keyword arguments to pass to each metric. Returns: Metrics instance with updated state. \"\"\" metrics = { name : metric . update ( ** kwargs ) for name , metric in self . metrics . items () } return self . replace ( metrics = metrics )","title":"Metrics"},{"location":"api/Metrics/#jax_metricsmetrics","text":"Bases: Metric Source code in jax_metrics/metrics/metrics.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 @dataclasses . dataclass class Metrics ( Metric ): metrics : tp . Dict [ str , Metric ] def reset ( self : M ) -> M : metrics = { name : metric . reset () for name , metric in self . metrics . items ()} return self . replace ( metrics = metrics ) def update ( self : M , ** kwargs ) -> M : \"\"\" Update all metrics with the given values. Each metric will receive the same keyword arguments but each can internally select the values to use. If a required value is not provided, the metric will raise a TypeError. Arguments: **kwargs: Keyword arguments to pass to each metric. Returns: Metrics instance with updated state. \"\"\" metrics = { name : metric . update ( ** kwargs ) for name , metric in self . metrics . items () } return self . replace ( metrics = metrics ) def compute ( self ) -> tp . Dict [ str , jax . Array ]: outputs = {} names = set () for name , metric in self . metrics . items (): value = metric . compute () for path , value , parent_iterable in utils . _flatten_names ( value ): name = utils . _unique_name ( names , name ) if path : if parent_iterable : name = f \" { path } / { name } \" else : name = path outputs [ name ] = value return outputs def slice ( self , ** kwargs : types . IndexLike ) -> \"Metrics\" : metrics = { name : metric . index_into ( ** kwargs ) for name , metric in self . metrics . items () } return self . replace ( metrics = metrics ) def merge ( self : M , other : M ) -> M : return type ( self )( metrics = { name : metric . merge ( other . metrics [ name ]) for name , metric in self . metrics . items () } ) def reduce ( self : M ) -> M : return type ( self )( metrics = { name : metric . reduce () for name , metric in self . metrics . items ()} )","title":"jax_metrics.Metrics"},{"location":"api/Metrics/#jax_metrics.metrics.metrics.Metrics.update","text":"Update all metrics with the given values. Each metric will receive the same keyword arguments but each can internally select the values to use. If a required value is not provided, the metric will raise a TypeError. Parameters: Name Type Description Default **kwargs Keyword arguments to pass to each metric. {} Returns: Type Description M Metrics instance with updated state. Source code in jax_metrics/metrics/metrics.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def update ( self : M , ** kwargs ) -> M : \"\"\" Update all metrics with the given values. Each metric will receive the same keyword arguments but each can internally select the values to use. If a required value is not provided, the metric will raise a TypeError. Arguments: **kwargs: Keyword arguments to pass to each metric. Returns: Metrics instance with updated state. \"\"\" metrics = { name : metric . update ( ** kwargs ) for name , metric in self . metrics . items () } return self . replace ( metrics = metrics )","title":"update()"},{"location":"api/Named/","text":"jax_metrics.Named Bases: tp . Generic [ A ] Source code in jax_metrics/types.py 26 27 28 29 30 31 32 33 34 35 36 37 @jax . tree_util . register_pytree_node_class @dataclass class Named ( tp . Generic [ A ]): name : str value : A def tree_flatten ( self ): return ( self . value ,), self . name @classmethod def tree_unflatten ( cls , name , children ): return cls ( name , children [ 0 ])","title":"Named"},{"location":"api/Named/#jax_metricsnamed","text":"Bases: tp . Generic [ A ] Source code in jax_metrics/types.py 26 27 28 29 30 31 32 33 34 35 36 37 @jax . tree_util . register_pytree_node_class @dataclass class Named ( tp . Generic [ A ]): name : str value : A def tree_flatten ( self ): return ( self . value ,), self . name @classmethod def tree_unflatten ( cls , name , children ): return cls ( name , children [ 0 ])","title":"jax_metrics.Named"},{"location":"api/SumMetric/","text":"jax_metrics.SumMetric Bases: Metric Source code in jax_metrics/metrics/metric.py 171 172 173 174 175 176 class SumMetric ( Metric ): def merge ( self : M , other : M ) -> M : return jax . tree_map ( lambda x , y : x + y , self , other ) def reduce ( self : M ) -> M : return jax . tree_map ( lambda x : jnp . sum ( x , axis = 0 ), self )","title":"SumMetric"},{"location":"api/SumMetric/#jax_metricssummetric","text":"Bases: Metric Source code in jax_metrics/metrics/metric.py 171 172 173 174 175 176 class SumMetric ( Metric ): def merge ( self : M , other : M ) -> M : return jax . tree_map ( lambda x , y : x + y , self , other ) def reduce ( self : M ) -> M : return jax . tree_map ( lambda x : jnp . sum ( x , axis = 0 ), self )","title":"jax_metrics.SumMetric"},{"location":"api/losses/CosineSimilarity/","text":"jax_metrics.losses.CosineSimilarity Bases: Loss Computes the mean squared logarithmic errors between target and predictions. loss = -sum(l2_norm(target) * l2_norm(preds)) Usage: target = jnp . array ([[ 0. , 1. ], [ 1. , 1. ]]) preds = jnp . array ([[ 1. , 0. ], [ 1. , 1. ]]) # Using 'auto'/'sum_over_batch_size' reduction type. cosine_loss = jm . losses . CosineSimilarity ( axis = 1 ) assert cosine_loss ( target , preds ) == - 0.49999997 # Calling with 'sample_weight'. assert cosine_loss ( target , preds , sample_weight = jnp . array ([ 0.8 , 0.2 ])) == - 0.099999994 # Using 'sum' reduction type. cosine_loss = jm . losses . CosineSimilarity ( axis = 1 , reduction = jm . losses . Reduction . SUM ) assert cosine_loss ( target , preds ) == - 0.99999994 # Using 'none' reduction type. cosine_loss = jm . losses . CosineSimilarity ( axis = 1 , reduction = jm . losses . Reduction . NONE ) assert jnp . equal ( cosine_loss ( target , preds ), jnp . array ([ - 0. , - 0.99999994 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . CosineSimilarity ( axis = 1 ), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/cosine_similarity.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 class CosineSimilarity ( Loss ): \"\"\" Computes the mean squared logarithmic errors between target and predictions. `loss = -sum(l2_norm(target) * l2_norm(preds))` Usage: ```python target = jnp.array([[0., 1.], [1., 1.]]) preds = jnp.array([[1., 0.], [1., 1.]]) # Using 'auto'/'sum_over_batch_size' reduction type. cosine_loss = jm.losses.CosineSimilarity(axis=1) assert cosine_loss(target, preds) == -0.49999997 # Calling with 'sample_weight'. assert cosine_loss(target, preds, sample_weight=jnp.array([0.8, 0.2])) == -0.099999994 # Using 'sum' reduction type. cosine_loss = jm.losses.CosineSimilarity(axis=1, reduction=jm.losses.Reduction.SUM ) assert cosine_loss(target, preds) == -0.99999994 # Using 'none' reduction type. cosine_loss = jm.losses.CosineSimilarity(axis=1, reduction=jm.losses.Reduction.NONE ) assert jnp.equal(cosine_loss(target, preds), jnp.array([-0., -0.99999994])).all() ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.CosineSimilarity(axis=1), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , axis : int = - 1 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `Mean` class. Arguments: axis: (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . axis = axis return super () . __init__ ( reduction = reduction , weight = weight ) def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `CosineSimilarity` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return cosine_similarity ( target , preds , self . axis ) __init__ ( axis =- 1 , reduction = None , weight = None ) Initializes Mean class. Parameters: Name Type Description Default axis int (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. -1 reduction tp . Optional [ Reduction ] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight tp . Optional [ float ] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/cosine_similarity.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def __init__ ( self , axis : int = - 1 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `Mean` class. Arguments: axis: (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . axis = axis return super () . __init__ ( reduction = reduction , weight = weight ) call ( target , preds , sample_weight = None , ** _ ) Invokes the CosineSimilarity instance. Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] required sample_weight tp . Optional [ jax . Array ] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description jax . Array Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/cosine_similarity.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `CosineSimilarity` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return cosine_similarity ( target , preds , self . axis )","title":"CosineSimilarity"},{"location":"api/losses/CosineSimilarity/#jax_metricslossescosinesimilarity","text":"Bases: Loss Computes the mean squared logarithmic errors between target and predictions. loss = -sum(l2_norm(target) * l2_norm(preds)) Usage: target = jnp . array ([[ 0. , 1. ], [ 1. , 1. ]]) preds = jnp . array ([[ 1. , 0. ], [ 1. , 1. ]]) # Using 'auto'/'sum_over_batch_size' reduction type. cosine_loss = jm . losses . CosineSimilarity ( axis = 1 ) assert cosine_loss ( target , preds ) == - 0.49999997 # Calling with 'sample_weight'. assert cosine_loss ( target , preds , sample_weight = jnp . array ([ 0.8 , 0.2 ])) == - 0.099999994 # Using 'sum' reduction type. cosine_loss = jm . losses . CosineSimilarity ( axis = 1 , reduction = jm . losses . Reduction . SUM ) assert cosine_loss ( target , preds ) == - 0.99999994 # Using 'none' reduction type. cosine_loss = jm . losses . CosineSimilarity ( axis = 1 , reduction = jm . losses . Reduction . NONE ) assert jnp . equal ( cosine_loss ( target , preds ), jnp . array ([ - 0. , - 0.99999994 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . CosineSimilarity ( axis = 1 ), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/cosine_similarity.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 class CosineSimilarity ( Loss ): \"\"\" Computes the mean squared logarithmic errors between target and predictions. `loss = -sum(l2_norm(target) * l2_norm(preds))` Usage: ```python target = jnp.array([[0., 1.], [1., 1.]]) preds = jnp.array([[1., 0.], [1., 1.]]) # Using 'auto'/'sum_over_batch_size' reduction type. cosine_loss = jm.losses.CosineSimilarity(axis=1) assert cosine_loss(target, preds) == -0.49999997 # Calling with 'sample_weight'. assert cosine_loss(target, preds, sample_weight=jnp.array([0.8, 0.2])) == -0.099999994 # Using 'sum' reduction type. cosine_loss = jm.losses.CosineSimilarity(axis=1, reduction=jm.losses.Reduction.SUM ) assert cosine_loss(target, preds) == -0.99999994 # Using 'none' reduction type. cosine_loss = jm.losses.CosineSimilarity(axis=1, reduction=jm.losses.Reduction.NONE ) assert jnp.equal(cosine_loss(target, preds), jnp.array([-0., -0.99999994])).all() ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.CosineSimilarity(axis=1), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , axis : int = - 1 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `Mean` class. Arguments: axis: (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . axis = axis return super () . __init__ ( reduction = reduction , weight = weight ) def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `CosineSimilarity` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return cosine_similarity ( target , preds , self . axis )","title":"jax_metrics.losses.CosineSimilarity"},{"location":"api/losses/CosineSimilarity/#jax_metrics.losses.cosine_similarity.CosineSimilarity.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default axis int (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. -1 reduction tp . Optional [ Reduction ] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight tp . Optional [ float ] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/cosine_similarity.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def __init__ ( self , axis : int = - 1 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `Mean` class. Arguments: axis: (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . axis = axis return super () . __init__ ( reduction = reduction , weight = weight )","title":"__init__()"},{"location":"api/losses/CosineSimilarity/#jax_metrics.losses.cosine_similarity.CosineSimilarity.call","text":"Invokes the CosineSimilarity instance. Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] required sample_weight tp . Optional [ jax . Array ] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description jax . Array Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/cosine_similarity.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `CosineSimilarity` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return cosine_similarity ( target , preds , self . axis )","title":"call()"},{"location":"api/losses/Crossentropy/","text":"jax_metrics.losses.Crossentropy Bases: Loss Computes the crossentropy loss between the target and predictions. Use this crossentropy loss function when there are two or more label classes. We expect target to be provided as integers. If you want to provide target using one-hot representation, please use CategoricalCrossentropy loss. There should be # classes floating point values per feature for preds and a single floating point value per feature for target . In the snippet below, there is a single floating point value per example for target and # classes floating pointing values per example for preds . The shape of target is [batch_size] and the shape of preds is [batch_size, num_classes] . Usage: target = jnp . array ([ 1 , 2 ]) preds = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = jm . losses . Crossentropy () result = scce ( target , preds ) # 1.177 assert np . isclose ( result , 1.177 , rtol = 0.01 ) # Calling with 'sample_weight'. result = scce ( target , preds , sample_weight = jnp . array ([ 0.3 , 0.7 ])) # 0.814 assert np . isclose ( result , 0.814 , rtol = 0.01 ) # Using 'sum' reduction type. scce = jm . losses . Crossentropy ( reduction = jm . losses . Reduction . SUM ) result = scce ( target , preds ) # 2.354 assert np . isclose ( result , 2.354 , rtol = 0.01 ) # Using 'none' reduction type. scce = jm . losses . Crossentropy ( reduction = jm . losses . Reduction . NONE ) result = scce ( target , preds ) # [0.0513, 2.303] assert jnp . all ( np . isclose ( result , [ 0.0513 , 2.303 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . Crossentropy (), metrics = elegy . metrics . Accuracy (), optimizer = optax . adam ( 1e-3 ), ) Source code in jax_metrics/losses/crossentropy.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 class Crossentropy ( Loss ): \"\"\" Computes the crossentropy loss between the target and predictions. Use this crossentropy loss function when there are two or more label classes. We expect target to be provided as integers. If you want to provide target using `one-hot` representation, please use `CategoricalCrossentropy` loss. There should be `# classes` floating point values per feature for `preds` and a single floating point value per feature for `target`. In the snippet below, there is a single floating point value per example for `target` and `# classes` floating pointing values per example for `preds`. The shape of `target` is `[batch_size]` and the shape of `preds` is `[batch_size, num_classes]`. Usage: ```python target = jnp.array([1, 2]) preds = jnp.array([[0.05, 0.95, 0], [0.1, 0.8, 0.1]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = jm.losses.Crossentropy() result = scce(target, preds) # 1.177 assert np.isclose(result, 1.177, rtol=0.01) # Calling with 'sample_weight'. result = scce(target, preds, sample_weight=jnp.array([0.3, 0.7])) # 0.814 assert np.isclose(result, 0.814, rtol=0.01) # Using 'sum' reduction type. scce = jm.losses.Crossentropy( reduction=jm.losses.Reduction.SUM ) result = scce(target, preds) # 2.354 assert np.isclose(result, 2.354, rtol=0.01) # Using 'none' reduction type. scce = jm.losses.Crossentropy( reduction=jm.losses.Reduction.NONE ) result = scce(target, preds) # [0.0513, 2.303] assert jnp.all(np.isclose(result, [0.0513, 2.303], rtol=0.01)) ``` Usage with the `Elegy` API: ```python model = elegy.Model( module_fn, loss=jm.losses.Crossentropy(), metrics=elegy.metrics.Accuracy(), optimizer=optax.adam(1e-3), ) ``` \"\"\" def __init__ ( self , * , from_logits : bool = True , binary : bool = False , label_smoothing : tp . Optional [ float ] = None , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `preds` is expected to be a logits tensor. By default, we assume that `preds` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" super () . __init__ ( reduction = reduction , weight = weight ) self . _from_logits = from_logits self . _binary = binary self . _label_smoothing = label_smoothing def call ( self , target , preds , sample_weight : tp . Optional [ jax . Array ] = None , ** _ , ) -> jax . Array : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: target: Ground truth values. preds: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return crossentropy ( target , preds , binary = self . _binary , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , ) __init__ ( * , from_logits = True , binary = False , label_smoothing = None , reduction = None , weight = None ) Initializes SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether preds is expected to be a logits tensor. By default, we assume that preds encodes a probability distribution. Note - Using from_logits=True is more numerically stable. True reduction tp . Optional [ Reduction ] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight tp . Optional [ float ] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/crossentropy.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def __init__ ( self , * , from_logits : bool = True , binary : bool = False , label_smoothing : tp . Optional [ float ] = None , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `preds` is expected to be a logits tensor. By default, we assume that `preds` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" super () . __init__ ( reduction = reduction , weight = weight ) self . _from_logits = from_logits self . _binary = binary self . _label_smoothing = label_smoothing call ( target , preds , sample_weight = None , ** _ ) Invokes the SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default target Ground truth values. required preds The predicted values. required sample_weight tp . Optional [ jax . Array ] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description jax . Array Loss values per sample. Source code in jax_metrics/losses/crossentropy.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 def call ( self , target , preds , sample_weight : tp . Optional [ jax . Array ] = None , ** _ , ) -> jax . Array : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: target: Ground truth values. preds: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return crossentropy ( target , preds , binary = self . _binary , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , )","title":"Crossentropy"},{"location":"api/losses/Crossentropy/#jax_metricslossescrossentropy","text":"Bases: Loss Computes the crossentropy loss between the target and predictions. Use this crossentropy loss function when there are two or more label classes. We expect target to be provided as integers. If you want to provide target using one-hot representation, please use CategoricalCrossentropy loss. There should be # classes floating point values per feature for preds and a single floating point value per feature for target . In the snippet below, there is a single floating point value per example for target and # classes floating pointing values per example for preds . The shape of target is [batch_size] and the shape of preds is [batch_size, num_classes] . Usage: target = jnp . array ([ 1 , 2 ]) preds = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = jm . losses . Crossentropy () result = scce ( target , preds ) # 1.177 assert np . isclose ( result , 1.177 , rtol = 0.01 ) # Calling with 'sample_weight'. result = scce ( target , preds , sample_weight = jnp . array ([ 0.3 , 0.7 ])) # 0.814 assert np . isclose ( result , 0.814 , rtol = 0.01 ) # Using 'sum' reduction type. scce = jm . losses . Crossentropy ( reduction = jm . losses . Reduction . SUM ) result = scce ( target , preds ) # 2.354 assert np . isclose ( result , 2.354 , rtol = 0.01 ) # Using 'none' reduction type. scce = jm . losses . Crossentropy ( reduction = jm . losses . Reduction . NONE ) result = scce ( target , preds ) # [0.0513, 2.303] assert jnp . all ( np . isclose ( result , [ 0.0513 , 2.303 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . Crossentropy (), metrics = elegy . metrics . Accuracy (), optimizer = optax . adam ( 1e-3 ), ) Source code in jax_metrics/losses/crossentropy.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 class Crossentropy ( Loss ): \"\"\" Computes the crossentropy loss between the target and predictions. Use this crossentropy loss function when there are two or more label classes. We expect target to be provided as integers. If you want to provide target using `one-hot` representation, please use `CategoricalCrossentropy` loss. There should be `# classes` floating point values per feature for `preds` and a single floating point value per feature for `target`. In the snippet below, there is a single floating point value per example for `target` and `# classes` floating pointing values per example for `preds`. The shape of `target` is `[batch_size]` and the shape of `preds` is `[batch_size, num_classes]`. Usage: ```python target = jnp.array([1, 2]) preds = jnp.array([[0.05, 0.95, 0], [0.1, 0.8, 0.1]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = jm.losses.Crossentropy() result = scce(target, preds) # 1.177 assert np.isclose(result, 1.177, rtol=0.01) # Calling with 'sample_weight'. result = scce(target, preds, sample_weight=jnp.array([0.3, 0.7])) # 0.814 assert np.isclose(result, 0.814, rtol=0.01) # Using 'sum' reduction type. scce = jm.losses.Crossentropy( reduction=jm.losses.Reduction.SUM ) result = scce(target, preds) # 2.354 assert np.isclose(result, 2.354, rtol=0.01) # Using 'none' reduction type. scce = jm.losses.Crossentropy( reduction=jm.losses.Reduction.NONE ) result = scce(target, preds) # [0.0513, 2.303] assert jnp.all(np.isclose(result, [0.0513, 2.303], rtol=0.01)) ``` Usage with the `Elegy` API: ```python model = elegy.Model( module_fn, loss=jm.losses.Crossentropy(), metrics=elegy.metrics.Accuracy(), optimizer=optax.adam(1e-3), ) ``` \"\"\" def __init__ ( self , * , from_logits : bool = True , binary : bool = False , label_smoothing : tp . Optional [ float ] = None , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `preds` is expected to be a logits tensor. By default, we assume that `preds` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" super () . __init__ ( reduction = reduction , weight = weight ) self . _from_logits = from_logits self . _binary = binary self . _label_smoothing = label_smoothing def call ( self , target , preds , sample_weight : tp . Optional [ jax . Array ] = None , ** _ , ) -> jax . Array : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: target: Ground truth values. preds: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return crossentropy ( target , preds , binary = self . _binary , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , )","title":"jax_metrics.losses.Crossentropy"},{"location":"api/losses/Crossentropy/#jax_metrics.losses.crossentropy.Crossentropy.__init__","text":"Initializes SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether preds is expected to be a logits tensor. By default, we assume that preds encodes a probability distribution. Note - Using from_logits=True is more numerically stable. True reduction tp . Optional [ Reduction ] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight tp . Optional [ float ] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/crossentropy.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def __init__ ( self , * , from_logits : bool = True , binary : bool = False , label_smoothing : tp . Optional [ float ] = None , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `preds` is expected to be a logits tensor. By default, we assume that `preds` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" super () . __init__ ( reduction = reduction , weight = weight ) self . _from_logits = from_logits self . _binary = binary self . _label_smoothing = label_smoothing","title":"__init__()"},{"location":"api/losses/Crossentropy/#jax_metrics.losses.crossentropy.Crossentropy.call","text":"Invokes the SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default target Ground truth values. required preds The predicted values. required sample_weight tp . Optional [ jax . Array ] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description jax . Array Loss values per sample. Source code in jax_metrics/losses/crossentropy.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 def call ( self , target , preds , sample_weight : tp . Optional [ jax . Array ] = None , ** _ , ) -> jax . Array : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: target: Ground truth values. preds: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return crossentropy ( target , preds , binary = self . _binary , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , )","title":"call()"},{"location":"api/losses/Huber/","text":"jax_metrics.losses.Huber Bases: Loss Computes the Huber loss between target and predictions. For each value x in error = target - preds: loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: target = jnp . array ([[ 0 , 1 ], [ 0 , 0 ]]) preds = jnp . array ([[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. huber_loss = jm . losses . Huber () assert huber_loss ( target , preds ) == 0.155 # Calling with 'sample_weight'. assert ( huber_loss ( target , preds , sample_weight = jnp . array ([ 0.8 , 0.2 ])) == 0.08500001 ) # Using 'sum' reduction type. huber_loss = jm . losses . Huber ( reduction = jm . losses . Reduction . SUM ) assert huber_loss ( target , preds ) == 0.31 # Using 'none' reduction type. huber_loss = jm . losses . Huber ( reduction = jm . losses . Reduction . NONE ) assert jnp . equal ( huber_loss ( target , preds ), jnp . array ([ 0.18 , 0.13000001 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . Huber ( delta = 1.0 ), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/huber.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 class Huber ( Loss ): r \"\"\" Computes the Huber loss between target and predictions. For each value x in error = target - preds: $$ loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} $$ where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: ```python target = jnp.array([[0, 1], [0, 0]]) preds = jnp.array([[0.6, 0.4], [0.4, 0.6]]) # Using 'auto'/'sum_over_batch_size' reduction type. huber_loss = jm.losses.Huber() assert huber_loss(target, preds) == 0.155 # Calling with 'sample_weight'. assert ( huber_loss(target, preds, sample_weight=jnp.array([0.8, 0.2])) == 0.08500001 ) # Using 'sum' reduction type. huber_loss = jm.losses.Huber( reduction=jm.losses.Reduction.SUM ) assert huber_loss(target, preds) == 0.31 # Using 'none' reduction type. huber_loss = jm.losses.Huber( reduction=jm.losses.Reduction.NONE ) assert jnp.equal(huber_loss(target, preds), jnp.array([0.18, 0.13000001])).all() ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.Huber(delta=1.0), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , delta : float = 1.0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `Mean` class. Arguments: delta: (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . delta = delta return super () . __init__ ( reduction = reduction , weight = weight ) def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `Huber` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return huber ( target , preds , self . delta ) __init__ ( delta = 1.0 , reduction = None , weight = None ) Initializes Mean class. Parameters: Name Type Description Default delta float (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. 1.0 reduction tp . Optional [ Reduction ] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight tp . Optional [ float ] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/huber.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 def __init__ ( self , delta : float = 1.0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `Mean` class. Arguments: delta: (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . delta = delta return super () . __init__ ( reduction = reduction , weight = weight ) call ( target , preds , sample_weight = None , ** _ ) Invokes the Huber instance. Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] required sample_weight tp . Optional [ jax . Array ] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description jax . Array Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/huber.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `Huber` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return huber ( target , preds , self . delta )","title":"Huber"},{"location":"api/losses/Huber/#jax_metricslosseshuber","text":"Bases: Loss Computes the Huber loss between target and predictions. For each value x in error = target - preds: loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: target = jnp . array ([[ 0 , 1 ], [ 0 , 0 ]]) preds = jnp . array ([[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. huber_loss = jm . losses . Huber () assert huber_loss ( target , preds ) == 0.155 # Calling with 'sample_weight'. assert ( huber_loss ( target , preds , sample_weight = jnp . array ([ 0.8 , 0.2 ])) == 0.08500001 ) # Using 'sum' reduction type. huber_loss = jm . losses . Huber ( reduction = jm . losses . Reduction . SUM ) assert huber_loss ( target , preds ) == 0.31 # Using 'none' reduction type. huber_loss = jm . losses . Huber ( reduction = jm . losses . Reduction . NONE ) assert jnp . equal ( huber_loss ( target , preds ), jnp . array ([ 0.18 , 0.13000001 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . Huber ( delta = 1.0 ), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/huber.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 class Huber ( Loss ): r \"\"\" Computes the Huber loss between target and predictions. For each value x in error = target - preds: $$ loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} $$ where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: ```python target = jnp.array([[0, 1], [0, 0]]) preds = jnp.array([[0.6, 0.4], [0.4, 0.6]]) # Using 'auto'/'sum_over_batch_size' reduction type. huber_loss = jm.losses.Huber() assert huber_loss(target, preds) == 0.155 # Calling with 'sample_weight'. assert ( huber_loss(target, preds, sample_weight=jnp.array([0.8, 0.2])) == 0.08500001 ) # Using 'sum' reduction type. huber_loss = jm.losses.Huber( reduction=jm.losses.Reduction.SUM ) assert huber_loss(target, preds) == 0.31 # Using 'none' reduction type. huber_loss = jm.losses.Huber( reduction=jm.losses.Reduction.NONE ) assert jnp.equal(huber_loss(target, preds), jnp.array([0.18, 0.13000001])).all() ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.Huber(delta=1.0), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , delta : float = 1.0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `Mean` class. Arguments: delta: (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . delta = delta return super () . __init__ ( reduction = reduction , weight = weight ) def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `Huber` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return huber ( target , preds , self . delta )","title":"jax_metrics.losses.Huber"},{"location":"api/losses/Huber/#jax_metrics.losses.huber.Huber.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default delta float (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. 1.0 reduction tp . Optional [ Reduction ] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight tp . Optional [ float ] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/huber.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 def __init__ ( self , delta : float = 1.0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `Mean` class. Arguments: delta: (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . delta = delta return super () . __init__ ( reduction = reduction , weight = weight )","title":"__init__()"},{"location":"api/losses/Huber/#jax_metrics.losses.huber.Huber.call","text":"Invokes the Huber instance. Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] required sample_weight tp . Optional [ jax . Array ] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description jax . Array Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/huber.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `Huber` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return huber ( target , preds , self . delta )","title":"call()"},{"location":"api/losses/Loss/","text":"jax_metrics.losses.Loss Bases: ABC Loss base class. To be implemented by subclasses: call() : Contains the logic for loss calculation. Example subclass implementation: class MeanSquaredError ( Loss ): def call ( self , target , preds ): return jnp . mean ( jnp . square ( preds - target ), axis =- 1 ) Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this. Source code in jax_metrics/losses/loss.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 class Loss ( ABC ): \"\"\" Loss base class. To be implemented by subclasses: * `call()`: Contains the logic for loss calculation. Example subclass implementation: ```python class MeanSquaredError(Loss): def call(self, target, preds): return jnp.mean(jnp.square(preds - target), axis=-1) ``` Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this. \"\"\" def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ types . ScalarLike ] = None , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . weight = ( jnp . asarray ( weight , dtype = jnp . float32 ) if weight is not None else jnp . array ( 1.0 , dtype = jnp . float32 ) ) self . reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) def __call__ ( self , ** kwargs , ) -> jax . Array : sample_weight : tp . Optional [ jax . Array ] = kwargs . pop ( \"sample_weight\" , None ) values = self . call ( ** kwargs ) return reduce_loss ( values , sample_weight , self . weight , self . reduction ) @abstractmethod def call ( self , ** kwargs ) -> jax . Array : ... def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedLoss\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python losses = jm.Losses([ jm.losses.Crossentropy().index_into(target=[\"a\"]), jm.losses.Crossentropy().index_into(target=[\"b\"]), ]).reset() losses = losses.update( target={ \"a\": y0, \"b\": y1, }, preds=logits, ) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedLoss instance \"\"\" return IndexedLoss ( self , kwargs ) def rename_arguments ( self , ** kwargs : str ) -> \"MapArgsLoss\" : \"\"\" Returns a loss that renames the keyword arguments expected by `__call__`. Example: ```python crossentropy_loss = jm.losses.Crossentropy().rename_arguments(target=\"y_true\", preds=\"y_pred\") ... loss = crossentropy_loss(y_true=y, y_pred=logits) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A MapArgsLoss instance \"\"\" return MapArgsLoss ( self , kwargs ) __init__ ( reduction = None , weight = None ) Initializes Loss class. Parameters: Name Type Description Default reduction tp . Optional [ Reduction ] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight tp . Optional [ types . ScalarLike ] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/loss.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ types . ScalarLike ] = None , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . weight = ( jnp . asarray ( weight , dtype = jnp . float32 ) if weight is not None else jnp . array ( 1.0 , dtype = jnp . float32 ) ) self . reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) index_into ( ** kwargs ) Returns a metric that \"indexes\" the specified keyword arguments expected by .update() . You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing ( __getitem__ ). Example: losses = jm . Losses ([ jm . losses . Crossentropy () . index_into ( target = [ \"a\" ]), jm . losses . Crossentropy () . index_into ( target = [ \"b\" ]), ]) . reset () losses = losses . update ( target = { \"a\" : y0 , \"b\" : y1 , }, preds = logits , ) Here values is set to a dict of arrays, but thanks to .index_into() each loss can index into its correspoding array. Parameters: Name Type Description Default **kwargs types . IndexLike keyword arguments to be indexed {} Returns: Type Description IndexedLoss A IndexedLoss instance Source code in jax_metrics/losses/loss.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedLoss\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python losses = jm.Losses([ jm.losses.Crossentropy().index_into(target=[\"a\"]), jm.losses.Crossentropy().index_into(target=[\"b\"]), ]).reset() losses = losses.update( target={ \"a\": y0, \"b\": y1, }, preds=logits, ) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedLoss instance \"\"\" return IndexedLoss ( self , kwargs ) rename_arguments ( ** kwargs ) Returns a loss that renames the keyword arguments expected by __call__ . Example: crossentropy_loss = jm . losses . Crossentropy () . rename_arguments ( target = \"y_true\" , preds = \"y_pred\" ) ... loss = crossentropy_loss ( y_true = y , y_pred = logits ) Parameters: Name Type Description Default **kwargs str keyword arguments to be renamed {} Returns: Type Description MapArgsLoss A MapArgsLoss instance Source code in jax_metrics/losses/loss.py 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def rename_arguments ( self , ** kwargs : str ) -> \"MapArgsLoss\" : \"\"\" Returns a loss that renames the keyword arguments expected by `__call__`. Example: ```python crossentropy_loss = jm.losses.Crossentropy().rename_arguments(target=\"y_true\", preds=\"y_pred\") ... loss = crossentropy_loss(y_true=y, y_pred=logits) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A MapArgsLoss instance \"\"\" return MapArgsLoss ( self , kwargs )","title":"Loss"},{"location":"api/losses/Loss/#jax_metricslossesloss","text":"Bases: ABC Loss base class. To be implemented by subclasses: call() : Contains the logic for loss calculation. Example subclass implementation: class MeanSquaredError ( Loss ): def call ( self , target , preds ): return jnp . mean ( jnp . square ( preds - target ), axis =- 1 ) Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this. Source code in jax_metrics/losses/loss.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 class Loss ( ABC ): \"\"\" Loss base class. To be implemented by subclasses: * `call()`: Contains the logic for loss calculation. Example subclass implementation: ```python class MeanSquaredError(Loss): def call(self, target, preds): return jnp.mean(jnp.square(preds - target), axis=-1) ``` Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this. \"\"\" def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ types . ScalarLike ] = None , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . weight = ( jnp . asarray ( weight , dtype = jnp . float32 ) if weight is not None else jnp . array ( 1.0 , dtype = jnp . float32 ) ) self . reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) def __call__ ( self , ** kwargs , ) -> jax . Array : sample_weight : tp . Optional [ jax . Array ] = kwargs . pop ( \"sample_weight\" , None ) values = self . call ( ** kwargs ) return reduce_loss ( values , sample_weight , self . weight , self . reduction ) @abstractmethod def call ( self , ** kwargs ) -> jax . Array : ... def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedLoss\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python losses = jm.Losses([ jm.losses.Crossentropy().index_into(target=[\"a\"]), jm.losses.Crossentropy().index_into(target=[\"b\"]), ]).reset() losses = losses.update( target={ \"a\": y0, \"b\": y1, }, preds=logits, ) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedLoss instance \"\"\" return IndexedLoss ( self , kwargs ) def rename_arguments ( self , ** kwargs : str ) -> \"MapArgsLoss\" : \"\"\" Returns a loss that renames the keyword arguments expected by `__call__`. Example: ```python crossentropy_loss = jm.losses.Crossentropy().rename_arguments(target=\"y_true\", preds=\"y_pred\") ... loss = crossentropy_loss(y_true=y, y_pred=logits) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A MapArgsLoss instance \"\"\" return MapArgsLoss ( self , kwargs )","title":"jax_metrics.losses.Loss"},{"location":"api/losses/Loss/#jax_metrics.losses.loss.Loss.__init__","text":"Initializes Loss class. Parameters: Name Type Description Default reduction tp . Optional [ Reduction ] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight tp . Optional [ types . ScalarLike ] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/loss.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ types . ScalarLike ] = None , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . weight = ( jnp . asarray ( weight , dtype = jnp . float32 ) if weight is not None else jnp . array ( 1.0 , dtype = jnp . float32 ) ) self . reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE )","title":"__init__()"},{"location":"api/losses/Loss/#jax_metrics.losses.loss.Loss.index_into","text":"Returns a metric that \"indexes\" the specified keyword arguments expected by .update() . You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing ( __getitem__ ). Example: losses = jm . Losses ([ jm . losses . Crossentropy () . index_into ( target = [ \"a\" ]), jm . losses . Crossentropy () . index_into ( target = [ \"b\" ]), ]) . reset () losses = losses . update ( target = { \"a\" : y0 , \"b\" : y1 , }, preds = logits , ) Here values is set to a dict of arrays, but thanks to .index_into() each loss can index into its correspoding array. Parameters: Name Type Description Default **kwargs types . IndexLike keyword arguments to be indexed {} Returns: Type Description IndexedLoss A IndexedLoss instance Source code in jax_metrics/losses/loss.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedLoss\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python losses = jm.Losses([ jm.losses.Crossentropy().index_into(target=[\"a\"]), jm.losses.Crossentropy().index_into(target=[\"b\"]), ]).reset() losses = losses.update( target={ \"a\": y0, \"b\": y1, }, preds=logits, ) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedLoss instance \"\"\" return IndexedLoss ( self , kwargs )","title":"index_into()"},{"location":"api/losses/Loss/#jax_metrics.losses.loss.Loss.rename_arguments","text":"Returns a loss that renames the keyword arguments expected by __call__ . Example: crossentropy_loss = jm . losses . Crossentropy () . rename_arguments ( target = \"y_true\" , preds = \"y_pred\" ) ... loss = crossentropy_loss ( y_true = y , y_pred = logits ) Parameters: Name Type Description Default **kwargs str keyword arguments to be renamed {} Returns: Type Description MapArgsLoss A MapArgsLoss instance Source code in jax_metrics/losses/loss.py 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def rename_arguments ( self , ** kwargs : str ) -> \"MapArgsLoss\" : \"\"\" Returns a loss that renames the keyword arguments expected by `__call__`. Example: ```python crossentropy_loss = jm.losses.Crossentropy().rename_arguments(target=\"y_true\", preds=\"y_pred\") ... loss = crossentropy_loss(y_true=y, y_pred=logits) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A MapArgsLoss instance \"\"\" return MapArgsLoss ( self , kwargs )","title":"rename_arguments()"},{"location":"api/losses/MeanAbsoluteError/","text":"jax_metrics.losses.MeanAbsoluteError Bases: Loss Computes the mean absolute errors between target and predictions. loss = mean(abs(target - preds)) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mae = jm . losses . MeanAbsoluteError () assert mae ( target , preds ) == 0.5 # Calling with 'sample_weight'. assert mae ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mae = jm . losses . MeanAbsoluteError ( reduction = jm . losses . Reduction . SUM ) assert mae ( target , preds ) == 1.0 # Using 'none' reduction type. mae = jm . losses . MeanAbsoluteError ( reduction = jm . losses . Reduction . NONE ) assert list ( mae ( target , preds )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanAbsoluteError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_absolute_error.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 class MeanAbsoluteError ( Loss ): \"\"\" Computes the mean absolute errors between target and predictions. `loss = mean(abs(target - preds))` Usage: ```python target = jnp.array([[0.0, 1.0], [0.0, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. mae = jm.losses.MeanAbsoluteError() assert mae(target, preds) == 0.5 # Calling with 'sample_weight'. assert mae(target, preds, sample_weight=jnp.array([0.7, 0.3])) == 0.25 # Using 'sum' reduction type. mae = jm.losses.MeanAbsoluteError(reduction=jm.losses.Reduction.SUM) assert mae(target, preds) == 1.0 # Using 'none' reduction type. mae = jm.losses.MeanAbsoluteError(reduction=jm.losses.Reduction.NONE) assert list(mae(target, preds)) == [0.5, 0.5] ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanAbsoluteError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanAbsoluteError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_error ( target , preds ) call ( target , preds , sample_weight = None , ** _ ) Invokes the MeanAbsoluteError instance. Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] required sample_weight tp . Optional [ jax . Array ] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description jax . Array Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_absolute_error.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanAbsoluteError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_error ( target , preds )","title":"MeanAbsoluteError"},{"location":"api/losses/MeanAbsoluteError/#jax_metricslossesmeanabsoluteerror","text":"Bases: Loss Computes the mean absolute errors between target and predictions. loss = mean(abs(target - preds)) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mae = jm . losses . MeanAbsoluteError () assert mae ( target , preds ) == 0.5 # Calling with 'sample_weight'. assert mae ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mae = jm . losses . MeanAbsoluteError ( reduction = jm . losses . Reduction . SUM ) assert mae ( target , preds ) == 1.0 # Using 'none' reduction type. mae = jm . losses . MeanAbsoluteError ( reduction = jm . losses . Reduction . NONE ) assert list ( mae ( target , preds )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanAbsoluteError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_absolute_error.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 class MeanAbsoluteError ( Loss ): \"\"\" Computes the mean absolute errors between target and predictions. `loss = mean(abs(target - preds))` Usage: ```python target = jnp.array([[0.0, 1.0], [0.0, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. mae = jm.losses.MeanAbsoluteError() assert mae(target, preds) == 0.5 # Calling with 'sample_weight'. assert mae(target, preds, sample_weight=jnp.array([0.7, 0.3])) == 0.25 # Using 'sum' reduction type. mae = jm.losses.MeanAbsoluteError(reduction=jm.losses.Reduction.SUM) assert mae(target, preds) == 1.0 # Using 'none' reduction type. mae = jm.losses.MeanAbsoluteError(reduction=jm.losses.Reduction.NONE) assert list(mae(target, preds)) == [0.5, 0.5] ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanAbsoluteError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanAbsoluteError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_error ( target , preds )","title":"jax_metrics.losses.MeanAbsoluteError"},{"location":"api/losses/MeanAbsoluteError/#jax_metrics.losses.mean_absolute_error.MeanAbsoluteError.call","text":"Invokes the MeanAbsoluteError instance. Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] required sample_weight tp . Optional [ jax . Array ] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description jax . Array Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_absolute_error.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanAbsoluteError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_error ( target , preds )","title":"call()"},{"location":"api/losses/MeanAbsolutePercentageError/","text":"jax_metrics.losses.MeanAbsolutePercentageError Bases: Loss Computes the mean absolute errors between target and predictions. loss = mean(abs((target - preds) / target)) Usage: target = jnp . array ([[ 1.0 , 1.0 ], [ 0.9 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mape = jm . losses . MeanAbsolutePercentageError () result = mape ( target , preds ) assert np . isclose ( result , 2.78 , rtol = 0.01 ) # Calling with 'sample_weight'. assert np . isclose ( mape ( target , preds , sample_weight = jnp . array ([ 0.1 , 0.9 ])), 2.5 , rtol = 0.01 ) # Using 'sum' reduction type. mape = jm . losses . MeanAbsolutePercentageError ( reduction = jm . losses . Reduction . SUM ) assert np . isclose ( mape ( target , preds ), 5.6 , rtol = 0.01 ) # Using 'none' reduction type. mape = jm . losses . MeanAbsolutePercentageError ( reduction = jm . losses . Reduction . NONE ) assert jnp . all ( np . isclose ( result , [ 0. , 5.6 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanAbsolutePercentageError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_absolute_percentage_error.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 class MeanAbsolutePercentageError ( Loss ): \"\"\" Computes the mean absolute errors between target and predictions. `loss = mean(abs((target - preds) / target))` Usage: ```python target = jnp.array([[1.0, 1.0], [0.9, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. mape = jm.losses.MeanAbsolutePercentageError() result = mape(target, preds) assert np.isclose(result, 2.78, rtol=0.01) # Calling with 'sample_weight'. assert np.isclose(mape(target, preds, sample_weight=jnp.array([0.1, 0.9])), 2.5, rtol=0.01) # Using 'sum' reduction type. mape = jm.losses.MeanAbsolutePercentageError(reduction=jm.losses.Reduction.SUM) assert np.isclose(mape(target, preds), 5.6, rtol=0.01) # Using 'none' reduction type. mape = jm.losses.MeanAbsolutePercentageError(reduction=jm.losses.Reduction.NONE) assert jnp.all(np.isclose(result, [0. , 5.6], rtol=0.01)) ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanAbsolutePercentageError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanAbsolutePercentageError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_percentage_error ( target , preds ) call ( target , preds , sample_weight = None , ** _ ) Invokes the MeanAbsolutePercentageError instance. Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] required sample_weight tp . Optional [ jax . Array ] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description jax . Array Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_absolute_percentage_error.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanAbsolutePercentageError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_percentage_error ( target , preds )","title":"MeanAbsolutePercentageError"},{"location":"api/losses/MeanAbsolutePercentageError/#jax_metricslossesmeanabsolutepercentageerror","text":"Bases: Loss Computes the mean absolute errors between target and predictions. loss = mean(abs((target - preds) / target)) Usage: target = jnp . array ([[ 1.0 , 1.0 ], [ 0.9 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mape = jm . losses . MeanAbsolutePercentageError () result = mape ( target , preds ) assert np . isclose ( result , 2.78 , rtol = 0.01 ) # Calling with 'sample_weight'. assert np . isclose ( mape ( target , preds , sample_weight = jnp . array ([ 0.1 , 0.9 ])), 2.5 , rtol = 0.01 ) # Using 'sum' reduction type. mape = jm . losses . MeanAbsolutePercentageError ( reduction = jm . losses . Reduction . SUM ) assert np . isclose ( mape ( target , preds ), 5.6 , rtol = 0.01 ) # Using 'none' reduction type. mape = jm . losses . MeanAbsolutePercentageError ( reduction = jm . losses . Reduction . NONE ) assert jnp . all ( np . isclose ( result , [ 0. , 5.6 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanAbsolutePercentageError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_absolute_percentage_error.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 class MeanAbsolutePercentageError ( Loss ): \"\"\" Computes the mean absolute errors between target and predictions. `loss = mean(abs((target - preds) / target))` Usage: ```python target = jnp.array([[1.0, 1.0], [0.9, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. mape = jm.losses.MeanAbsolutePercentageError() result = mape(target, preds) assert np.isclose(result, 2.78, rtol=0.01) # Calling with 'sample_weight'. assert np.isclose(mape(target, preds, sample_weight=jnp.array([0.1, 0.9])), 2.5, rtol=0.01) # Using 'sum' reduction type. mape = jm.losses.MeanAbsolutePercentageError(reduction=jm.losses.Reduction.SUM) assert np.isclose(mape(target, preds), 5.6, rtol=0.01) # Using 'none' reduction type. mape = jm.losses.MeanAbsolutePercentageError(reduction=jm.losses.Reduction.NONE) assert jnp.all(np.isclose(result, [0. , 5.6], rtol=0.01)) ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanAbsolutePercentageError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanAbsolutePercentageError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_percentage_error ( target , preds )","title":"jax_metrics.losses.MeanAbsolutePercentageError"},{"location":"api/losses/MeanAbsolutePercentageError/#jax_metrics.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError.call","text":"Invokes the MeanAbsolutePercentageError instance. Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] required sample_weight tp . Optional [ jax . Array ] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description jax . Array Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_absolute_percentage_error.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanAbsolutePercentageError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_percentage_error ( target , preds )","title":"call()"},{"location":"api/losses/MeanSquaredError/","text":"jax_metrics.losses.MeanSquaredError Bases: Loss Computes the mean of squares of errors between target and predictions. loss = square(target - preds) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = jm . losses . MeanSquaredError () assert mse ( target , preds ) == 0.5 # Calling with 'sample_weight'. assert mse ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mse = jm . losses . MeanSquaredError ( reduction = jm . losses . Reduction . SUM ) assert mse ( target , preds ) == 1.0 # Using 'none' reduction type. mse = jm . losses . MeanSquaredError ( reduction = jm . losses . Reduction . NONE ) assert list ( mse ( target , preds )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanSquaredError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_squared_error.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 class MeanSquaredError ( Loss ): \"\"\" Computes the mean of squares of errors between target and predictions. `loss = square(target - preds)` Usage: ```python target = jnp.array([[0.0, 1.0], [0.0, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = jm.losses.MeanSquaredError() assert mse(target, preds) == 0.5 # Calling with 'sample_weight'. assert mse(target, preds, sample_weight=jnp.array([0.7, 0.3])) == 0.25 # Using 'sum' reduction type. mse = jm.losses.MeanSquaredError(reduction=jm.losses.Reduction.SUM) assert mse(target, preds) == 1.0 # Using 'none' reduction type. mse = jm.losses.MeanSquaredError(reduction=jm.losses.Reduction.NONE) assert list(mse(target, preds)) == [0.5, 0.5] ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanSquaredError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( target , preds ) call ( target , preds , sample_weight = None , ** _ ) Invokes the MeanSquaredError instance. Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] required sample_weight tp . Optional [ jax . Array ] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description jax . Array Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_squared_error.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( target , preds )","title":"MeanSquaredError"},{"location":"api/losses/MeanSquaredError/#jax_metricslossesmeansquarederror","text":"Bases: Loss Computes the mean of squares of errors between target and predictions. loss = square(target - preds) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = jm . losses . MeanSquaredError () assert mse ( target , preds ) == 0.5 # Calling with 'sample_weight'. assert mse ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mse = jm . losses . MeanSquaredError ( reduction = jm . losses . Reduction . SUM ) assert mse ( target , preds ) == 1.0 # Using 'none' reduction type. mse = jm . losses . MeanSquaredError ( reduction = jm . losses . Reduction . NONE ) assert list ( mse ( target , preds )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanSquaredError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_squared_error.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 class MeanSquaredError ( Loss ): \"\"\" Computes the mean of squares of errors between target and predictions. `loss = square(target - preds)` Usage: ```python target = jnp.array([[0.0, 1.0], [0.0, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = jm.losses.MeanSquaredError() assert mse(target, preds) == 0.5 # Calling with 'sample_weight'. assert mse(target, preds, sample_weight=jnp.array([0.7, 0.3])) == 0.25 # Using 'sum' reduction type. mse = jm.losses.MeanSquaredError(reduction=jm.losses.Reduction.SUM) assert mse(target, preds) == 1.0 # Using 'none' reduction type. mse = jm.losses.MeanSquaredError(reduction=jm.losses.Reduction.NONE) assert list(mse(target, preds)) == [0.5, 0.5] ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanSquaredError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( target , preds )","title":"jax_metrics.losses.MeanSquaredError"},{"location":"api/losses/MeanSquaredError/#jax_metrics.losses.mean_squared_error.MeanSquaredError.call","text":"Invokes the MeanSquaredError instance. Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] required sample_weight tp . Optional [ jax . Array ] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description jax . Array Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_squared_error.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( target , preds )","title":"call()"},{"location":"api/losses/MeanSquaredLogarithmicError/","text":"jax_metrics.losses.MeanSquaredLogarithmicError Bases: Loss Computes the mean squared logarithmic errors between target and predictions. loss = mean(square(log(target + 1) - log(preds + 1)), axis=-1) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. msle = jm . losses . MeanSquaredLogarithmicError () assert msle ( target , preds ) == 0.24022643 # Calling with 'sample_weight'. assert msle ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) = 0.12011322 # Using 'sum' reduction type. msle = jm . losses . MeanSquaredLogarithmicError ( reduction = jm . losses . Reduction . SUM ) assert msle ( target , preds ) == 0.48045287 # Using 'none' reduction type. msle = jm . losses . MeanSquaredLogarithmicError ( reduction = jm . losses . Reduction . NONE ) assert jnp . equal ( msle ( target , preds ), jnp . array ([ 0.24022643 , 0.24022643 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanSquaredLogarithmicError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_squared_logarithmic_error.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 class MeanSquaredLogarithmicError ( Loss ): \"\"\" Computes the mean squared logarithmic errors between target and predictions. `loss = mean(square(log(target + 1) - log(preds + 1)), axis=-1)` Usage: ```python target = jnp.array([[0.0, 1.0], [0.0, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. msle = jm.losses.MeanSquaredLogarithmicError() assert msle(target, preds) == 0.24022643 # Calling with 'sample_weight'. assert msle(target, preds, sample_weight=jnp.array([0.7, 0.3])) = 0.12011322 # Using 'sum' reduction type. msle = jm.losses.MeanSquaredLogarithmicError(reduction=jm.losses.Reduction.SUM) assert msle(target, preds) == 0.48045287 # Using 'none' reduction type. msle = jm.losses.MeanSquaredLogarithmicError(reduction=jm.losses.Reduction.NONE) assert jnp.equal(msle(target, preds), jnp.array([0.24022643, 0.24022643])).all() ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanSquaredLogarithmicError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanSquaredLogarithmicError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_logarithmic_error ( target , preds ) call ( target , preds , sample_weight = None , ** _ ) Invokes the MeanSquaredLogarithmicError instance. Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] required sample_weight tp . Optional [ jax . Array ] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description jax . Array Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_squared_logarithmic_error.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanSquaredLogarithmicError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_logarithmic_error ( target , preds )","title":"MeanSquaredLogarithmicError"},{"location":"api/losses/MeanSquaredLogarithmicError/#jax_metricslossesmeansquaredlogarithmicerror","text":"Bases: Loss Computes the mean squared logarithmic errors between target and predictions. loss = mean(square(log(target + 1) - log(preds + 1)), axis=-1) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. msle = jm . losses . MeanSquaredLogarithmicError () assert msle ( target , preds ) == 0.24022643 # Calling with 'sample_weight'. assert msle ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) = 0.12011322 # Using 'sum' reduction type. msle = jm . losses . MeanSquaredLogarithmicError ( reduction = jm . losses . Reduction . SUM ) assert msle ( target , preds ) == 0.48045287 # Using 'none' reduction type. msle = jm . losses . MeanSquaredLogarithmicError ( reduction = jm . losses . Reduction . NONE ) assert jnp . equal ( msle ( target , preds ), jnp . array ([ 0.24022643 , 0.24022643 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanSquaredLogarithmicError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_squared_logarithmic_error.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 class MeanSquaredLogarithmicError ( Loss ): \"\"\" Computes the mean squared logarithmic errors between target and predictions. `loss = mean(square(log(target + 1) - log(preds + 1)), axis=-1)` Usage: ```python target = jnp.array([[0.0, 1.0], [0.0, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. msle = jm.losses.MeanSquaredLogarithmicError() assert msle(target, preds) == 0.24022643 # Calling with 'sample_weight'. assert msle(target, preds, sample_weight=jnp.array([0.7, 0.3])) = 0.12011322 # Using 'sum' reduction type. msle = jm.losses.MeanSquaredLogarithmicError(reduction=jm.losses.Reduction.SUM) assert msle(target, preds) == 0.48045287 # Using 'none' reduction type. msle = jm.losses.MeanSquaredLogarithmicError(reduction=jm.losses.Reduction.NONE) assert jnp.equal(msle(target, preds), jnp.array([0.24022643, 0.24022643])).all() ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanSquaredLogarithmicError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanSquaredLogarithmicError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_logarithmic_error ( target , preds )","title":"jax_metrics.losses.MeanSquaredLogarithmicError"},{"location":"api/losses/MeanSquaredLogarithmicError/#jax_metrics.losses.mean_squared_logarithmic_error.MeanSquaredLogarithmicError.call","text":"Invokes the MeanSquaredLogarithmicError instance. Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] required sample_weight tp . Optional [ jax . Array ] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description jax . Array Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_squared_logarithmic_error.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanSquaredLogarithmicError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_logarithmic_error ( target , preds )","title":"call()"},{"location":"api/losses/Reduction/","text":"jax_metrics.losses.Reduction Bases: Enum Types of loss reduction. Contains the following values: * NONE : Weighted losses with one dimension reduced (axis=-1, or axis specified by loss function). When this reduction type used with built-in Keras training loops like fit / evaluate , the unreduced vector loss is passed to the optimizer but the reported loss will be a scalar value. * SUM : Scalar sum of weighted losses. * SUM_OVER_BATCH_SIZE : Scalar SUM divided by number of elements in losses. Source code in jax_metrics/losses/loss.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class Reduction ( Enum ): \"\"\" Types of loss reduction. Contains the following values: * `NONE`: Weighted losses with one dimension reduced (axis=-1, or axis specified by loss function). When this reduction type used with built-in Keras training loops like `fit`/`evaluate`, the unreduced vector loss is passed to the optimizer but the reported loss will be a scalar value. * `SUM`: Scalar sum of weighted losses. * `SUM_OVER_BATCH_SIZE`: Scalar `SUM` divided by number of elements in losses. \"\"\" # AUTO = \"auto\" NONE = \"none\" SUM = \"sum\" SUM_OVER_BATCH_SIZE = \"sum_over_batch_size\" @classmethod def all ( cls ): return ( # cls.AUTO, cls . NONE , cls . SUM , cls . SUM_OVER_BATCH_SIZE , ) @classmethod def validate ( cls , key ): if key not in cls . all (): raise ValueError ( \"Invalid Reduction Key %s .\" % key )","title":"Reduction"},{"location":"api/losses/Reduction/#jax_metricslossesreduction","text":"Bases: Enum Types of loss reduction. Contains the following values: * NONE : Weighted losses with one dimension reduced (axis=-1, or axis specified by loss function). When this reduction type used with built-in Keras training loops like fit / evaluate , the unreduced vector loss is passed to the optimizer but the reported loss will be a scalar value. * SUM : Scalar sum of weighted losses. * SUM_OVER_BATCH_SIZE : Scalar SUM divided by number of elements in losses. Source code in jax_metrics/losses/loss.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class Reduction ( Enum ): \"\"\" Types of loss reduction. Contains the following values: * `NONE`: Weighted losses with one dimension reduced (axis=-1, or axis specified by loss function). When this reduction type used with built-in Keras training loops like `fit`/`evaluate`, the unreduced vector loss is passed to the optimizer but the reported loss will be a scalar value. * `SUM`: Scalar sum of weighted losses. * `SUM_OVER_BATCH_SIZE`: Scalar `SUM` divided by number of elements in losses. \"\"\" # AUTO = \"auto\" NONE = \"none\" SUM = \"sum\" SUM_OVER_BATCH_SIZE = \"sum_over_batch_size\" @classmethod def all ( cls ): return ( # cls.AUTO, cls . NONE , cls . SUM , cls . SUM_OVER_BATCH_SIZE , ) @classmethod def validate ( cls , key ): if key not in cls . all (): raise ValueError ( \"Invalid Reduction Key %s .\" % key )","title":"jax_metrics.losses.Reduction"},{"location":"api/losses/cosine_similarity/","text":"jax_metrics.losses.cosine_similarity CosineSimilarity Bases: Loss Computes the mean squared logarithmic errors between target and predictions. loss = -sum(l2_norm(target) * l2_norm(preds)) Usage: target = jnp . array ([[ 0. , 1. ], [ 1. , 1. ]]) preds = jnp . array ([[ 1. , 0. ], [ 1. , 1. ]]) # Using 'auto'/'sum_over_batch_size' reduction type. cosine_loss = jm . losses . CosineSimilarity ( axis = 1 ) assert cosine_loss ( target , preds ) == - 0.49999997 # Calling with 'sample_weight'. assert cosine_loss ( target , preds , sample_weight = jnp . array ([ 0.8 , 0.2 ])) == - 0.099999994 # Using 'sum' reduction type. cosine_loss = jm . losses . CosineSimilarity ( axis = 1 , reduction = jm . losses . Reduction . SUM ) assert cosine_loss ( target , preds ) == - 0.99999994 # Using 'none' reduction type. cosine_loss = jm . losses . CosineSimilarity ( axis = 1 , reduction = jm . losses . Reduction . NONE ) assert jnp . equal ( cosine_loss ( target , preds ), jnp . array ([ - 0. , - 0.99999994 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . CosineSimilarity ( axis = 1 ), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/cosine_similarity.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 class CosineSimilarity ( Loss ): \"\"\" Computes the mean squared logarithmic errors between target and predictions. `loss = -sum(l2_norm(target) * l2_norm(preds))` Usage: ```python target = jnp.array([[0., 1.], [1., 1.]]) preds = jnp.array([[1., 0.], [1., 1.]]) # Using 'auto'/'sum_over_batch_size' reduction type. cosine_loss = jm.losses.CosineSimilarity(axis=1) assert cosine_loss(target, preds) == -0.49999997 # Calling with 'sample_weight'. assert cosine_loss(target, preds, sample_weight=jnp.array([0.8, 0.2])) == -0.099999994 # Using 'sum' reduction type. cosine_loss = jm.losses.CosineSimilarity(axis=1, reduction=jm.losses.Reduction.SUM ) assert cosine_loss(target, preds) == -0.99999994 # Using 'none' reduction type. cosine_loss = jm.losses.CosineSimilarity(axis=1, reduction=jm.losses.Reduction.NONE ) assert jnp.equal(cosine_loss(target, preds), jnp.array([-0., -0.99999994])).all() ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.CosineSimilarity(axis=1), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , axis : int = - 1 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `Mean` class. Arguments: axis: (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . axis = axis return super () . __init__ ( reduction = reduction , weight = weight ) def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `CosineSimilarity` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return cosine_similarity ( target , preds , self . axis ) __init__ ( axis =- 1 , reduction = None , weight = None ) Initializes Mean class. Parameters: Name Type Description Default axis int (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. -1 reduction tp . Optional [ Reduction ] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight tp . Optional [ float ] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/cosine_similarity.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def __init__ ( self , axis : int = - 1 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `Mean` class. Arguments: axis: (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . axis = axis return super () . __init__ ( reduction = reduction , weight = weight ) call ( target , preds , sample_weight = None , ** _ ) Invokes the CosineSimilarity instance. Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] required sample_weight tp . Optional [ jax . Array ] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description jax . Array Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/cosine_similarity.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `CosineSimilarity` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return cosine_similarity ( target , preds , self . axis ) cosine_similarity ( target , preds , axis ) Computes the cosine similarity between target and predictions. loss = - sum ( l2_norm ( target ) * l2_norm ( preds )) Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = jm . losses . cosine_similarity ( target , preds , axis = 1 ) assert loss . shape == ( 2 ,) target = target / jnp . maximum ( jnp . linalg . norm ( target , axis = 1 , keepdims = True ), jnp . sqrt ( types . EPSILON )) preds = preds / jnp . maximum ( jnp . linalg . norm ( preds , axis = 1 , keepdims = True ), jnp . sqrt ( types . EPSILON )) assert jnp . array_equal ( loss , - jnp . sum ( target * preds , axis = 1 )) Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] . required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] . required axis int The dimension along which the cosine similarity is computed. required Returns: Type Description jax . Array cosine similarity Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Source code in jax_metrics/losses/cosine_similarity.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def cosine_similarity ( target : jax . Array , preds : jax . Array , axis : int ) -> jax . Array : \"\"\" Computes the cosine similarity between target and predictions. ```python loss = -sum(l2_norm(target) * l2_norm(preds)) ``` Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = jm.losses.cosine_similarity(target, preds, axis=1) assert loss.shape == (2,) target = target / jnp.maximum(jnp.linalg.norm(target, axis=1, keepdims=True), jnp.sqrt(types.EPSILON)) preds = preds / jnp.maximum(jnp.linalg.norm(preds, axis=1, keepdims=True), jnp.sqrt(types.EPSILON)) assert jnp.array_equal(loss, -jnp.sum(target * preds, axis=1)) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. axis: The dimension along which the cosine similarity is computed. Returns: cosine similarity Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) \"\"\" target = target / jnp . maximum ( jnp . linalg . norm ( target , axis = axis , keepdims = True ), jnp . sqrt ( types . EPSILON ) ) preds = preds / jnp . maximum ( jnp . linalg . norm ( preds , axis = axis , keepdims = True ), jnp . sqrt ( types . EPSILON ) ) return - jnp . sum ( target * preds , axis = axis )","title":"cosine_similarity"},{"location":"api/losses/cosine_similarity/#jax_metricslossescosine_similarity","text":"","title":"jax_metrics.losses.cosine_similarity"},{"location":"api/losses/cosine_similarity/#jax_metrics.losses.cosine_similarity.CosineSimilarity","text":"Bases: Loss Computes the mean squared logarithmic errors between target and predictions. loss = -sum(l2_norm(target) * l2_norm(preds)) Usage: target = jnp . array ([[ 0. , 1. ], [ 1. , 1. ]]) preds = jnp . array ([[ 1. , 0. ], [ 1. , 1. ]]) # Using 'auto'/'sum_over_batch_size' reduction type. cosine_loss = jm . losses . CosineSimilarity ( axis = 1 ) assert cosine_loss ( target , preds ) == - 0.49999997 # Calling with 'sample_weight'. assert cosine_loss ( target , preds , sample_weight = jnp . array ([ 0.8 , 0.2 ])) == - 0.099999994 # Using 'sum' reduction type. cosine_loss = jm . losses . CosineSimilarity ( axis = 1 , reduction = jm . losses . Reduction . SUM ) assert cosine_loss ( target , preds ) == - 0.99999994 # Using 'none' reduction type. cosine_loss = jm . losses . CosineSimilarity ( axis = 1 , reduction = jm . losses . Reduction . NONE ) assert jnp . equal ( cosine_loss ( target , preds ), jnp . array ([ - 0. , - 0.99999994 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . CosineSimilarity ( axis = 1 ), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/cosine_similarity.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 class CosineSimilarity ( Loss ): \"\"\" Computes the mean squared logarithmic errors between target and predictions. `loss = -sum(l2_norm(target) * l2_norm(preds))` Usage: ```python target = jnp.array([[0., 1.], [1., 1.]]) preds = jnp.array([[1., 0.], [1., 1.]]) # Using 'auto'/'sum_over_batch_size' reduction type. cosine_loss = jm.losses.CosineSimilarity(axis=1) assert cosine_loss(target, preds) == -0.49999997 # Calling with 'sample_weight'. assert cosine_loss(target, preds, sample_weight=jnp.array([0.8, 0.2])) == -0.099999994 # Using 'sum' reduction type. cosine_loss = jm.losses.CosineSimilarity(axis=1, reduction=jm.losses.Reduction.SUM ) assert cosine_loss(target, preds) == -0.99999994 # Using 'none' reduction type. cosine_loss = jm.losses.CosineSimilarity(axis=1, reduction=jm.losses.Reduction.NONE ) assert jnp.equal(cosine_loss(target, preds), jnp.array([-0., -0.99999994])).all() ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.CosineSimilarity(axis=1), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , axis : int = - 1 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `Mean` class. Arguments: axis: (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . axis = axis return super () . __init__ ( reduction = reduction , weight = weight ) def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `CosineSimilarity` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return cosine_similarity ( target , preds , self . axis )","title":"CosineSimilarity"},{"location":"api/losses/cosine_similarity/#jax_metrics.losses.cosine_similarity.CosineSimilarity.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default axis int (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. -1 reduction tp . Optional [ Reduction ] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight tp . Optional [ float ] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/cosine_similarity.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def __init__ ( self , axis : int = - 1 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `Mean` class. Arguments: axis: (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . axis = axis return super () . __init__ ( reduction = reduction , weight = weight )","title":"__init__()"},{"location":"api/losses/cosine_similarity/#jax_metrics.losses.cosine_similarity.CosineSimilarity.call","text":"Invokes the CosineSimilarity instance. Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] required sample_weight tp . Optional [ jax . Array ] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description jax . Array Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/cosine_similarity.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `CosineSimilarity` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return cosine_similarity ( target , preds , self . axis )","title":"call()"},{"location":"api/losses/cosine_similarity/#jax_metrics.losses.cosine_similarity.cosine_similarity","text":"Computes the cosine similarity between target and predictions. loss = - sum ( l2_norm ( target ) * l2_norm ( preds )) Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = jm . losses . cosine_similarity ( target , preds , axis = 1 ) assert loss . shape == ( 2 ,) target = target / jnp . maximum ( jnp . linalg . norm ( target , axis = 1 , keepdims = True ), jnp . sqrt ( types . EPSILON )) preds = preds / jnp . maximum ( jnp . linalg . norm ( preds , axis = 1 , keepdims = True ), jnp . sqrt ( types . EPSILON )) assert jnp . array_equal ( loss , - jnp . sum ( target * preds , axis = 1 )) Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] . required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] . required axis int The dimension along which the cosine similarity is computed. required Returns: Type Description jax . Array cosine similarity Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Source code in jax_metrics/losses/cosine_similarity.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def cosine_similarity ( target : jax . Array , preds : jax . Array , axis : int ) -> jax . Array : \"\"\" Computes the cosine similarity between target and predictions. ```python loss = -sum(l2_norm(target) * l2_norm(preds)) ``` Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = jm.losses.cosine_similarity(target, preds, axis=1) assert loss.shape == (2,) target = target / jnp.maximum(jnp.linalg.norm(target, axis=1, keepdims=True), jnp.sqrt(types.EPSILON)) preds = preds / jnp.maximum(jnp.linalg.norm(preds, axis=1, keepdims=True), jnp.sqrt(types.EPSILON)) assert jnp.array_equal(loss, -jnp.sum(target * preds, axis=1)) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. axis: The dimension along which the cosine similarity is computed. Returns: cosine similarity Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) \"\"\" target = target / jnp . maximum ( jnp . linalg . norm ( target , axis = axis , keepdims = True ), jnp . sqrt ( types . EPSILON ) ) preds = preds / jnp . maximum ( jnp . linalg . norm ( preds , axis = axis , keepdims = True ), jnp . sqrt ( types . EPSILON ) ) return - jnp . sum ( target * preds , axis = axis )","title":"cosine_similarity()"},{"location":"api/losses/crossentropy/","text":"jax_metrics.losses.crossentropy Crossentropy Bases: Loss Computes the crossentropy loss between the target and predictions. Use this crossentropy loss function when there are two or more label classes. We expect target to be provided as integers. If you want to provide target using one-hot representation, please use CategoricalCrossentropy loss. There should be # classes floating point values per feature for preds and a single floating point value per feature for target . In the snippet below, there is a single floating point value per example for target and # classes floating pointing values per example for preds . The shape of target is [batch_size] and the shape of preds is [batch_size, num_classes] . Usage: target = jnp . array ([ 1 , 2 ]) preds = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = jm . losses . Crossentropy () result = scce ( target , preds ) # 1.177 assert np . isclose ( result , 1.177 , rtol = 0.01 ) # Calling with 'sample_weight'. result = scce ( target , preds , sample_weight = jnp . array ([ 0.3 , 0.7 ])) # 0.814 assert np . isclose ( result , 0.814 , rtol = 0.01 ) # Using 'sum' reduction type. scce = jm . losses . Crossentropy ( reduction = jm . losses . Reduction . SUM ) result = scce ( target , preds ) # 2.354 assert np . isclose ( result , 2.354 , rtol = 0.01 ) # Using 'none' reduction type. scce = jm . losses . Crossentropy ( reduction = jm . losses . Reduction . NONE ) result = scce ( target , preds ) # [0.0513, 2.303] assert jnp . all ( np . isclose ( result , [ 0.0513 , 2.303 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . Crossentropy (), metrics = elegy . metrics . Accuracy (), optimizer = optax . adam ( 1e-3 ), ) Source code in jax_metrics/losses/crossentropy.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 class Crossentropy ( Loss ): \"\"\" Computes the crossentropy loss between the target and predictions. Use this crossentropy loss function when there are two or more label classes. We expect target to be provided as integers. If you want to provide target using `one-hot` representation, please use `CategoricalCrossentropy` loss. There should be `# classes` floating point values per feature for `preds` and a single floating point value per feature for `target`. In the snippet below, there is a single floating point value per example for `target` and `# classes` floating pointing values per example for `preds`. The shape of `target` is `[batch_size]` and the shape of `preds` is `[batch_size, num_classes]`. Usage: ```python target = jnp.array([1, 2]) preds = jnp.array([[0.05, 0.95, 0], [0.1, 0.8, 0.1]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = jm.losses.Crossentropy() result = scce(target, preds) # 1.177 assert np.isclose(result, 1.177, rtol=0.01) # Calling with 'sample_weight'. result = scce(target, preds, sample_weight=jnp.array([0.3, 0.7])) # 0.814 assert np.isclose(result, 0.814, rtol=0.01) # Using 'sum' reduction type. scce = jm.losses.Crossentropy( reduction=jm.losses.Reduction.SUM ) result = scce(target, preds) # 2.354 assert np.isclose(result, 2.354, rtol=0.01) # Using 'none' reduction type. scce = jm.losses.Crossentropy( reduction=jm.losses.Reduction.NONE ) result = scce(target, preds) # [0.0513, 2.303] assert jnp.all(np.isclose(result, [0.0513, 2.303], rtol=0.01)) ``` Usage with the `Elegy` API: ```python model = elegy.Model( module_fn, loss=jm.losses.Crossentropy(), metrics=elegy.metrics.Accuracy(), optimizer=optax.adam(1e-3), ) ``` \"\"\" def __init__ ( self , * , from_logits : bool = True , binary : bool = False , label_smoothing : tp . Optional [ float ] = None , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `preds` is expected to be a logits tensor. By default, we assume that `preds` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" super () . __init__ ( reduction = reduction , weight = weight ) self . _from_logits = from_logits self . _binary = binary self . _label_smoothing = label_smoothing def call ( self , target , preds , sample_weight : tp . Optional [ jax . Array ] = None , ** _ , ) -> jax . Array : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: target: Ground truth values. preds: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return crossentropy ( target , preds , binary = self . _binary , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , ) __init__ ( * , from_logits = True , binary = False , label_smoothing = None , reduction = None , weight = None ) Initializes SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether preds is expected to be a logits tensor. By default, we assume that preds encodes a probability distribution. Note - Using from_logits=True is more numerically stable. True reduction tp . Optional [ Reduction ] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight tp . Optional [ float ] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/crossentropy.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def __init__ ( self , * , from_logits : bool = True , binary : bool = False , label_smoothing : tp . Optional [ float ] = None , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `preds` is expected to be a logits tensor. By default, we assume that `preds` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" super () . __init__ ( reduction = reduction , weight = weight ) self . _from_logits = from_logits self . _binary = binary self . _label_smoothing = label_smoothing call ( target , preds , sample_weight = None , ** _ ) Invokes the SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default target Ground truth values. required preds The predicted values. required sample_weight tp . Optional [ jax . Array ] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description jax . Array Loss values per sample. Source code in jax_metrics/losses/crossentropy.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 def call ( self , target , preds , sample_weight : tp . Optional [ jax . Array ] = None , ** _ , ) -> jax . Array : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: target: Ground truth values. preds: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return crossentropy ( target , preds , binary = self . _binary , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , )","title":"crossentropy"},{"location":"api/losses/crossentropy/#jax_metricslossescrossentropy","text":"","title":"jax_metrics.losses.crossentropy"},{"location":"api/losses/crossentropy/#jax_metrics.losses.crossentropy.Crossentropy","text":"Bases: Loss Computes the crossentropy loss between the target and predictions. Use this crossentropy loss function when there are two or more label classes. We expect target to be provided as integers. If you want to provide target using one-hot representation, please use CategoricalCrossentropy loss. There should be # classes floating point values per feature for preds and a single floating point value per feature for target . In the snippet below, there is a single floating point value per example for target and # classes floating pointing values per example for preds . The shape of target is [batch_size] and the shape of preds is [batch_size, num_classes] . Usage: target = jnp . array ([ 1 , 2 ]) preds = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = jm . losses . Crossentropy () result = scce ( target , preds ) # 1.177 assert np . isclose ( result , 1.177 , rtol = 0.01 ) # Calling with 'sample_weight'. result = scce ( target , preds , sample_weight = jnp . array ([ 0.3 , 0.7 ])) # 0.814 assert np . isclose ( result , 0.814 , rtol = 0.01 ) # Using 'sum' reduction type. scce = jm . losses . Crossentropy ( reduction = jm . losses . Reduction . SUM ) result = scce ( target , preds ) # 2.354 assert np . isclose ( result , 2.354 , rtol = 0.01 ) # Using 'none' reduction type. scce = jm . losses . Crossentropy ( reduction = jm . losses . Reduction . NONE ) result = scce ( target , preds ) # [0.0513, 2.303] assert jnp . all ( np . isclose ( result , [ 0.0513 , 2.303 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . Crossentropy (), metrics = elegy . metrics . Accuracy (), optimizer = optax . adam ( 1e-3 ), ) Source code in jax_metrics/losses/crossentropy.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 class Crossentropy ( Loss ): \"\"\" Computes the crossentropy loss between the target and predictions. Use this crossentropy loss function when there are two or more label classes. We expect target to be provided as integers. If you want to provide target using `one-hot` representation, please use `CategoricalCrossentropy` loss. There should be `# classes` floating point values per feature for `preds` and a single floating point value per feature for `target`. In the snippet below, there is a single floating point value per example for `target` and `# classes` floating pointing values per example for `preds`. The shape of `target` is `[batch_size]` and the shape of `preds` is `[batch_size, num_classes]`. Usage: ```python target = jnp.array([1, 2]) preds = jnp.array([[0.05, 0.95, 0], [0.1, 0.8, 0.1]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = jm.losses.Crossentropy() result = scce(target, preds) # 1.177 assert np.isclose(result, 1.177, rtol=0.01) # Calling with 'sample_weight'. result = scce(target, preds, sample_weight=jnp.array([0.3, 0.7])) # 0.814 assert np.isclose(result, 0.814, rtol=0.01) # Using 'sum' reduction type. scce = jm.losses.Crossentropy( reduction=jm.losses.Reduction.SUM ) result = scce(target, preds) # 2.354 assert np.isclose(result, 2.354, rtol=0.01) # Using 'none' reduction type. scce = jm.losses.Crossentropy( reduction=jm.losses.Reduction.NONE ) result = scce(target, preds) # [0.0513, 2.303] assert jnp.all(np.isclose(result, [0.0513, 2.303], rtol=0.01)) ``` Usage with the `Elegy` API: ```python model = elegy.Model( module_fn, loss=jm.losses.Crossentropy(), metrics=elegy.metrics.Accuracy(), optimizer=optax.adam(1e-3), ) ``` \"\"\" def __init__ ( self , * , from_logits : bool = True , binary : bool = False , label_smoothing : tp . Optional [ float ] = None , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `preds` is expected to be a logits tensor. By default, we assume that `preds` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" super () . __init__ ( reduction = reduction , weight = weight ) self . _from_logits = from_logits self . _binary = binary self . _label_smoothing = label_smoothing def call ( self , target , preds , sample_weight : tp . Optional [ jax . Array ] = None , ** _ , ) -> jax . Array : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: target: Ground truth values. preds: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return crossentropy ( target , preds , binary = self . _binary , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , )","title":"Crossentropy"},{"location":"api/losses/crossentropy/#jax_metrics.losses.crossentropy.Crossentropy.__init__","text":"Initializes SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether preds is expected to be a logits tensor. By default, we assume that preds encodes a probability distribution. Note - Using from_logits=True is more numerically stable. True reduction tp . Optional [ Reduction ] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight tp . Optional [ float ] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/crossentropy.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def __init__ ( self , * , from_logits : bool = True , binary : bool = False , label_smoothing : tp . Optional [ float ] = None , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `preds` is expected to be a logits tensor. By default, we assume that `preds` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" super () . __init__ ( reduction = reduction , weight = weight ) self . _from_logits = from_logits self . _binary = binary self . _label_smoothing = label_smoothing","title":"__init__()"},{"location":"api/losses/crossentropy/#jax_metrics.losses.crossentropy.Crossentropy.call","text":"Invokes the SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default target Ground truth values. required preds The predicted values. required sample_weight tp . Optional [ jax . Array ] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description jax . Array Loss values per sample. Source code in jax_metrics/losses/crossentropy.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 def call ( self , target , preds , sample_weight : tp . Optional [ jax . Array ] = None , ** _ , ) -> jax . Array : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: target: Ground truth values. preds: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return crossentropy ( target , preds , binary = self . _binary , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , )","title":"call()"},{"location":"api/losses/huber/","text":"jax_metrics.losses.huber Huber Bases: Loss Computes the Huber loss between target and predictions. For each value x in error = target - preds: loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: target = jnp . array ([[ 0 , 1 ], [ 0 , 0 ]]) preds = jnp . array ([[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. huber_loss = jm . losses . Huber () assert huber_loss ( target , preds ) == 0.155 # Calling with 'sample_weight'. assert ( huber_loss ( target , preds , sample_weight = jnp . array ([ 0.8 , 0.2 ])) == 0.08500001 ) # Using 'sum' reduction type. huber_loss = jm . losses . Huber ( reduction = jm . losses . Reduction . SUM ) assert huber_loss ( target , preds ) == 0.31 # Using 'none' reduction type. huber_loss = jm . losses . Huber ( reduction = jm . losses . Reduction . NONE ) assert jnp . equal ( huber_loss ( target , preds ), jnp . array ([ 0.18 , 0.13000001 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . Huber ( delta = 1.0 ), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/huber.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 class Huber ( Loss ): r \"\"\" Computes the Huber loss between target and predictions. For each value x in error = target - preds: $$ loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} $$ where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: ```python target = jnp.array([[0, 1], [0, 0]]) preds = jnp.array([[0.6, 0.4], [0.4, 0.6]]) # Using 'auto'/'sum_over_batch_size' reduction type. huber_loss = jm.losses.Huber() assert huber_loss(target, preds) == 0.155 # Calling with 'sample_weight'. assert ( huber_loss(target, preds, sample_weight=jnp.array([0.8, 0.2])) == 0.08500001 ) # Using 'sum' reduction type. huber_loss = jm.losses.Huber( reduction=jm.losses.Reduction.SUM ) assert huber_loss(target, preds) == 0.31 # Using 'none' reduction type. huber_loss = jm.losses.Huber( reduction=jm.losses.Reduction.NONE ) assert jnp.equal(huber_loss(target, preds), jnp.array([0.18, 0.13000001])).all() ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.Huber(delta=1.0), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , delta : float = 1.0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `Mean` class. Arguments: delta: (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . delta = delta return super () . __init__ ( reduction = reduction , weight = weight ) def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `Huber` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return huber ( target , preds , self . delta ) __init__ ( delta = 1.0 , reduction = None , weight = None ) Initializes Mean class. Parameters: Name Type Description Default delta float (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. 1.0 reduction tp . Optional [ Reduction ] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight tp . Optional [ float ] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/huber.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 def __init__ ( self , delta : float = 1.0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `Mean` class. Arguments: delta: (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . delta = delta return super () . __init__ ( reduction = reduction , weight = weight ) call ( target , preds , sample_weight = None , ** _ ) Invokes the Huber instance. Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] required sample_weight tp . Optional [ jax . Array ] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description jax . Array Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/huber.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `Huber` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return huber ( target , preds , self . delta ) huber ( target , preds , delta ) Computes the Huber loss between target and predictions. For each value x in error = target - preds: loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = jm . losses . huber ( target , preds , delta = 1.0 ) assert loss . shape == ( 2 ,) preds = preds . astype ( float ) target = target . astype ( float ) delta = 1.0 error = jnp . subtract ( preds , target ) abs_error = jnp . abs ( error ) quadratic = jnp . minimum ( abs_error , delta ) linear = jnp . subtract ( abs_error , quadratic ) assert jnp . array_equal ( loss , jnp . mean ( jnp . add ( jnp . multiply ( 0.5 , jnp . multiply ( quadratic , quadratic ) ), jnp . multiply ( delta , linear )), axis =- 1 )) Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] . required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] . required delta float A float, the point where the Huber loss function changes from a quadratic to linear. required Returns: Type Description jax . Array huber loss Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Source code in jax_metrics/losses/huber.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def huber ( target : jax . Array , preds : jax . Array , delta : float ) -> jax . Array : r \"\"\" Computes the Huber loss between target and predictions. For each value x in error = target - preds: $$ loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} $$ where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = jm.losses.huber(target, preds, delta=1.0) assert loss.shape == (2,) preds = preds.astype(float) target = target.astype(float) delta = 1.0 error = jnp.subtract(preds, target) abs_error = jnp.abs(error) quadratic = jnp.minimum(abs_error, delta) linear = jnp.subtract(abs_error, quadratic) assert jnp.array_equal(loss, jnp.mean( jnp.add( jnp.multiply( 0.5, jnp.multiply(quadratic, quadratic) ), jnp.multiply(delta, linear)), axis=-1 )) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. delta: A float, the point where the Huber loss function changes from a quadratic to linear. Returns: huber loss Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) \"\"\" preds = preds . astype ( float ) target = target . astype ( float ) delta = float ( delta ) error = jnp . subtract ( preds , target ) abs_error = jnp . abs ( error ) quadratic = jnp . minimum ( abs_error , delta ) linear = jnp . subtract ( abs_error , quadratic ) return jnp . mean ( jnp . add ( jnp . multiply ( 0.5 , jnp . multiply ( quadratic , quadratic )), jnp . multiply ( delta , linear ), ), axis =- 1 , )","title":"huber"},{"location":"api/losses/huber/#jax_metricslosseshuber","text":"","title":"jax_metrics.losses.huber"},{"location":"api/losses/huber/#jax_metrics.losses.huber.Huber","text":"Bases: Loss Computes the Huber loss between target and predictions. For each value x in error = target - preds: loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: target = jnp . array ([[ 0 , 1 ], [ 0 , 0 ]]) preds = jnp . array ([[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. huber_loss = jm . losses . Huber () assert huber_loss ( target , preds ) == 0.155 # Calling with 'sample_weight'. assert ( huber_loss ( target , preds , sample_weight = jnp . array ([ 0.8 , 0.2 ])) == 0.08500001 ) # Using 'sum' reduction type. huber_loss = jm . losses . Huber ( reduction = jm . losses . Reduction . SUM ) assert huber_loss ( target , preds ) == 0.31 # Using 'none' reduction type. huber_loss = jm . losses . Huber ( reduction = jm . losses . Reduction . NONE ) assert jnp . equal ( huber_loss ( target , preds ), jnp . array ([ 0.18 , 0.13000001 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . Huber ( delta = 1.0 ), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/huber.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 class Huber ( Loss ): r \"\"\" Computes the Huber loss between target and predictions. For each value x in error = target - preds: $$ loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} $$ where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: ```python target = jnp.array([[0, 1], [0, 0]]) preds = jnp.array([[0.6, 0.4], [0.4, 0.6]]) # Using 'auto'/'sum_over_batch_size' reduction type. huber_loss = jm.losses.Huber() assert huber_loss(target, preds) == 0.155 # Calling with 'sample_weight'. assert ( huber_loss(target, preds, sample_weight=jnp.array([0.8, 0.2])) == 0.08500001 ) # Using 'sum' reduction type. huber_loss = jm.losses.Huber( reduction=jm.losses.Reduction.SUM ) assert huber_loss(target, preds) == 0.31 # Using 'none' reduction type. huber_loss = jm.losses.Huber( reduction=jm.losses.Reduction.NONE ) assert jnp.equal(huber_loss(target, preds), jnp.array([0.18, 0.13000001])).all() ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.Huber(delta=1.0), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , delta : float = 1.0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `Mean` class. Arguments: delta: (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . delta = delta return super () . __init__ ( reduction = reduction , weight = weight ) def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `Huber` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return huber ( target , preds , self . delta )","title":"Huber"},{"location":"api/losses/huber/#jax_metrics.losses.huber.Huber.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default delta float (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. 1.0 reduction tp . Optional [ Reduction ] (Optional) Type of jm.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight tp . Optional [ float ] Optional weight contribution for the total loss. Defaults to 1 . None Source code in jax_metrics/losses/huber.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 def __init__ ( self , delta : float = 1.0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `Mean` class. Arguments: delta: (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. reduction: (Optional) Type of `jm.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" self . delta = delta return super () . __init__ ( reduction = reduction , weight = weight )","title":"__init__()"},{"location":"api/losses/huber/#jax_metrics.losses.huber.Huber.call","text":"Invokes the Huber instance. Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] required sample_weight tp . Optional [ jax . Array ] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description jax . Array Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/huber.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `Huber` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return huber ( target , preds , self . delta )","title":"call()"},{"location":"api/losses/huber/#jax_metrics.losses.huber.huber","text":"Computes the Huber loss between target and predictions. For each value x in error = target - preds: loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = jm . losses . huber ( target , preds , delta = 1.0 ) assert loss . shape == ( 2 ,) preds = preds . astype ( float ) target = target . astype ( float ) delta = 1.0 error = jnp . subtract ( preds , target ) abs_error = jnp . abs ( error ) quadratic = jnp . minimum ( abs_error , delta ) linear = jnp . subtract ( abs_error , quadratic ) assert jnp . array_equal ( loss , jnp . mean ( jnp . add ( jnp . multiply ( 0.5 , jnp . multiply ( quadratic , quadratic ) ), jnp . multiply ( delta , linear )), axis =- 1 )) Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] . required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] . required delta float A float, the point where the Huber loss function changes from a quadratic to linear. required Returns: Type Description jax . Array huber loss Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Source code in jax_metrics/losses/huber.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def huber ( target : jax . Array , preds : jax . Array , delta : float ) -> jax . Array : r \"\"\" Computes the Huber loss between target and predictions. For each value x in error = target - preds: $$ loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} $$ where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = jm.losses.huber(target, preds, delta=1.0) assert loss.shape == (2,) preds = preds.astype(float) target = target.astype(float) delta = 1.0 error = jnp.subtract(preds, target) abs_error = jnp.abs(error) quadratic = jnp.minimum(abs_error, delta) linear = jnp.subtract(abs_error, quadratic) assert jnp.array_equal(loss, jnp.mean( jnp.add( jnp.multiply( 0.5, jnp.multiply(quadratic, quadratic) ), jnp.multiply(delta, linear)), axis=-1 )) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. delta: A float, the point where the Huber loss function changes from a quadratic to linear. Returns: huber loss Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) \"\"\" preds = preds . astype ( float ) target = target . astype ( float ) delta = float ( delta ) error = jnp . subtract ( preds , target ) abs_error = jnp . abs ( error ) quadratic = jnp . minimum ( abs_error , delta ) linear = jnp . subtract ( abs_error , quadratic ) return jnp . mean ( jnp . add ( jnp . multiply ( 0.5 , jnp . multiply ( quadratic , quadratic )), jnp . multiply ( delta , linear ), ), axis =- 1 , )","title":"huber()"},{"location":"api/losses/mean_absolute_error/","text":"jax_metrics.losses.mean_absolute_error MeanAbsoluteError Bases: Loss Computes the mean absolute errors between target and predictions. loss = mean(abs(target - preds)) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mae = jm . losses . MeanAbsoluteError () assert mae ( target , preds ) == 0.5 # Calling with 'sample_weight'. assert mae ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mae = jm . losses . MeanAbsoluteError ( reduction = jm . losses . Reduction . SUM ) assert mae ( target , preds ) == 1.0 # Using 'none' reduction type. mae = jm . losses . MeanAbsoluteError ( reduction = jm . losses . Reduction . NONE ) assert list ( mae ( target , preds )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanAbsoluteError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_absolute_error.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 class MeanAbsoluteError ( Loss ): \"\"\" Computes the mean absolute errors between target and predictions. `loss = mean(abs(target - preds))` Usage: ```python target = jnp.array([[0.0, 1.0], [0.0, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. mae = jm.losses.MeanAbsoluteError() assert mae(target, preds) == 0.5 # Calling with 'sample_weight'. assert mae(target, preds, sample_weight=jnp.array([0.7, 0.3])) == 0.25 # Using 'sum' reduction type. mae = jm.losses.MeanAbsoluteError(reduction=jm.losses.Reduction.SUM) assert mae(target, preds) == 1.0 # Using 'none' reduction type. mae = jm.losses.MeanAbsoluteError(reduction=jm.losses.Reduction.NONE) assert list(mae(target, preds)) == [0.5, 0.5] ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanAbsoluteError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanAbsoluteError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_error ( target , preds ) call ( target , preds , sample_weight = None , ** _ ) Invokes the MeanAbsoluteError instance. Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] required sample_weight tp . Optional [ jax . Array ] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description jax . Array Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_absolute_error.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanAbsoluteError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_error ( target , preds ) mean_absolute_error ( target , preds ) Computes the mean absolute error between target and predictions. After computing the absolute distance between the inputs, the mean value over the last dimension is returned. loss = mean ( abs ( target - preds ), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = jm . losses . mean_absolute_error ( target , preds ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , jnp . mean ( jnp . abs ( target - preds ), axis =- 1 )) Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] . required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description jax . Array Mean absolute error values. shape = [batch_size, d0, .. dN-1] . Source code in jax_metrics/losses/mean_absolute_error.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def mean_absolute_error ( target : jax . Array , preds : jax . Array ) -> jax . Array : \"\"\" Computes the mean absolute error between target and predictions. After computing the absolute distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(abs(target - preds), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = jm.losses.mean_absolute_error(target, preds) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.abs(target - preds), axis=-1)) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean absolute error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" target = target . astype ( preds . dtype ) return jnp . mean ( jnp . abs ( preds - target ), axis =- 1 )","title":"mean_absolute_error"},{"location":"api/losses/mean_absolute_error/#jax_metricslossesmean_absolute_error","text":"","title":"jax_metrics.losses.mean_absolute_error"},{"location":"api/losses/mean_absolute_error/#jax_metrics.losses.mean_absolute_error.MeanAbsoluteError","text":"Bases: Loss Computes the mean absolute errors between target and predictions. loss = mean(abs(target - preds)) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mae = jm . losses . MeanAbsoluteError () assert mae ( target , preds ) == 0.5 # Calling with 'sample_weight'. assert mae ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mae = jm . losses . MeanAbsoluteError ( reduction = jm . losses . Reduction . SUM ) assert mae ( target , preds ) == 1.0 # Using 'none' reduction type. mae = jm . losses . MeanAbsoluteError ( reduction = jm . losses . Reduction . NONE ) assert list ( mae ( target , preds )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanAbsoluteError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_absolute_error.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 class MeanAbsoluteError ( Loss ): \"\"\" Computes the mean absolute errors between target and predictions. `loss = mean(abs(target - preds))` Usage: ```python target = jnp.array([[0.0, 1.0], [0.0, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. mae = jm.losses.MeanAbsoluteError() assert mae(target, preds) == 0.5 # Calling with 'sample_weight'. assert mae(target, preds, sample_weight=jnp.array([0.7, 0.3])) == 0.25 # Using 'sum' reduction type. mae = jm.losses.MeanAbsoluteError(reduction=jm.losses.Reduction.SUM) assert mae(target, preds) == 1.0 # Using 'none' reduction type. mae = jm.losses.MeanAbsoluteError(reduction=jm.losses.Reduction.NONE) assert list(mae(target, preds)) == [0.5, 0.5] ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanAbsoluteError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanAbsoluteError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_error ( target , preds )","title":"MeanAbsoluteError"},{"location":"api/losses/mean_absolute_error/#jax_metrics.losses.mean_absolute_error.MeanAbsoluteError.call","text":"Invokes the MeanAbsoluteError instance. Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] required sample_weight tp . Optional [ jax . Array ] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description jax . Array Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_absolute_error.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanAbsoluteError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_error ( target , preds )","title":"call()"},{"location":"api/losses/mean_absolute_error/#jax_metrics.losses.mean_absolute_error.mean_absolute_error","text":"Computes the mean absolute error between target and predictions. After computing the absolute distance between the inputs, the mean value over the last dimension is returned. loss = mean ( abs ( target - preds ), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = jm . losses . mean_absolute_error ( target , preds ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , jnp . mean ( jnp . abs ( target - preds ), axis =- 1 )) Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] . required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description jax . Array Mean absolute error values. shape = [batch_size, d0, .. dN-1] . Source code in jax_metrics/losses/mean_absolute_error.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def mean_absolute_error ( target : jax . Array , preds : jax . Array ) -> jax . Array : \"\"\" Computes the mean absolute error between target and predictions. After computing the absolute distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(abs(target - preds), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = jm.losses.mean_absolute_error(target, preds) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.abs(target - preds), axis=-1)) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean absolute error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" target = target . astype ( preds . dtype ) return jnp . mean ( jnp . abs ( preds - target ), axis =- 1 )","title":"mean_absolute_error()"},{"location":"api/losses/mean_absolute_percentage_error/","text":"jax_metrics.losses.mean_absolute_percentage_error MeanAbsolutePercentageError Bases: Loss Computes the mean absolute errors between target and predictions. loss = mean(abs((target - preds) / target)) Usage: target = jnp . array ([[ 1.0 , 1.0 ], [ 0.9 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mape = jm . losses . MeanAbsolutePercentageError () result = mape ( target , preds ) assert np . isclose ( result , 2.78 , rtol = 0.01 ) # Calling with 'sample_weight'. assert np . isclose ( mape ( target , preds , sample_weight = jnp . array ([ 0.1 , 0.9 ])), 2.5 , rtol = 0.01 ) # Using 'sum' reduction type. mape = jm . losses . MeanAbsolutePercentageError ( reduction = jm . losses . Reduction . SUM ) assert np . isclose ( mape ( target , preds ), 5.6 , rtol = 0.01 ) # Using 'none' reduction type. mape = jm . losses . MeanAbsolutePercentageError ( reduction = jm . losses . Reduction . NONE ) assert jnp . all ( np . isclose ( result , [ 0. , 5.6 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanAbsolutePercentageError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_absolute_percentage_error.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 class MeanAbsolutePercentageError ( Loss ): \"\"\" Computes the mean absolute errors between target and predictions. `loss = mean(abs((target - preds) / target))` Usage: ```python target = jnp.array([[1.0, 1.0], [0.9, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. mape = jm.losses.MeanAbsolutePercentageError() result = mape(target, preds) assert np.isclose(result, 2.78, rtol=0.01) # Calling with 'sample_weight'. assert np.isclose(mape(target, preds, sample_weight=jnp.array([0.1, 0.9])), 2.5, rtol=0.01) # Using 'sum' reduction type. mape = jm.losses.MeanAbsolutePercentageError(reduction=jm.losses.Reduction.SUM) assert np.isclose(mape(target, preds), 5.6, rtol=0.01) # Using 'none' reduction type. mape = jm.losses.MeanAbsolutePercentageError(reduction=jm.losses.Reduction.NONE) assert jnp.all(np.isclose(result, [0. , 5.6], rtol=0.01)) ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanAbsolutePercentageError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanAbsolutePercentageError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_percentage_error ( target , preds ) call ( target , preds , sample_weight = None , ** _ ) Invokes the MeanAbsolutePercentageError instance. Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] required sample_weight tp . Optional [ jax . Array ] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description jax . Array Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_absolute_percentage_error.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanAbsolutePercentageError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_percentage_error ( target , preds ) mean_absolute_percentage_error ( target , preds ) Computes the mean absolute percentage error (MAPE) between target and predictions. After computing the absolute distance between the true value and the prediction value and divide by the true value, the mean value over the last dimension is returned. Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = jm . losses . mean_absolute_percentage_error ( target , preds ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , 100. * jnp . mean ( jnp . abs (( preds - target ) / jnp . clip ( target , types . EPSILON , None )))) Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] . required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description jax . Array Mean absolute percentage error values. shape = [batch_size, d0, .. dN-1] . Source code in jax_metrics/losses/mean_absolute_percentage_error.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def mean_absolute_percentage_error ( target : jax . Array , preds : jax . Array ) -> jax . Array : \"\"\" Computes the mean absolute percentage error (MAPE) between target and predictions. After computing the absolute distance between the true value and the prediction value and divide by the true value, the mean value over the last dimension is returned. Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = jm.losses.mean_absolute_percentage_error(target, preds) assert loss.shape == (2,) assert jnp.array_equal(loss, 100. * jnp.mean(jnp.abs((preds - target) / jnp.clip(target, types.EPSILON, None)))) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean absolute percentage error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" target = target . astype ( preds . dtype ) diff = jnp . abs (( preds - target ) / jnp . maximum ( jnp . abs ( target ), types . EPSILON )) return 100.0 * jnp . mean ( diff , axis =- 1 )","title":"mean_absolute_percentage_error"},{"location":"api/losses/mean_absolute_percentage_error/#jax_metricslossesmean_absolute_percentage_error","text":"","title":"jax_metrics.losses.mean_absolute_percentage_error"},{"location":"api/losses/mean_absolute_percentage_error/#jax_metrics.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError","text":"Bases: Loss Computes the mean absolute errors between target and predictions. loss = mean(abs((target - preds) / target)) Usage: target = jnp . array ([[ 1.0 , 1.0 ], [ 0.9 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mape = jm . losses . MeanAbsolutePercentageError () result = mape ( target , preds ) assert np . isclose ( result , 2.78 , rtol = 0.01 ) # Calling with 'sample_weight'. assert np . isclose ( mape ( target , preds , sample_weight = jnp . array ([ 0.1 , 0.9 ])), 2.5 , rtol = 0.01 ) # Using 'sum' reduction type. mape = jm . losses . MeanAbsolutePercentageError ( reduction = jm . losses . Reduction . SUM ) assert np . isclose ( mape ( target , preds ), 5.6 , rtol = 0.01 ) # Using 'none' reduction type. mape = jm . losses . MeanAbsolutePercentageError ( reduction = jm . losses . Reduction . NONE ) assert jnp . all ( np . isclose ( result , [ 0. , 5.6 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanAbsolutePercentageError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_absolute_percentage_error.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 class MeanAbsolutePercentageError ( Loss ): \"\"\" Computes the mean absolute errors between target and predictions. `loss = mean(abs((target - preds) / target))` Usage: ```python target = jnp.array([[1.0, 1.0], [0.9, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. mape = jm.losses.MeanAbsolutePercentageError() result = mape(target, preds) assert np.isclose(result, 2.78, rtol=0.01) # Calling with 'sample_weight'. assert np.isclose(mape(target, preds, sample_weight=jnp.array([0.1, 0.9])), 2.5, rtol=0.01) # Using 'sum' reduction type. mape = jm.losses.MeanAbsolutePercentageError(reduction=jm.losses.Reduction.SUM) assert np.isclose(mape(target, preds), 5.6, rtol=0.01) # Using 'none' reduction type. mape = jm.losses.MeanAbsolutePercentageError(reduction=jm.losses.Reduction.NONE) assert jnp.all(np.isclose(result, [0. , 5.6], rtol=0.01)) ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanAbsolutePercentageError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanAbsolutePercentageError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_percentage_error ( target , preds )","title":"MeanAbsolutePercentageError"},{"location":"api/losses/mean_absolute_percentage_error/#jax_metrics.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError.call","text":"Invokes the MeanAbsolutePercentageError instance. Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] required sample_weight tp . Optional [ jax . Array ] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description jax . Array Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_absolute_percentage_error.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanAbsolutePercentageError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_percentage_error ( target , preds )","title":"call()"},{"location":"api/losses/mean_absolute_percentage_error/#jax_metrics.losses.mean_absolute_percentage_error.mean_absolute_percentage_error","text":"Computes the mean absolute percentage error (MAPE) between target and predictions. After computing the absolute distance between the true value and the prediction value and divide by the true value, the mean value over the last dimension is returned. Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = jm . losses . mean_absolute_percentage_error ( target , preds ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , 100. * jnp . mean ( jnp . abs (( preds - target ) / jnp . clip ( target , types . EPSILON , None )))) Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] . required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description jax . Array Mean absolute percentage error values. shape = [batch_size, d0, .. dN-1] . Source code in jax_metrics/losses/mean_absolute_percentage_error.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def mean_absolute_percentage_error ( target : jax . Array , preds : jax . Array ) -> jax . Array : \"\"\" Computes the mean absolute percentage error (MAPE) between target and predictions. After computing the absolute distance between the true value and the prediction value and divide by the true value, the mean value over the last dimension is returned. Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = jm.losses.mean_absolute_percentage_error(target, preds) assert loss.shape == (2,) assert jnp.array_equal(loss, 100. * jnp.mean(jnp.abs((preds - target) / jnp.clip(target, types.EPSILON, None)))) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean absolute percentage error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" target = target . astype ( preds . dtype ) diff = jnp . abs (( preds - target ) / jnp . maximum ( jnp . abs ( target ), types . EPSILON )) return 100.0 * jnp . mean ( diff , axis =- 1 )","title":"mean_absolute_percentage_error()"},{"location":"api/losses/mean_squared_error/","text":"jax_metrics.losses.mean_squared_error MeanSquaredError Bases: Loss Computes the mean of squares of errors between target and predictions. loss = square(target - preds) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = jm . losses . MeanSquaredError () assert mse ( target , preds ) == 0.5 # Calling with 'sample_weight'. assert mse ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mse = jm . losses . MeanSquaredError ( reduction = jm . losses . Reduction . SUM ) assert mse ( target , preds ) == 1.0 # Using 'none' reduction type. mse = jm . losses . MeanSquaredError ( reduction = jm . losses . Reduction . NONE ) assert list ( mse ( target , preds )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanSquaredError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_squared_error.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 class MeanSquaredError ( Loss ): \"\"\" Computes the mean of squares of errors between target and predictions. `loss = square(target - preds)` Usage: ```python target = jnp.array([[0.0, 1.0], [0.0, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = jm.losses.MeanSquaredError() assert mse(target, preds) == 0.5 # Calling with 'sample_weight'. assert mse(target, preds, sample_weight=jnp.array([0.7, 0.3])) == 0.25 # Using 'sum' reduction type. mse = jm.losses.MeanSquaredError(reduction=jm.losses.Reduction.SUM) assert mse(target, preds) == 1.0 # Using 'none' reduction type. mse = jm.losses.MeanSquaredError(reduction=jm.losses.Reduction.NONE) assert list(mse(target, preds)) == [0.5, 0.5] ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanSquaredError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( target , preds ) call ( target , preds , sample_weight = None , ** _ ) Invokes the MeanSquaredError instance. Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] required sample_weight tp . Optional [ jax . Array ] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description jax . Array Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_squared_error.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( target , preds ) mean_squared_error ( target , preds ) Computes the mean squared error between target and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean ( square ( target - preds ), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = jm . losses . mean_squared_error ( target , preds ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , jnp . mean ( jnp . square ( target - preds ), axis =- 1 )) Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] . required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description jax . Array Mean squared error values. shape = [batch_size, d0, .. dN-1] . Source code in jax_metrics/losses/mean_squared_error.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def mean_squared_error ( target : jax . Array , preds : jax . Array ) -> jax . Array : \"\"\" Computes the mean squared error between target and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(square(target - preds), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = jm.losses.mean_squared_error(target, preds) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.square(target - preds), axis=-1)) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" target = target . astype ( preds . dtype ) return jnp . mean ( jnp . square ( preds - target ), axis =- 1 )","title":"mean_squared_error"},{"location":"api/losses/mean_squared_error/#jax_metricslossesmean_squared_error","text":"","title":"jax_metrics.losses.mean_squared_error"},{"location":"api/losses/mean_squared_error/#jax_metrics.losses.mean_squared_error.MeanSquaredError","text":"Bases: Loss Computes the mean of squares of errors between target and predictions. loss = square(target - preds) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = jm . losses . MeanSquaredError () assert mse ( target , preds ) == 0.5 # Calling with 'sample_weight'. assert mse ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mse = jm . losses . MeanSquaredError ( reduction = jm . losses . Reduction . SUM ) assert mse ( target , preds ) == 1.0 # Using 'none' reduction type. mse = jm . losses . MeanSquaredError ( reduction = jm . losses . Reduction . NONE ) assert list ( mse ( target , preds )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanSquaredError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_squared_error.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 class MeanSquaredError ( Loss ): \"\"\" Computes the mean of squares of errors between target and predictions. `loss = square(target - preds)` Usage: ```python target = jnp.array([[0.0, 1.0], [0.0, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = jm.losses.MeanSquaredError() assert mse(target, preds) == 0.5 # Calling with 'sample_weight'. assert mse(target, preds, sample_weight=jnp.array([0.7, 0.3])) == 0.25 # Using 'sum' reduction type. mse = jm.losses.MeanSquaredError(reduction=jm.losses.Reduction.SUM) assert mse(target, preds) == 1.0 # Using 'none' reduction type. mse = jm.losses.MeanSquaredError(reduction=jm.losses.Reduction.NONE) assert list(mse(target, preds)) == [0.5, 0.5] ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanSquaredError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( target , preds )","title":"MeanSquaredError"},{"location":"api/losses/mean_squared_error/#jax_metrics.losses.mean_squared_error.MeanSquaredError.call","text":"Invokes the MeanSquaredError instance. Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] required sample_weight tp . Optional [ jax . Array ] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description jax . Array Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_squared_error.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( target , preds )","title":"call()"},{"location":"api/losses/mean_squared_error/#jax_metrics.losses.mean_squared_error.mean_squared_error","text":"Computes the mean squared error between target and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean ( square ( target - preds ), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = jm . losses . mean_squared_error ( target , preds ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , jnp . mean ( jnp . square ( target - preds ), axis =- 1 )) Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] . required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description jax . Array Mean squared error values. shape = [batch_size, d0, .. dN-1] . Source code in jax_metrics/losses/mean_squared_error.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def mean_squared_error ( target : jax . Array , preds : jax . Array ) -> jax . Array : \"\"\" Computes the mean squared error between target and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(square(target - preds), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = jm.losses.mean_squared_error(target, preds) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.square(target - preds), axis=-1)) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" target = target . astype ( preds . dtype ) return jnp . mean ( jnp . square ( preds - target ), axis =- 1 )","title":"mean_squared_error()"},{"location":"api/losses/mean_squared_logarithmic_error/","text":"jax_metrics.losses.mean_squared_logarithmic_error MeanSquaredLogarithmicError Bases: Loss Computes the mean squared logarithmic errors between target and predictions. loss = mean(square(log(target + 1) - log(preds + 1)), axis=-1) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. msle = jm . losses . MeanSquaredLogarithmicError () assert msle ( target , preds ) == 0.24022643 # Calling with 'sample_weight'. assert msle ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) = 0.12011322 # Using 'sum' reduction type. msle = jm . losses . MeanSquaredLogarithmicError ( reduction = jm . losses . Reduction . SUM ) assert msle ( target , preds ) == 0.48045287 # Using 'none' reduction type. msle = jm . losses . MeanSquaredLogarithmicError ( reduction = jm . losses . Reduction . NONE ) assert jnp . equal ( msle ( target , preds ), jnp . array ([ 0.24022643 , 0.24022643 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanSquaredLogarithmicError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_squared_logarithmic_error.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 class MeanSquaredLogarithmicError ( Loss ): \"\"\" Computes the mean squared logarithmic errors between target and predictions. `loss = mean(square(log(target + 1) - log(preds + 1)), axis=-1)` Usage: ```python target = jnp.array([[0.0, 1.0], [0.0, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. msle = jm.losses.MeanSquaredLogarithmicError() assert msle(target, preds) == 0.24022643 # Calling with 'sample_weight'. assert msle(target, preds, sample_weight=jnp.array([0.7, 0.3])) = 0.12011322 # Using 'sum' reduction type. msle = jm.losses.MeanSquaredLogarithmicError(reduction=jm.losses.Reduction.SUM) assert msle(target, preds) == 0.48045287 # Using 'none' reduction type. msle = jm.losses.MeanSquaredLogarithmicError(reduction=jm.losses.Reduction.NONE) assert jnp.equal(msle(target, preds), jnp.array([0.24022643, 0.24022643])).all() ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanSquaredLogarithmicError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanSquaredLogarithmicError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_logarithmic_error ( target , preds ) call ( target , preds , sample_weight = None , ** _ ) Invokes the MeanSquaredLogarithmicError instance. Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] required sample_weight tp . Optional [ jax . Array ] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description jax . Array Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_squared_logarithmic_error.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanSquaredLogarithmicError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_logarithmic_error ( target , preds ) mean_squared_logarithmic_error ( target , preds ) Computes the mean squared logarithmic error between target and predictions. loss = mean ( square ( log ( target + 1 ) - log ( preds + 1 )), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = jm . losses . mean_squared_logarithmic_error ( target , preds ) assert loss . shape == ( 2 ,) first_log = jnp . log ( jnp . maximum ( target , types . EPSILON ) + 1.0 ) second_log = jnp . log ( jnp . maximum ( preds , types . EPSILON ) + 1.0 ) assert jnp . array_equal ( loss , jnp . mean ( jnp . square ( first_log - second_log ), axis =- 1 )) Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] . required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description jax . Array Mean squared logarithmic error values. shape = [batch_size, d0, .. dN-1] . Source code in jax_metrics/losses/mean_squared_logarithmic_error.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def mean_squared_logarithmic_error ( target : jax . Array , preds : jax . Array ) -> jax . Array : \"\"\" Computes the mean squared logarithmic error between target and predictions. ```python loss = mean(square(log(target + 1) - log(preds + 1)), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = jm.losses.mean_squared_logarithmic_error(target, preds) assert loss.shape == (2,) first_log = jnp.log(jnp.maximum(target, types.EPSILON) + 1.0) second_log = jnp.log(jnp.maximum(preds, types.EPSILON) + 1.0) assert jnp.array_equal(loss, jnp.mean(jnp.square(first_log - second_log), axis=-1)) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared logarithmic error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" target = target . astype ( preds . dtype ) first_log = jnp . log ( jnp . maximum ( target , types . EPSILON ) + 1.0 ) second_log = jnp . log ( jnp . maximum ( preds , types . EPSILON ) + 1.0 ) return jnp . mean ( jnp . square ( first_log - second_log ), axis =- 1 )","title":"mean_squared_logarithmic_error"},{"location":"api/losses/mean_squared_logarithmic_error/#jax_metricslossesmean_squared_logarithmic_error","text":"","title":"jax_metrics.losses.mean_squared_logarithmic_error"},{"location":"api/losses/mean_squared_logarithmic_error/#jax_metrics.losses.mean_squared_logarithmic_error.MeanSquaredLogarithmicError","text":"Bases: Loss Computes the mean squared logarithmic errors between target and predictions. loss = mean(square(log(target + 1) - log(preds + 1)), axis=-1) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. msle = jm . losses . MeanSquaredLogarithmicError () assert msle ( target , preds ) == 0.24022643 # Calling with 'sample_weight'. assert msle ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) = 0.12011322 # Using 'sum' reduction type. msle = jm . losses . MeanSquaredLogarithmicError ( reduction = jm . losses . Reduction . SUM ) assert msle ( target , preds ) == 0.48045287 # Using 'none' reduction type. msle = jm . losses . MeanSquaredLogarithmicError ( reduction = jm . losses . Reduction . NONE ) assert jnp . equal ( msle ( target , preds ), jnp . array ([ 0.24022643 , 0.24022643 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanSquaredLogarithmicError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/losses/mean_squared_logarithmic_error.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 class MeanSquaredLogarithmicError ( Loss ): \"\"\" Computes the mean squared logarithmic errors between target and predictions. `loss = mean(square(log(target + 1) - log(preds + 1)), axis=-1)` Usage: ```python target = jnp.array([[0.0, 1.0], [0.0, 0.0]]) preds = jnp.array([[1.0, 1.0], [1.0, 0.0]]) # Using 'auto'/'sum_over_batch_size' reduction type. msle = jm.losses.MeanSquaredLogarithmicError() assert msle(target, preds) == 0.24022643 # Calling with 'sample_weight'. assert msle(target, preds, sample_weight=jnp.array([0.7, 0.3])) = 0.12011322 # Using 'sum' reduction type. msle = jm.losses.MeanSquaredLogarithmicError(reduction=jm.losses.Reduction.SUM) assert msle(target, preds) == 0.48045287 # Using 'none' reduction type. msle = jm.losses.MeanSquaredLogarithmicError(reduction=jm.losses.Reduction.NONE) assert jnp.equal(msle(target, preds), jnp.array([0.24022643, 0.24022643])).all() ``` Usage with the Elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanSquaredLogarithmicError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanSquaredLogarithmicError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_logarithmic_error ( target , preds )","title":"MeanSquaredLogarithmicError"},{"location":"api/losses/mean_squared_logarithmic_error/#jax_metrics.losses.mean_squared_logarithmic_error.MeanSquaredLogarithmicError.call","text":"Invokes the MeanSquaredLogarithmicError instance. Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] required sample_weight tp . Optional [ jax . Array ] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description jax . Array Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: Type Description ValueError If the shape of sample_weight is invalid. Source code in jax_metrics/losses/mean_squared_logarithmic_error.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def call ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , # not used, __call__ handles it, left for documentation purposes. ** _ , ) -> jax . Array : \"\"\" Invokes the `MeanSquaredLogarithmicError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_logarithmic_error ( target , preds )","title":"call()"},{"location":"api/losses/mean_squared_logarithmic_error/#jax_metrics.losses.mean_squared_logarithmic_error.mean_squared_logarithmic_error","text":"Computes the mean squared logarithmic error between target and predictions. loss = mean ( square ( log ( target + 1 ) - log ( preds + 1 )), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = jm . losses . mean_squared_logarithmic_error ( target , preds ) assert loss . shape == ( 2 ,) first_log = jnp . log ( jnp . maximum ( target , types . EPSILON ) + 1.0 ) second_log = jnp . log ( jnp . maximum ( preds , types . EPSILON ) + 1.0 ) assert jnp . array_equal ( loss , jnp . mean ( jnp . square ( first_log - second_log ), axis =- 1 )) Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] . required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description jax . Array Mean squared logarithmic error values. shape = [batch_size, d0, .. dN-1] . Source code in jax_metrics/losses/mean_squared_logarithmic_error.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def mean_squared_logarithmic_error ( target : jax . Array , preds : jax . Array ) -> jax . Array : \"\"\" Computes the mean squared logarithmic error between target and predictions. ```python loss = mean(square(log(target + 1) - log(preds + 1)), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = jm.losses.mean_squared_logarithmic_error(target, preds) assert loss.shape == (2,) first_log = jnp.log(jnp.maximum(target, types.EPSILON) + 1.0) second_log = jnp.log(jnp.maximum(preds, types.EPSILON) + 1.0) assert jnp.array_equal(loss, jnp.mean(jnp.square(first_log - second_log), axis=-1)) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared logarithmic error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" target = target . astype ( preds . dtype ) first_log = jnp . log ( jnp . maximum ( target , types . EPSILON ) + 1.0 ) second_log = jnp . log ( jnp . maximum ( preds , types . EPSILON ) + 1.0 ) return jnp . mean ( jnp . square ( first_log - second_log ), axis =- 1 )","title":"mean_squared_logarithmic_error()"},{"location":"api/metrics/Accuracy/","text":"jax_metrics.metrics.Accuracy Bases: SumMetric Computes Accuracy, ported from torchmetrics . .. math:: \\text{Accuracy} = \\frac{1}{N}\\sum_i^N 1(y_i = \\hat{y}_i) Where :math: y is a tensor of target values, and :math: \\hat{y} is a tensor of predictions. For multi-class and multi-dimensional multi-class data with probability or logits predictions, the parameter top_k generalizes this metric to a Top-K accuracy metric: for each sample the top-K highest probability or logit score items are considered to find the correct label. For multi-label and multi-dimensional multi-class inputs, this metric computes the \"glob\" accuracy by default, which counts all target or sub-samples separately. This can be changed to subset accuracy (which requires all target or sub-samples in the sample to be correctly predicted) by setting subset_accuracy=True . Parameters: Name Type Description Default num_classes Optional [ int ] Number of classes. Necessary for 'macro' , 'weighted' and None average methods. None threshold float Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities. 0.5 average Union [ str , AverageMethod ] Defines the reduction that is applied. Should be one of the following: 'micro' [default]: Calculate the metric globally, across all samples and classes. 'macro' : Calculate the metric for each class separately, and average the metrics across classes (with equal weights for each class). 'weighted' : Calculate the metric for each class separately, and average the metrics across classes, weighting each class by its support ( tp + fn ). 'none' or None : Calculate the metric for each class separately, and return the metric for every class. 'samples' : Calculate the metric for each sample, and average the metrics across samples (with equal weights for each sample). .. note:: What is considered a sample in the multi-dimensional multi-class case depends on the value of mdmc_average . .. note:: If 'none' and a given class doesn't occur in the preds or target , the value for the class will be nan . AverageMethod.MICRO mdmc_average Union [ str , MDMCAverageMethod ] Defines how averaging is done for multi-dimensional multi-class inputs (on top of the average parameter). Should be one of the following: None [default]: Should be left unchanged if your data is not multi-dimensional multi-class. 'samplewise' : In this case, the statistics are computed separately for each sample on the N axis, and then averaged over samples. The computation for each sample is done by treating the flattened extra axes ... (see :ref: references/modules:input types ) as the N dimension within the sample, and computing the metric for the sample based on that. 'global' : In this case the N and ... dimensions of the inputs (see :ref: references/modules:input types ) are flattened into a new N_X sample axis, i.e. the inputs are treated as if they were (N_X, C) . From here on the average parameter applies as usual. MDMCAverageMethod.GLOBAL ignore_index Optional [ int ] Integer specifying a target class to ignore. If given, this class index does not contribute to the returned score, regardless of reduction method. If an index is ignored, and average=None or 'none' , the score for the ignored class will be returned as nan . None top_k Optional [ int ] Number of highest probability or logit score predictions considered to find the correct label, relevant only for (multi-dimensional) multi-class inputs. The default value ( None ) will be interpreted as 1 for these inputs. Should be left at default ( None ) for all other types of inputs. None multiclass Optional [ bool ] Used only in certain special cases, where you want to treat inputs as a different type than what they appear to be. See the parameter's :ref: documentation section <references/modules:using the multiclass parameter> for a more detailed explanation and examples. None subset_accuracy bool Whether to compute subset accuracy for multi-label and multi-dimensional multi-class inputs (has no effect for other input types). For multi-label inputs, if the parameter is set to True , then all target for each sample must be correctly predicted for the sample to count as correct. If it is set to False , then all target are counted separately - this is equivalent to flattening inputs beforehand (i.e. preds = preds.flatten() and same for target ). For multi-dimensional multi-class inputs, if the parameter is set to True , then all sub-sample (on the extra axis) must be correct for the sample to be counted as correct. If it is set to False , then all sub-samples are counter separately - this is equivalent, in the case of label predictions, to flattening the inputs beforehand (i.e. preds = preds.flatten() and same for target ). Note that the top_k parameter still applies in both cases, if set. False Source code in jax_metrics/metrics/accuracy.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 class Accuracy ( SumMetric ): r \"\"\" Computes Accuracy, ported from [torchmetrics](https://github.com/PytorchLightning/metrics). .. math:: \\text{Accuracy} = \\frac{1}{N}\\sum_i^N 1(y_i = \\hat{y}_i) Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. For multi-class and multi-dimensional multi-class data with probability or logits predictions, the parameter `top_k` generalizes this metric to a Top-K accuracy metric: for each sample the top-K highest probability or logit score items are considered to find the correct label. For multi-label and multi-dimensional multi-class inputs, this metric computes the \"glob\" accuracy by default, which counts all target or sub-samples separately. This can be changed to subset accuracy (which requires all target or sub-samples in the sample to be correctly predicted) by setting `subset_accuracy=True`. Arguments: num_classes: Number of classes. Necessary for `'macro'`, `'weighted'` and `None` average methods. threshold: Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities. average: Defines the reduction that is applied. Should be one of the following: - `'micro'` [default]: Calculate the metric globally, across all samples and classes. - `'macro'`: Calculate the metric for each class separately, and average the metrics across classes (with equal weights for each class). - `'weighted'`: Calculate the metric for each class separately, and average the metrics across classes, weighting each class by its support (`tp + fn`). - `'none'` or `None`: Calculate the metric for each class separately, and return the metric for every class. - `'samples'`: Calculate the metric for each sample, and average the metrics across samples (with equal weights for each sample). .. note:: What is considered a sample in the multi-dimensional multi-class case depends on the value of `mdmc_average`. .. note:: If `'none'` and a given class doesn't occur in the `preds` or `target`, the value for the class will be `nan`. mdmc_average: Defines how averaging is done for multi-dimensional multi-class inputs (on top of the `average` parameter). Should be one of the following: - `None` [default]: Should be left unchanged if your data is not multi-dimensional multi-class. - `'samplewise'`: In this case, the statistics are computed separately for each sample on the `N` axis, and then averaged over samples. The computation for each sample is done by treating the flattened extra axes `...` (see :ref:`references/modules:input types`) as the `N` dimension within the sample, and computing the metric for the sample based on that. - `'global'`: In this case the `N` and `...` dimensions of the inputs (see :ref:`references/modules:input types`) are flattened into a new `N_X` sample axis, i.e. the inputs are treated as if they were `(N_X, C)`. From here on the `average` parameter applies as usual. ignore_index: Integer specifying a target class to ignore. If given, this class index does not contribute to the returned score, regardless of reduction method. If an index is ignored, and `average=None` or `'none'`, the score for the ignored class will be returned as `nan`. top_k: Number of highest probability or logit score predictions considered to find the correct label, relevant only for (multi-dimensional) multi-class inputs. The default value (`None`) will be interpreted as 1 for these inputs. Should be left at default (`None`) for all other types of inputs. multiclass: Used only in certain special cases, where you want to treat inputs as a different type than what they appear to be. See the parameter's :ref:`documentation section <references/modules:using the multiclass parameter>` for a more detailed explanation and examples. subset_accuracy: Whether to compute subset accuracy for multi-label and multi-dimensional multi-class inputs (has no effect for other input types). - For multi-label inputs, if the parameter is set to `True`, then all target for each sample must be correctly predicted for the sample to count as correct. If it is set to `False`, then all target are counted separately - this is equivalent to flattening inputs beforehand (i.e. `preds = preds.flatten()` and same for `target`). - For multi-dimensional multi-class inputs, if the parameter is set to `True`, then all sub-sample (on the extra axis) must be correct for the sample to be counted as correct. If it is set to `False`, then all sub-samples are counter separately - this is equivalent, in the case of label predictions, to flattening the inputs beforehand (i.e. `preds = preds.flatten()` and same for `target`). Note that the `top_k` parameter still applies in both cases, if set. Raises: ValueError: If `top_k` is not an `integer` larger than `0`. ValueError: If `average` is none of `\"micro\"`, `\"macro\"`, `\"weighted\"`, `\"samples\"`, `\"none\"`, `None`. ValueError: If two different input modes are provided, eg. using `multi-label` with `multi-class`. ValueError: If `top_k` parameter is set for `multi-label` inputs. \"\"\" tp : jax . Array fp : jax . Array tn : jax . Array fn : jax . Array average : AverageMethod = static_field () mdmc_average : MDMCAverageMethod = static_field () num_classes : Optional [ int ] = static_field () threshold : float = static_field () multiclass : Optional [ bool ] = static_field () ignore_index : Optional [ int ] = static_field () top_k : Optional [ int ] = static_field () subset_accuracy : bool = static_field () mode : DataType = static_field () def __init__ ( self , threshold : float = 0.5 , num_classes : Optional [ int ] = None , average : Union [ str , AverageMethod ] = AverageMethod . MICRO , mdmc_average : Union [ str , MDMCAverageMethod ] = MDMCAverageMethod . GLOBAL , ignore_index : Optional [ int ] = None , top_k : Optional [ int ] = None , multiclass : Optional [ bool ] = None , subset_accuracy : bool = False , # compute_on_step: bool = True, # dist_sync_on_step: bool = False, # process_group: Optional[Any] = None, # dist_sync_fn: Callable = None, mode : DataType = DataType . MULTICLASS , ): if isinstance ( average , str ): average = AverageMethod [ average . upper ()] if isinstance ( mdmc_average , str ): mdmc_average = MDMCAverageMethod [ mdmc_average . upper ()] average = ( AverageMethod . MACRO if average in [ AverageMethod . WEIGHTED , AverageMethod . NONE ] else average ) if average not in [ AverageMethod . MICRO , AverageMethod . MACRO , # AverageMethod.SAMPLES, ]: raise ValueError ( f \"The `reduce` { average } is not valid.\" ) if average == AverageMethod . MACRO and ( not num_classes or num_classes < 1 ): raise ValueError ( \"When you set `reduce` as 'macro', you have to provide the number of\" \"classes.\" ) if top_k is not None and top_k <= 0 : raise ValueError ( f \"The `top_k` should be an integer larger than 0, got { top_k } \" ) if ( num_classes and ignore_index is not None and ( not 0 <= ignore_index < num_classes or num_classes == 1 ) ): raise ValueError ( f \"The `ignore_index` { ignore_index } is not valid for inputs \" \"with {num_classes} classes\" ) # Update states if average == AverageMethod . SAMPLES : raise ValueError ( f \"The `average` method ' { average } ' is not yet supported.\" ) if mdmc_average == MDMCAverageMethod . SAMPLEWISE : raise ValueError ( f \"The `mdmc_average` method ' { mdmc_average } ' is not yet supported.\" ) self . average = average self . mdmc_average = mdmc_average self . num_classes = num_classes self . threshold = threshold self . multiclass = multiclass self . ignore_index = ignore_index self . top_k = top_k self . subset_accuracy = subset_accuracy self . mode = mode self . __dict__ . update ( self . _initial_values ()) def _initial_values ( self ) -> Dict [ str , jax . Array ]: # nodes zeros_shape : List [ int ] if self . average == AverageMethod . MICRO : zeros_shape = [] elif self . average == AverageMethod . MACRO : if self . num_classes is None : raise ValueError ( \"The `num_classes` parameter must be defined to use `average='macro'`.\" ) zeros_shape = [ self . num_classes ] else : raise ValueError ( f 'Wrong reduce=\" { self . average } \"' ) initial_value = jnp . zeros ( zeros_shape , dtype = jnp . uint32 ) return dict ( tp = initial_value , fp = initial_value , tn = initial_value , fn = initial_value , ) def reset ( self ): return self . replace ( ** self . _initial_values ()) def update ( self , * , preds : jax . Array , target : jax . Array , ** _ ) -> \"Accuracy\" : \"\"\"Updates Accuracy metric state. Example: ``python accuracy = Accuracy().reset() accuracy = accuracy.update(preds=preds, target=target) Args: preds: Predictions from model (logits, probabilities, or target) target: Ground truth target Returns: Updated Accuracy instance \"\"\" tp , fp , tn , fn = metric_utils . stat_scores_update ( preds , target , intended_mode = self . mode , average_method = self . average , mdmc_average_method = self . mdmc_average , threshold = self . threshold , num_classes = self . num_classes , top_k = self . top_k , multiclass = self . multiclass , ) return self . replace ( tp = self . tp + tp , fp = self . fp + fp , tn = self . tn + tn , fn = self . fn + fn , ) def compute ( self ) -> jax . Array : \"\"\" Computes accuracy based on inputs passed in to `update` previously. Returns: Accuracy score \"\"\" # if self.mode is None: # raise RuntimeError(\"You have to have determined mode.\") return metric_utils . accuracy_compute ( self . tp , self . fp , self . tn , self . fn , self . average , self . mdmc_average , self . mode , ) compute () Computes accuracy based on inputs passed in to update previously. Returns: Type Description jax . Array Accuracy score Source code in jax_metrics/metrics/accuracy.py 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 def compute ( self ) -> jax . Array : \"\"\" Computes accuracy based on inputs passed in to `update` previously. Returns: Accuracy score \"\"\" # if self.mode is None: # raise RuntimeError(\"You have to have determined mode.\") return metric_utils . accuracy_compute ( self . tp , self . fp , self . tn , self . fn , self . average , self . mdmc_average , self . mode , ) update ( * , preds , target , ** _ ) Updates Accuracy metric state. Example: ``python accuracy = Accuracy().reset() accuracy = accuracy.update(preds=preds, target=target) Parameters: Name Type Description Default preds jax . Array Predictions from model (logits, probabilities, or target) required target jax . Array Ground truth target required Returns: Type Description Accuracy Updated Accuracy instance Source code in jax_metrics/metrics/accuracy.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 def update ( self , * , preds : jax . Array , target : jax . Array , ** _ ) -> \"Accuracy\" : \"\"\"Updates Accuracy metric state. Example: ``python accuracy = Accuracy().reset() accuracy = accuracy.update(preds=preds, target=target) Args: preds: Predictions from model (logits, probabilities, or target) target: Ground truth target Returns: Updated Accuracy instance \"\"\" tp , fp , tn , fn = metric_utils . stat_scores_update ( preds , target , intended_mode = self . mode , average_method = self . average , mdmc_average_method = self . mdmc_average , threshold = self . threshold , num_classes = self . num_classes , top_k = self . top_k , multiclass = self . multiclass , ) return self . replace ( tp = self . tp + tp , fp = self . fp + fp , tn = self . tn + tn , fn = self . fn + fn , )","title":"Accuracy"},{"location":"api/metrics/Accuracy/#jax_metricsmetricsaccuracy","text":"Bases: SumMetric Computes Accuracy, ported from torchmetrics . .. math:: \\text{Accuracy} = \\frac{1}{N}\\sum_i^N 1(y_i = \\hat{y}_i) Where :math: y is a tensor of target values, and :math: \\hat{y} is a tensor of predictions. For multi-class and multi-dimensional multi-class data with probability or logits predictions, the parameter top_k generalizes this metric to a Top-K accuracy metric: for each sample the top-K highest probability or logit score items are considered to find the correct label. For multi-label and multi-dimensional multi-class inputs, this metric computes the \"glob\" accuracy by default, which counts all target or sub-samples separately. This can be changed to subset accuracy (which requires all target or sub-samples in the sample to be correctly predicted) by setting subset_accuracy=True . Parameters: Name Type Description Default num_classes Optional [ int ] Number of classes. Necessary for 'macro' , 'weighted' and None average methods. None threshold float Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities. 0.5 average Union [ str , AverageMethod ] Defines the reduction that is applied. Should be one of the following: 'micro' [default]: Calculate the metric globally, across all samples and classes. 'macro' : Calculate the metric for each class separately, and average the metrics across classes (with equal weights for each class). 'weighted' : Calculate the metric for each class separately, and average the metrics across classes, weighting each class by its support ( tp + fn ). 'none' or None : Calculate the metric for each class separately, and return the metric for every class. 'samples' : Calculate the metric for each sample, and average the metrics across samples (with equal weights for each sample). .. note:: What is considered a sample in the multi-dimensional multi-class case depends on the value of mdmc_average . .. note:: If 'none' and a given class doesn't occur in the preds or target , the value for the class will be nan . AverageMethod.MICRO mdmc_average Union [ str , MDMCAverageMethod ] Defines how averaging is done for multi-dimensional multi-class inputs (on top of the average parameter). Should be one of the following: None [default]: Should be left unchanged if your data is not multi-dimensional multi-class. 'samplewise' : In this case, the statistics are computed separately for each sample on the N axis, and then averaged over samples. The computation for each sample is done by treating the flattened extra axes ... (see :ref: references/modules:input types ) as the N dimension within the sample, and computing the metric for the sample based on that. 'global' : In this case the N and ... dimensions of the inputs (see :ref: references/modules:input types ) are flattened into a new N_X sample axis, i.e. the inputs are treated as if they were (N_X, C) . From here on the average parameter applies as usual. MDMCAverageMethod.GLOBAL ignore_index Optional [ int ] Integer specifying a target class to ignore. If given, this class index does not contribute to the returned score, regardless of reduction method. If an index is ignored, and average=None or 'none' , the score for the ignored class will be returned as nan . None top_k Optional [ int ] Number of highest probability or logit score predictions considered to find the correct label, relevant only for (multi-dimensional) multi-class inputs. The default value ( None ) will be interpreted as 1 for these inputs. Should be left at default ( None ) for all other types of inputs. None multiclass Optional [ bool ] Used only in certain special cases, where you want to treat inputs as a different type than what they appear to be. See the parameter's :ref: documentation section <references/modules:using the multiclass parameter> for a more detailed explanation and examples. None subset_accuracy bool Whether to compute subset accuracy for multi-label and multi-dimensional multi-class inputs (has no effect for other input types). For multi-label inputs, if the parameter is set to True , then all target for each sample must be correctly predicted for the sample to count as correct. If it is set to False , then all target are counted separately - this is equivalent to flattening inputs beforehand (i.e. preds = preds.flatten() and same for target ). For multi-dimensional multi-class inputs, if the parameter is set to True , then all sub-sample (on the extra axis) must be correct for the sample to be counted as correct. If it is set to False , then all sub-samples are counter separately - this is equivalent, in the case of label predictions, to flattening the inputs beforehand (i.e. preds = preds.flatten() and same for target ). Note that the top_k parameter still applies in both cases, if set. False Source code in jax_metrics/metrics/accuracy.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 class Accuracy ( SumMetric ): r \"\"\" Computes Accuracy, ported from [torchmetrics](https://github.com/PytorchLightning/metrics). .. math:: \\text{Accuracy} = \\frac{1}{N}\\sum_i^N 1(y_i = \\hat{y}_i) Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. For multi-class and multi-dimensional multi-class data with probability or logits predictions, the parameter `top_k` generalizes this metric to a Top-K accuracy metric: for each sample the top-K highest probability or logit score items are considered to find the correct label. For multi-label and multi-dimensional multi-class inputs, this metric computes the \"glob\" accuracy by default, which counts all target or sub-samples separately. This can be changed to subset accuracy (which requires all target or sub-samples in the sample to be correctly predicted) by setting `subset_accuracy=True`. Arguments: num_classes: Number of classes. Necessary for `'macro'`, `'weighted'` and `None` average methods. threshold: Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities. average: Defines the reduction that is applied. Should be one of the following: - `'micro'` [default]: Calculate the metric globally, across all samples and classes. - `'macro'`: Calculate the metric for each class separately, and average the metrics across classes (with equal weights for each class). - `'weighted'`: Calculate the metric for each class separately, and average the metrics across classes, weighting each class by its support (`tp + fn`). - `'none'` or `None`: Calculate the metric for each class separately, and return the metric for every class. - `'samples'`: Calculate the metric for each sample, and average the metrics across samples (with equal weights for each sample). .. note:: What is considered a sample in the multi-dimensional multi-class case depends on the value of `mdmc_average`. .. note:: If `'none'` and a given class doesn't occur in the `preds` or `target`, the value for the class will be `nan`. mdmc_average: Defines how averaging is done for multi-dimensional multi-class inputs (on top of the `average` parameter). Should be one of the following: - `None` [default]: Should be left unchanged if your data is not multi-dimensional multi-class. - `'samplewise'`: In this case, the statistics are computed separately for each sample on the `N` axis, and then averaged over samples. The computation for each sample is done by treating the flattened extra axes `...` (see :ref:`references/modules:input types`) as the `N` dimension within the sample, and computing the metric for the sample based on that. - `'global'`: In this case the `N` and `...` dimensions of the inputs (see :ref:`references/modules:input types`) are flattened into a new `N_X` sample axis, i.e. the inputs are treated as if they were `(N_X, C)`. From here on the `average` parameter applies as usual. ignore_index: Integer specifying a target class to ignore. If given, this class index does not contribute to the returned score, regardless of reduction method. If an index is ignored, and `average=None` or `'none'`, the score for the ignored class will be returned as `nan`. top_k: Number of highest probability or logit score predictions considered to find the correct label, relevant only for (multi-dimensional) multi-class inputs. The default value (`None`) will be interpreted as 1 for these inputs. Should be left at default (`None`) for all other types of inputs. multiclass: Used only in certain special cases, where you want to treat inputs as a different type than what they appear to be. See the parameter's :ref:`documentation section <references/modules:using the multiclass parameter>` for a more detailed explanation and examples. subset_accuracy: Whether to compute subset accuracy for multi-label and multi-dimensional multi-class inputs (has no effect for other input types). - For multi-label inputs, if the parameter is set to `True`, then all target for each sample must be correctly predicted for the sample to count as correct. If it is set to `False`, then all target are counted separately - this is equivalent to flattening inputs beforehand (i.e. `preds = preds.flatten()` and same for `target`). - For multi-dimensional multi-class inputs, if the parameter is set to `True`, then all sub-sample (on the extra axis) must be correct for the sample to be counted as correct. If it is set to `False`, then all sub-samples are counter separately - this is equivalent, in the case of label predictions, to flattening the inputs beforehand (i.e. `preds = preds.flatten()` and same for `target`). Note that the `top_k` parameter still applies in both cases, if set. Raises: ValueError: If `top_k` is not an `integer` larger than `0`. ValueError: If `average` is none of `\"micro\"`, `\"macro\"`, `\"weighted\"`, `\"samples\"`, `\"none\"`, `None`. ValueError: If two different input modes are provided, eg. using `multi-label` with `multi-class`. ValueError: If `top_k` parameter is set for `multi-label` inputs. \"\"\" tp : jax . Array fp : jax . Array tn : jax . Array fn : jax . Array average : AverageMethod = static_field () mdmc_average : MDMCAverageMethod = static_field () num_classes : Optional [ int ] = static_field () threshold : float = static_field () multiclass : Optional [ bool ] = static_field () ignore_index : Optional [ int ] = static_field () top_k : Optional [ int ] = static_field () subset_accuracy : bool = static_field () mode : DataType = static_field () def __init__ ( self , threshold : float = 0.5 , num_classes : Optional [ int ] = None , average : Union [ str , AverageMethod ] = AverageMethod . MICRO , mdmc_average : Union [ str , MDMCAverageMethod ] = MDMCAverageMethod . GLOBAL , ignore_index : Optional [ int ] = None , top_k : Optional [ int ] = None , multiclass : Optional [ bool ] = None , subset_accuracy : bool = False , # compute_on_step: bool = True, # dist_sync_on_step: bool = False, # process_group: Optional[Any] = None, # dist_sync_fn: Callable = None, mode : DataType = DataType . MULTICLASS , ): if isinstance ( average , str ): average = AverageMethod [ average . upper ()] if isinstance ( mdmc_average , str ): mdmc_average = MDMCAverageMethod [ mdmc_average . upper ()] average = ( AverageMethod . MACRO if average in [ AverageMethod . WEIGHTED , AverageMethod . NONE ] else average ) if average not in [ AverageMethod . MICRO , AverageMethod . MACRO , # AverageMethod.SAMPLES, ]: raise ValueError ( f \"The `reduce` { average } is not valid.\" ) if average == AverageMethod . MACRO and ( not num_classes or num_classes < 1 ): raise ValueError ( \"When you set `reduce` as 'macro', you have to provide the number of\" \"classes.\" ) if top_k is not None and top_k <= 0 : raise ValueError ( f \"The `top_k` should be an integer larger than 0, got { top_k } \" ) if ( num_classes and ignore_index is not None and ( not 0 <= ignore_index < num_classes or num_classes == 1 ) ): raise ValueError ( f \"The `ignore_index` { ignore_index } is not valid for inputs \" \"with {num_classes} classes\" ) # Update states if average == AverageMethod . SAMPLES : raise ValueError ( f \"The `average` method ' { average } ' is not yet supported.\" ) if mdmc_average == MDMCAverageMethod . SAMPLEWISE : raise ValueError ( f \"The `mdmc_average` method ' { mdmc_average } ' is not yet supported.\" ) self . average = average self . mdmc_average = mdmc_average self . num_classes = num_classes self . threshold = threshold self . multiclass = multiclass self . ignore_index = ignore_index self . top_k = top_k self . subset_accuracy = subset_accuracy self . mode = mode self . __dict__ . update ( self . _initial_values ()) def _initial_values ( self ) -> Dict [ str , jax . Array ]: # nodes zeros_shape : List [ int ] if self . average == AverageMethod . MICRO : zeros_shape = [] elif self . average == AverageMethod . MACRO : if self . num_classes is None : raise ValueError ( \"The `num_classes` parameter must be defined to use `average='macro'`.\" ) zeros_shape = [ self . num_classes ] else : raise ValueError ( f 'Wrong reduce=\" { self . average } \"' ) initial_value = jnp . zeros ( zeros_shape , dtype = jnp . uint32 ) return dict ( tp = initial_value , fp = initial_value , tn = initial_value , fn = initial_value , ) def reset ( self ): return self . replace ( ** self . _initial_values ()) def update ( self , * , preds : jax . Array , target : jax . Array , ** _ ) -> \"Accuracy\" : \"\"\"Updates Accuracy metric state. Example: ``python accuracy = Accuracy().reset() accuracy = accuracy.update(preds=preds, target=target) Args: preds: Predictions from model (logits, probabilities, or target) target: Ground truth target Returns: Updated Accuracy instance \"\"\" tp , fp , tn , fn = metric_utils . stat_scores_update ( preds , target , intended_mode = self . mode , average_method = self . average , mdmc_average_method = self . mdmc_average , threshold = self . threshold , num_classes = self . num_classes , top_k = self . top_k , multiclass = self . multiclass , ) return self . replace ( tp = self . tp + tp , fp = self . fp + fp , tn = self . tn + tn , fn = self . fn + fn , ) def compute ( self ) -> jax . Array : \"\"\" Computes accuracy based on inputs passed in to `update` previously. Returns: Accuracy score \"\"\" # if self.mode is None: # raise RuntimeError(\"You have to have determined mode.\") return metric_utils . accuracy_compute ( self . tp , self . fp , self . tn , self . fn , self . average , self . mdmc_average , self . mode , )","title":"jax_metrics.metrics.Accuracy"},{"location":"api/metrics/Accuracy/#jax_metrics.metrics.accuracy.Accuracy.compute","text":"Computes accuracy based on inputs passed in to update previously. Returns: Type Description jax . Array Accuracy score Source code in jax_metrics/metrics/accuracy.py 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 def compute ( self ) -> jax . Array : \"\"\" Computes accuracy based on inputs passed in to `update` previously. Returns: Accuracy score \"\"\" # if self.mode is None: # raise RuntimeError(\"You have to have determined mode.\") return metric_utils . accuracy_compute ( self . tp , self . fp , self . tn , self . fn , self . average , self . mdmc_average , self . mode , )","title":"compute()"},{"location":"api/metrics/Accuracy/#jax_metrics.metrics.accuracy.Accuracy.update","text":"Updates Accuracy metric state. Example: ``python accuracy = Accuracy().reset() accuracy = accuracy.update(preds=preds, target=target) Parameters: Name Type Description Default preds jax . Array Predictions from model (logits, probabilities, or target) required target jax . Array Ground truth target required Returns: Type Description Accuracy Updated Accuracy instance Source code in jax_metrics/metrics/accuracy.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 def update ( self , * , preds : jax . Array , target : jax . Array , ** _ ) -> \"Accuracy\" : \"\"\"Updates Accuracy metric state. Example: ``python accuracy = Accuracy().reset() accuracy = accuracy.update(preds=preds, target=target) Args: preds: Predictions from model (logits, probabilities, or target) target: Ground truth target Returns: Updated Accuracy instance \"\"\" tp , fp , tn , fn = metric_utils . stat_scores_update ( preds , target , intended_mode = self . mode , average_method = self . average , mdmc_average_method = self . mdmc_average , threshold = self . threshold , num_classes = self . num_classes , top_k = self . top_k , multiclass = self . multiclass , ) return self . replace ( tp = self . tp + tp , fp = self . fp + fp , tn = self . tn + tn , fn = self . fn + fn , )","title":"update()"},{"location":"api/metrics/AuxMetrics/","text":"jax_metrics.metrics.AuxMetrics Bases: SumMetric Source code in jax_metrics/metrics/metrics.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 class AuxMetrics ( SumMetric ): totals : tp . Optional [ tp . Dict [ str , jax . Array ]] = field () counts : tp . Optional [ tp . Dict [ str , jax . Array ]] = field () names : tp . Tuple [ str , ... ] = static_field () def __init__ ( self , names : tp . Iterable [ str ]): self . names = tuple ( names ) self . __dict__ . update ( self . _initial_values ()) def _initial_values ( self : A ) -> tp . Dict [ str , tp . Any ]: totals = { name : jnp . array ( 0.0 , dtype = jnp . float32 ) for name in self . names } counts = { name : jnp . array ( 0 , dtype = jnp . uint32 ) for name in self . names } return dict ( totals = totals , counts = counts ) def reset ( self : A ) -> A : return self . replace ( ** self . _initial_values ()) def update ( self : A , aux_values : tp . Dict [ str , jax . Array ], ** _ ) -> A : if self . totals is None or self . counts is None : raise ValueError ( \"AuxMetrics not initialized, call 'reset()' first\" ) totals = { name : ( self . totals [ name ] + aux_values [ name ]) . astype ( self . totals [ name ] . dtype ) for name in self . totals } counts = { name : ( self . counts [ name ] + np . prod ( aux_values [ name ] . shape )) . astype ( self . counts [ name ] . dtype ) for name in self . counts } return self . replace ( totals = totals , counts = counts ) def compute ( self ) -> tp . Dict [ str , jax . Array ]: if self . totals is None or self . counts is None : raise ValueError ( \"AuxMetrics not initialized, call `reset()` first\" ) return { name : self . totals [ name ] / self . counts [ name ] for name in self . totals } def compute_logs ( self ) -> tp . Dict [ str , jax . Array ]: return self . compute () def __call__ ( self : A , aux_values : tp . Any ) -> tp . Tuple [ tp . Dict [ str , jax . Array ], A ]: return super () . __call__ ( aux_values = aux_values )","title":"AuxMetrics"},{"location":"api/metrics/AuxMetrics/#jax_metricsmetricsauxmetrics","text":"Bases: SumMetric Source code in jax_metrics/metrics/metrics.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 class AuxMetrics ( SumMetric ): totals : tp . Optional [ tp . Dict [ str , jax . Array ]] = field () counts : tp . Optional [ tp . Dict [ str , jax . Array ]] = field () names : tp . Tuple [ str , ... ] = static_field () def __init__ ( self , names : tp . Iterable [ str ]): self . names = tuple ( names ) self . __dict__ . update ( self . _initial_values ()) def _initial_values ( self : A ) -> tp . Dict [ str , tp . Any ]: totals = { name : jnp . array ( 0.0 , dtype = jnp . float32 ) for name in self . names } counts = { name : jnp . array ( 0 , dtype = jnp . uint32 ) for name in self . names } return dict ( totals = totals , counts = counts ) def reset ( self : A ) -> A : return self . replace ( ** self . _initial_values ()) def update ( self : A , aux_values : tp . Dict [ str , jax . Array ], ** _ ) -> A : if self . totals is None or self . counts is None : raise ValueError ( \"AuxMetrics not initialized, call 'reset()' first\" ) totals = { name : ( self . totals [ name ] + aux_values [ name ]) . astype ( self . totals [ name ] . dtype ) for name in self . totals } counts = { name : ( self . counts [ name ] + np . prod ( aux_values [ name ] . shape )) . astype ( self . counts [ name ] . dtype ) for name in self . counts } return self . replace ( totals = totals , counts = counts ) def compute ( self ) -> tp . Dict [ str , jax . Array ]: if self . totals is None or self . counts is None : raise ValueError ( \"AuxMetrics not initialized, call `reset()` first\" ) return { name : self . totals [ name ] / self . counts [ name ] for name in self . totals } def compute_logs ( self ) -> tp . Dict [ str , jax . Array ]: return self . compute () def __call__ ( self : A , aux_values : tp . Any ) -> tp . Tuple [ tp . Dict [ str , jax . Array ], A ]: return super () . __call__ ( aux_values = aux_values )","title":"jax_metrics.metrics.AuxMetrics"},{"location":"api/metrics/Losses/","text":"jax_metrics.metrics.Losses Bases: SumMetric Source code in jax_metrics/metrics/losses.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 class Losses ( SumMetric ): totals : tp . Dict [ str , jax . Array ] counts : tp . Dict [ str , jax . Array ] losses : tp . Dict [ str , LossFn ] = static_field () def __init__ ( self , losses : tp . Dict [ str , LossFn ], ): self . losses = losses self . totals = { name : jnp . array ( 0.0 , dtype = jnp . float32 ) for name in self . losses } self . counts = { name : jnp . array ( 0 , dtype = jnp . uint32 ) for name in self . losses } def reset ( self : M ) -> M : return jax . tree_map ( jnp . zeros_like , self ) def update ( self : M , ** kwargs ) -> M : if self . totals is None or self . counts is None : raise ValueError ( \"Losses not initialized, call 'reset()' first\" ) totals = { name : self . totals [ name ] + loss ( ** kwargs ) for name , loss in self . losses . items () } counts = { name : value + 1 for name , value in self . counts . items ()} return self . replace ( totals = totals , counts = counts ) def compute ( self ) -> tp . Dict [ str , jax . Array ]: return { name : self . totals [ name ] / self . counts [ name ] for name in self . losses } def total_loss ( self ) -> jax . Array : return sum ( self . compute () . values (), jnp . array ( 0.0 )) def loss_and_update ( self : M , ** kwargs ) -> tp . Tuple [ jax . Array , M ]: batch_updates = self . batch_updates ( ** kwargs ) loss = batch_updates . total_loss () metrics = self . merge ( batch_updates ) return loss , metrics","title":"Losses"},{"location":"api/metrics/Losses/#jax_metricsmetricslosses","text":"Bases: SumMetric Source code in jax_metrics/metrics/losses.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 class Losses ( SumMetric ): totals : tp . Dict [ str , jax . Array ] counts : tp . Dict [ str , jax . Array ] losses : tp . Dict [ str , LossFn ] = static_field () def __init__ ( self , losses : tp . Dict [ str , LossFn ], ): self . losses = losses self . totals = { name : jnp . array ( 0.0 , dtype = jnp . float32 ) for name in self . losses } self . counts = { name : jnp . array ( 0 , dtype = jnp . uint32 ) for name in self . losses } def reset ( self : M ) -> M : return jax . tree_map ( jnp . zeros_like , self ) def update ( self : M , ** kwargs ) -> M : if self . totals is None or self . counts is None : raise ValueError ( \"Losses not initialized, call 'reset()' first\" ) totals = { name : self . totals [ name ] + loss ( ** kwargs ) for name , loss in self . losses . items () } counts = { name : value + 1 for name , value in self . counts . items ()} return self . replace ( totals = totals , counts = counts ) def compute ( self ) -> tp . Dict [ str , jax . Array ]: return { name : self . totals [ name ] / self . counts [ name ] for name in self . losses } def total_loss ( self ) -> jax . Array : return sum ( self . compute () . values (), jnp . array ( 0.0 )) def loss_and_update ( self : M , ** kwargs ) -> tp . Tuple [ jax . Array , M ]: batch_updates = self . batch_updates ( ** kwargs ) loss = batch_updates . total_loss () metrics = self . merge ( batch_updates ) return loss , metrics","title":"jax_metrics.metrics.Losses"},{"location":"api/metrics/MAE/","text":"jax_metrics.metrics.MAE","title":"MAE"},{"location":"api/metrics/MAE/#jax_metricsmetricsmae","text":"","title":"jax_metrics.metrics.MAE"},{"location":"api/metrics/MSE/","text":"jax_metrics.metrics.MSE","title":"MSE"},{"location":"api/metrics/MSE/#jax_metricsmetricsmse","text":"","title":"jax_metrics.metrics.MSE"},{"location":"api/metrics/Mean/","text":"jax_metrics.metrics.Mean Bases: Reduce Computes the (weighted) mean of the given values. For example, if values is [1, 3, 5, 7] then the mean is 4 . If the weights were specified as [1, 1, 0, 0] then the mean would be 2 . This metric creates two variables, total and count that are used to compute the average of values . This average is ultimately returned as mean which is an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: mean = elegy . metrics . Mean () result = mean ([ 1 , 3 , 5 , 7 ]) # 16 / 4 assert result == 4.0 result = mean ([ 4 , 10 ]) # 30 / 6 assert result == 5.0 Usage with elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanSquaredError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/metrics/mean.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 class Mean ( Reduce ): \"\"\" Computes the (weighted) mean of the given values. For example, if values is `[1, 3, 5, 7]` then the mean is `4`. If the weights were specified as `[1, 1, 0, 0]` then the mean would be `2`. This metric creates two variables, `total` and `count` that are used to compute the average of `values`. This average is ultimately returned as `mean` which is an idempotent operation that simply divides `total` by `count`. If `sample_weight` is `None`, weights default to 1. Use `sample_weight` of 0 to mask values. Usage: ```python mean = elegy.metrics.Mean() result = mean([1, 3, 5, 7]) # 16 / 4 assert result == 4.0 result = mean([4, 10]) # 30 / 6 assert result == 5.0 ``` Usage with elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanSquaredError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\"Creates a `Mean` instance. Arguments: dtype: (Optional) data type of the metric result. Defaults to `float32`. \"\"\" super () . __init__ ( reduction = Reduction . weighted_mean , dtype = dtype , ) def update ( self : M , values : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , ** _ , ) -> M : \"\"\" Accumulates the mean statistic over various batches. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Returns: Mean instance with updated state. \"\"\" return super () . update ( values = values , sample_weight = sample_weight , ) __init__ ( dtype = None ) Creates a Mean instance. Parameters: Name Type Description Default dtype tp . Optional [ jnp . dtype ] (Optional) data type of the metric result. Defaults to float32 . None Source code in jax_metrics/metrics/mean.py 47 48 49 50 51 52 53 54 55 56 57 58 def __init__ ( self , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\"Creates a `Mean` instance. Arguments: dtype: (Optional) data type of the metric result. Defaults to `float32`. \"\"\" super () . __init__ ( reduction = Reduction . weighted_mean , dtype = dtype , ) update ( values , sample_weight = None , ** _ ) Accumulates the mean statistic over various batches. Parameters: Name Type Description Default values jax . Array Per-example value. required sample_weight tp . Optional [ jax . Array ] Optional weighting of each example. None Returns: Type Description M Mean instance with updated state. Source code in jax_metrics/metrics/mean.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def update ( self : M , values : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , ** _ , ) -> M : \"\"\" Accumulates the mean statistic over various batches. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Returns: Mean instance with updated state. \"\"\" return super () . update ( values = values , sample_weight = sample_weight , )","title":"Mean"},{"location":"api/metrics/Mean/#jax_metricsmetricsmean","text":"Bases: Reduce Computes the (weighted) mean of the given values. For example, if values is [1, 3, 5, 7] then the mean is 4 . If the weights were specified as [1, 1, 0, 0] then the mean would be 2 . This metric creates two variables, total and count that are used to compute the average of values . This average is ultimately returned as mean which is an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: mean = elegy . metrics . Mean () result = mean ([ 1 , 3 , 5 , 7 ]) # 16 / 4 assert result == 4.0 result = mean ([ 4 , 10 ]) # 30 / 6 assert result == 5.0 Usage with elegy API: model = elegy . Model ( module_fn , loss = jm . losses . MeanSquaredError (), metrics = elegy . metrics . Mean (), ) Source code in jax_metrics/metrics/mean.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 class Mean ( Reduce ): \"\"\" Computes the (weighted) mean of the given values. For example, if values is `[1, 3, 5, 7]` then the mean is `4`. If the weights were specified as `[1, 1, 0, 0]` then the mean would be `2`. This metric creates two variables, `total` and `count` that are used to compute the average of `values`. This average is ultimately returned as `mean` which is an idempotent operation that simply divides `total` by `count`. If `sample_weight` is `None`, weights default to 1. Use `sample_weight` of 0 to mask values. Usage: ```python mean = elegy.metrics.Mean() result = mean([1, 3, 5, 7]) # 16 / 4 assert result == 4.0 result = mean([4, 10]) # 30 / 6 assert result == 5.0 ``` Usage with elegy API: ```python model = elegy.Model( module_fn, loss=jm.losses.MeanSquaredError(), metrics=elegy.metrics.Mean(), ) ``` \"\"\" def __init__ ( self , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\"Creates a `Mean` instance. Arguments: dtype: (Optional) data type of the metric result. Defaults to `float32`. \"\"\" super () . __init__ ( reduction = Reduction . weighted_mean , dtype = dtype , ) def update ( self : M , values : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , ** _ , ) -> M : \"\"\" Accumulates the mean statistic over various batches. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Returns: Mean instance with updated state. \"\"\" return super () . update ( values = values , sample_weight = sample_weight , )","title":"jax_metrics.metrics.Mean"},{"location":"api/metrics/Mean/#jax_metrics.metrics.mean.Mean.__init__","text":"Creates a Mean instance. Parameters: Name Type Description Default dtype tp . Optional [ jnp . dtype ] (Optional) data type of the metric result. Defaults to float32 . None Source code in jax_metrics/metrics/mean.py 47 48 49 50 51 52 53 54 55 56 57 58 def __init__ ( self , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\"Creates a `Mean` instance. Arguments: dtype: (Optional) data type of the metric result. Defaults to `float32`. \"\"\" super () . __init__ ( reduction = Reduction . weighted_mean , dtype = dtype , )","title":"__init__()"},{"location":"api/metrics/Mean/#jax_metrics.metrics.mean.Mean.update","text":"Accumulates the mean statistic over various batches. Parameters: Name Type Description Default values jax . Array Per-example value. required sample_weight tp . Optional [ jax . Array ] Optional weighting of each example. None Returns: Type Description M Mean instance with updated state. Source code in jax_metrics/metrics/mean.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def update ( self : M , values : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , ** _ , ) -> M : \"\"\" Accumulates the mean statistic over various batches. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Returns: Mean instance with updated state. \"\"\" return super () . update ( values = values , sample_weight = sample_weight , )","title":"update()"},{"location":"api/metrics/MeanAbsoluteError/","text":"jax_metrics.metrics.MeanAbsoluteError Bases: Mean Source code in jax_metrics/metrics/mean_absolute_error.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 class MeanAbsoluteError ( Mean ): def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Absolute Error`_ (MAE): .. math:: \\text{MAE} = \\frac{1}{N}\\sum_i^N | y_i - \\hat{y_i} | Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from jax_metrics.metrics.mean_absolute_error import MeanAbsolutError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mae = MeanAbsolutError() >>> mae(preds, target) \"\"\" super () . __init__ ( dtype = dtype ) def update ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , ** _ , ) -> tp . Any : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: MeanAbsoluteError instance with updated state \"\"\" values = _mean_absolute_error ( preds , target ) return super () . update ( values , sample_weight ) __init__ ( name = None , dtype = None ) Computes Mean Absolute Error _ (MAE): .. math:: ext{MAE} = rac{1}{N}\\sum_i^N | y_i - \\hat{y_i} | Where :math: y is a tensor of target values, and :math: \\hat{y} is a tensor of predictions. Parameters: Name Type Description Default name tp . Optional [ str ] Module name None dtype tp . Optional [ jnp . dtype ] Metrics states initialization dtype None Example: import jax.numpy as jnp from jax_metrics.metrics.mean_absolute_error import MeanAbsolutError target = jnp.array([3.0, -0.5, 2.0, 7.0]) preds = jnp.array([3.0, -0.5, 2.0, 7.0]) mae = MeanAbsolutError() mae(preds, target) Source code in jax_metrics/metrics/mean_absolute_error.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Absolute Error`_ (MAE): .. math:: \\text{MAE} = \\frac{1}{N}\\sum_i^N | y_i - \\hat{y_i} | Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from jax_metrics.metrics.mean_absolute_error import MeanAbsolutError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mae = MeanAbsolutError() >>> mae(preds, target) \"\"\" super () . __init__ ( dtype = dtype ) update ( target , preds , sample_weight = None , ** _ ) Accumulates metric statistics. target and preds should have the same shape. Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] . required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] required sample_weight tp . Optional [ jax . Array ] Optional weighting of each example. Defaults to 1. shape = [batch_size, d0, .. dN] None Returns: Type Description tp . Any MeanAbsoluteError instance with updated state Source code in jax_metrics/metrics/mean_absolute_error.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def update ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , ** _ , ) -> tp . Any : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: MeanAbsoluteError instance with updated state \"\"\" values = _mean_absolute_error ( preds , target ) return super () . update ( values , sample_weight )","title":"MeanAbsoluteError"},{"location":"api/metrics/MeanAbsoluteError/#jax_metricsmetricsmeanabsoluteerror","text":"Bases: Mean Source code in jax_metrics/metrics/mean_absolute_error.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 class MeanAbsoluteError ( Mean ): def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Absolute Error`_ (MAE): .. math:: \\text{MAE} = \\frac{1}{N}\\sum_i^N | y_i - \\hat{y_i} | Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from jax_metrics.metrics.mean_absolute_error import MeanAbsolutError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mae = MeanAbsolutError() >>> mae(preds, target) \"\"\" super () . __init__ ( dtype = dtype ) def update ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , ** _ , ) -> tp . Any : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: MeanAbsoluteError instance with updated state \"\"\" values = _mean_absolute_error ( preds , target ) return super () . update ( values , sample_weight )","title":"jax_metrics.metrics.MeanAbsoluteError"},{"location":"api/metrics/MeanAbsoluteError/#jax_metrics.metrics.mean_absolute_error.MeanAbsoluteError.__init__","text":"Computes Mean Absolute Error _ (MAE): .. math:: ext{MAE} = rac{1}{N}\\sum_i^N | y_i - \\hat{y_i} | Where :math: y is a tensor of target values, and :math: \\hat{y} is a tensor of predictions. Parameters: Name Type Description Default name tp . Optional [ str ] Module name None dtype tp . Optional [ jnp . dtype ] Metrics states initialization dtype None Example: import jax.numpy as jnp from jax_metrics.metrics.mean_absolute_error import MeanAbsolutError target = jnp.array([3.0, -0.5, 2.0, 7.0]) preds = jnp.array([3.0, -0.5, 2.0, 7.0]) mae = MeanAbsolutError() mae(preds, target) Source code in jax_metrics/metrics/mean_absolute_error.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Absolute Error`_ (MAE): .. math:: \\text{MAE} = \\frac{1}{N}\\sum_i^N | y_i - \\hat{y_i} | Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from jax_metrics.metrics.mean_absolute_error import MeanAbsolutError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mae = MeanAbsolutError() >>> mae(preds, target) \"\"\" super () . __init__ ( dtype = dtype )","title":"__init__()"},{"location":"api/metrics/MeanAbsoluteError/#jax_metrics.metrics.mean_absolute_error.MeanAbsoluteError.update","text":"Accumulates metric statistics. target and preds should have the same shape. Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] . required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] required sample_weight tp . Optional [ jax . Array ] Optional weighting of each example. Defaults to 1. shape = [batch_size, d0, .. dN] None Returns: Type Description tp . Any MeanAbsoluteError instance with updated state Source code in jax_metrics/metrics/mean_absolute_error.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def update ( self , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , ** _ , ) -> tp . Any : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: MeanAbsoluteError instance with updated state \"\"\" values = _mean_absolute_error ( preds , target ) return super () . update ( values , sample_weight )","title":"update()"},{"location":"api/metrics/MeanSquareError/","text":"jax_metrics.metrics.MeanSquareError Bases: Mean Source code in jax_metrics/metrics/mean_square_error.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 class MeanSquareError ( Mean ): def __init__ ( self , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Square Error`_ (MSE): .. math:: \\text{MSE} = \\frac{1}{N}\\sum_i^N(y_i - \\hat{y_i})^2 Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from jax_metrics.metrics.mean_square_error import MeanSquareError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mse = MeanSquareError() >>> mse(preds, target) \"\"\" super () . __init__ ( dtype = dtype ) def update ( self : M , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , ** _ , ) -> M : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: MeanSquareError instance with updated state \"\"\" values = _mean_square_error ( preds , target ) return super () . update ( values , sample_weight ) __init__ ( dtype = None ) Computes Mean Square Error _ (MSE): .. math:: ext{MSE} = rac{1}{N}\\sum_i^N(y_i - \\hat{y_i})^2 Where :math: y is a tensor of target values, and :math: \\hat{y} is a tensor of predictions. Parameters: Name Type Description Default name Module name required dtype tp . Optional [ jnp . dtype ] Metrics states initialization dtype None Example: import jax.numpy as jnp from jax_metrics.metrics.mean_square_error import MeanSquareError target = jnp.array([3.0, -0.5, 2.0, 7.0]) preds = jnp.array([3.0, -0.5, 2.0, 7.0]) mse = MeanSquareError() mse(preds, target) Source code in jax_metrics/metrics/mean_square_error.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Square Error`_ (MSE): .. math:: \\text{MSE} = \\frac{1}{N}\\sum_i^N(y_i - \\hat{y_i})^2 Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from jax_metrics.metrics.mean_square_error import MeanSquareError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mse = MeanSquareError() >>> mse(preds, target) \"\"\" super () . __init__ ( dtype = dtype ) update ( target , preds , sample_weight = None , ** _ ) Accumulates metric statistics. target and preds should have the same shape. Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] . required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] required sample_weight tp . Optional [ jax . Array ] Optional weighting of each example. Defaults to 1. shape = [batch_size, d0, .. dN] None Returns: Type Description M MeanSquareError instance with updated state Source code in jax_metrics/metrics/mean_square_error.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def update ( self : M , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , ** _ , ) -> M : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: MeanSquareError instance with updated state \"\"\" values = _mean_square_error ( preds , target ) return super () . update ( values , sample_weight )","title":"MeanSquareError"},{"location":"api/metrics/MeanSquareError/#jax_metricsmetricsmeansquareerror","text":"Bases: Mean Source code in jax_metrics/metrics/mean_square_error.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 class MeanSquareError ( Mean ): def __init__ ( self , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Square Error`_ (MSE): .. math:: \\text{MSE} = \\frac{1}{N}\\sum_i^N(y_i - \\hat{y_i})^2 Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from jax_metrics.metrics.mean_square_error import MeanSquareError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mse = MeanSquareError() >>> mse(preds, target) \"\"\" super () . __init__ ( dtype = dtype ) def update ( self : M , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , ** _ , ) -> M : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: MeanSquareError instance with updated state \"\"\" values = _mean_square_error ( preds , target ) return super () . update ( values , sample_weight )","title":"jax_metrics.metrics.MeanSquareError"},{"location":"api/metrics/MeanSquareError/#jax_metrics.metrics.mean_square_error.MeanSquareError.__init__","text":"Computes Mean Square Error _ (MSE): .. math:: ext{MSE} = rac{1}{N}\\sum_i^N(y_i - \\hat{y_i})^2 Where :math: y is a tensor of target values, and :math: \\hat{y} is a tensor of predictions. Parameters: Name Type Description Default name Module name required dtype tp . Optional [ jnp . dtype ] Metrics states initialization dtype None Example: import jax.numpy as jnp from jax_metrics.metrics.mean_square_error import MeanSquareError target = jnp.array([3.0, -0.5, 2.0, 7.0]) preds = jnp.array([3.0, -0.5, 2.0, 7.0]) mse = MeanSquareError() mse(preds, target) Source code in jax_metrics/metrics/mean_square_error.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Square Error`_ (MSE): .. math:: \\text{MSE} = \\frac{1}{N}\\sum_i^N(y_i - \\hat{y_i})^2 Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from jax_metrics.metrics.mean_square_error import MeanSquareError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mse = MeanSquareError() >>> mse(preds, target) \"\"\" super () . __init__ ( dtype = dtype )","title":"__init__()"},{"location":"api/metrics/MeanSquareError/#jax_metrics.metrics.mean_square_error.MeanSquareError.update","text":"Accumulates metric statistics. target and preds should have the same shape. Parameters: Name Type Description Default target jax . Array Ground truth values. shape = [batch_size, d0, .. dN] . required preds jax . Array The predicted values. shape = [batch_size, d0, .. dN] required sample_weight tp . Optional [ jax . Array ] Optional weighting of each example. Defaults to 1. shape = [batch_size, d0, .. dN] None Returns: Type Description M MeanSquareError instance with updated state Source code in jax_metrics/metrics/mean_square_error.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def update ( self : M , target : jax . Array , preds : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , ** _ , ) -> M : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: MeanSquareError instance with updated state \"\"\" values = _mean_square_error ( preds , target ) return super () . update ( values , sample_weight )","title":"update()"},{"location":"api/metrics/Metric/","text":"jax_metrics.metrics.Metric Bases: Pytree Encapsulates metric logic and state. Metrics accumulate state between calls such that their output value reflect the metric as if calculated on the whole data given up to that point. Source code in jax_metrics/metrics/metric.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 class Metric ( Pytree ): \"\"\" Encapsulates metric logic and state. Metrics accumulate state between calls such that their output value reflect the metric as if calculated on the whole data given up to that point. \"\"\" def __call__ ( self : M , ** kwargs : tp . Any ) -> tp . Tuple [ tp . Any , M ]: metric : M = self batch_updates = metric . batch_updates ( ** kwargs ) batch_values = batch_updates . compute () metric = metric . merge ( batch_updates ) return batch_values , metric @abstractmethod def reset ( self : M ) -> M : \"\"\" Resets the metric state. Returns: Metric with the initial state \"\"\" ... @abstractmethod def update ( self : M , ** kwargs : tp . Any ) -> M : \"\"\" Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining `**kwargs`. Arguments: **kwargs: data to update the metric with Returns: Metric with updated state \"\"\" ... @abstractmethod def compute ( self ) -> tp . Any : \"\"\" Compute the current metric value. \"\"\" ... @abstractmethod def reduce ( self : M ) -> M : \"\"\" Aggregate metric state. It assumes the metric's internal state has an additional 'device' dimension on the 0th axis. Example: ```python batch_updates = metric.batch_updates(**kwargs) batch_updates = jax.lax.all_gather(batch_updates, axis_name=\"device\") batch_updates = batch_updates.reduce() metric = metric.merge(batch_updates) ``` Returns: Metric with aggregated state \"\"\" # return jax.tree_map(lambda x: jnp.sum(x, axis=0), self) ... @abstractmethod def merge ( self : M , other : M ) -> M : \"\"\" Merge the state of two metrics of the same type. Usually used to merge a metric with its batch_updates. Example: ```python batch_updates = metric.batch_updates(**kwargs) metric = metric.merge(batch_updates) ``` \"\"\" # return jax.tree_map(lambda x, y: x + y, self, other) ... def batch_updates ( self : M , ** kwargs : tp . Any ) -> M : \"\"\" Compute metric updates for a batch of data. Equivalent to `.reset().update(**kwargs)`. Arguments: kwargs: data to update the metric with Returns: Metric with updated state \"\"\" return self . reset () . update ( ** kwargs ) def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedMetric\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python metrics = jm.Metrics([ jm.metrics.Mean().index_into(values=[\"a\"]), jm.metrics.Mean().index_into(values=[\"b\"]), ]).reset() metrics = metrics.update(values={ \"a\": loss0, \"b\": loss1, }) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. This also works with Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedMetric instance \"\"\" return IndexedMetric ( self , kwargs ) def rename_arguments ( self , ** kwargs : str ) -> \"RenameArguments\" : \"\"\" Returns a metric that renames the keyword arguments expected by `.update()`. Example: ```python mean = jm.metrics.Mean().rename_arguments(values=\"loss\") ... loss = loss_fn(x, y) mean = mean.update(loss=loss) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A RenameArguments instance \"\"\" return RenameArguments ( self , kwargs ) batch_updates ( ** kwargs ) Compute metric updates for a batch of data. Equivalent to .reset().update(**kwargs) . Parameters: Name Type Description Default kwargs tp . Any data to update the metric with {} Returns: Type Description M Metric with updated state Source code in jax_metrics/metrics/metric.py 103 104 105 106 107 108 109 110 111 112 113 114 def batch_updates ( self : M , ** kwargs : tp . Any ) -> M : \"\"\" Compute metric updates for a batch of data. Equivalent to `.reset().update(**kwargs)`. Arguments: kwargs: data to update the metric with Returns: Metric with updated state \"\"\" return self . reset () . update ( ** kwargs ) compute () abstractmethod Compute the current metric value. Source code in jax_metrics/metrics/metric.py 58 59 60 61 62 63 @abstractmethod def compute ( self ) -> tp . Any : \"\"\" Compute the current metric value. \"\"\" ... index_into ( ** kwargs ) Returns a metric that \"indexes\" the specified keyword arguments expected by .update() . You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing ( __getitem__ ). Example: metrics = jm . Metrics ([ jm . metrics . Mean () . index_into ( values = [ \"a\" ]), jm . metrics . Mean () . index_into ( values = [ \"b\" ]), ]) . reset () metrics = metrics . update ( values = { \"a\" : loss0 , \"b\" : loss1 , }) Here values is set to a dict of arrays, but thanks to .index_into() each loss can index into its correspoding array. This also works with Parameters: Name Type Description Default **kwargs types . IndexLike keyword arguments to be indexed {} Returns: Type Description IndexedMetric A IndexedMetric instance Source code in jax_metrics/metrics/metric.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedMetric\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python metrics = jm.Metrics([ jm.metrics.Mean().index_into(values=[\"a\"]), jm.metrics.Mean().index_into(values=[\"b\"]), ]).reset() metrics = metrics.update(values={ \"a\": loss0, \"b\": loss1, }) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. This also works with Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedMetric instance \"\"\" return IndexedMetric ( self , kwargs ) merge ( other ) abstractmethod Merge the state of two metrics of the same type. Usually used to merge a metric with its batch_updates. Example: batch_updates = metric . batch_updates ( ** kwargs ) metric = metric . merge ( batch_updates ) Source code in jax_metrics/metrics/metric.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 @abstractmethod def merge ( self : M , other : M ) -> M : \"\"\" Merge the state of two metrics of the same type. Usually used to merge a metric with its batch_updates. Example: ```python batch_updates = metric.batch_updates(**kwargs) metric = metric.merge(batch_updates) ``` \"\"\" # return jax.tree_map(lambda x, y: x + y, self, other) ... reduce () abstractmethod Aggregate metric state. It assumes the metric's internal state has an additional 'device' dimension on the 0th axis. Example: batch_updates = metric . batch_updates ( ** kwargs ) batch_updates = jax . lax . all_gather ( batch_updates , axis_name = \"device\" ) batch_updates = batch_updates . reduce () metric = metric . merge ( batch_updates ) Returns: Type Description M Metric with aggregated state Source code in jax_metrics/metrics/metric.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 @abstractmethod def reduce ( self : M ) -> M : \"\"\" Aggregate metric state. It assumes the metric's internal state has an additional 'device' dimension on the 0th axis. Example: ```python batch_updates = metric.batch_updates(**kwargs) batch_updates = jax.lax.all_gather(batch_updates, axis_name=\"device\") batch_updates = batch_updates.reduce() metric = metric.merge(batch_updates) ``` Returns: Metric with aggregated state \"\"\" # return jax.tree_map(lambda x: jnp.sum(x, axis=0), self) ... rename_arguments ( ** kwargs ) Returns a metric that renames the keyword arguments expected by .update() . Example: mean = jm . metrics . Mean () . rename_arguments ( values = \"loss\" ) ... loss = loss_fn ( x , y ) mean = mean . update ( loss = loss ) Parameters: Name Type Description Default **kwargs str keyword arguments to be renamed {} Returns: Type Description RenameArguments A RenameArguments instance Source code in jax_metrics/metrics/metric.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def rename_arguments ( self , ** kwargs : str ) -> \"RenameArguments\" : \"\"\" Returns a metric that renames the keyword arguments expected by `.update()`. Example: ```python mean = jm.metrics.Mean().rename_arguments(values=\"loss\") ... loss = loss_fn(x, y) mean = mean.update(loss=loss) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A RenameArguments instance \"\"\" return RenameArguments ( self , kwargs ) reset () abstractmethod Resets the metric state. Returns: Type Description M Metric with the initial state Source code in jax_metrics/metrics/metric.py 33 34 35 36 37 38 39 40 41 @abstractmethod def reset ( self : M ) -> M : \"\"\" Resets the metric state. Returns: Metric with the initial state \"\"\" ... update ( ** kwargs ) abstractmethod Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining **kwargs . Parameters: Name Type Description Default **kwargs tp . Any data to update the metric with {} Returns: Type Description M Metric with updated state Source code in jax_metrics/metrics/metric.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 @abstractmethod def update ( self : M , ** kwargs : tp . Any ) -> M : \"\"\" Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining `**kwargs`. Arguments: **kwargs: data to update the metric with Returns: Metric with updated state \"\"\" ...","title":"Metric"},{"location":"api/metrics/Metric/#jax_metricsmetricsmetric","text":"Bases: Pytree Encapsulates metric logic and state. Metrics accumulate state between calls such that their output value reflect the metric as if calculated on the whole data given up to that point. Source code in jax_metrics/metrics/metric.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 class Metric ( Pytree ): \"\"\" Encapsulates metric logic and state. Metrics accumulate state between calls such that their output value reflect the metric as if calculated on the whole data given up to that point. \"\"\" def __call__ ( self : M , ** kwargs : tp . Any ) -> tp . Tuple [ tp . Any , M ]: metric : M = self batch_updates = metric . batch_updates ( ** kwargs ) batch_values = batch_updates . compute () metric = metric . merge ( batch_updates ) return batch_values , metric @abstractmethod def reset ( self : M ) -> M : \"\"\" Resets the metric state. Returns: Metric with the initial state \"\"\" ... @abstractmethod def update ( self : M , ** kwargs : tp . Any ) -> M : \"\"\" Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining `**kwargs`. Arguments: **kwargs: data to update the metric with Returns: Metric with updated state \"\"\" ... @abstractmethod def compute ( self ) -> tp . Any : \"\"\" Compute the current metric value. \"\"\" ... @abstractmethod def reduce ( self : M ) -> M : \"\"\" Aggregate metric state. It assumes the metric's internal state has an additional 'device' dimension on the 0th axis. Example: ```python batch_updates = metric.batch_updates(**kwargs) batch_updates = jax.lax.all_gather(batch_updates, axis_name=\"device\") batch_updates = batch_updates.reduce() metric = metric.merge(batch_updates) ``` Returns: Metric with aggregated state \"\"\" # return jax.tree_map(lambda x: jnp.sum(x, axis=0), self) ... @abstractmethod def merge ( self : M , other : M ) -> M : \"\"\" Merge the state of two metrics of the same type. Usually used to merge a metric with its batch_updates. Example: ```python batch_updates = metric.batch_updates(**kwargs) metric = metric.merge(batch_updates) ``` \"\"\" # return jax.tree_map(lambda x, y: x + y, self, other) ... def batch_updates ( self : M , ** kwargs : tp . Any ) -> M : \"\"\" Compute metric updates for a batch of data. Equivalent to `.reset().update(**kwargs)`. Arguments: kwargs: data to update the metric with Returns: Metric with updated state \"\"\" return self . reset () . update ( ** kwargs ) def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedMetric\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python metrics = jm.Metrics([ jm.metrics.Mean().index_into(values=[\"a\"]), jm.metrics.Mean().index_into(values=[\"b\"]), ]).reset() metrics = metrics.update(values={ \"a\": loss0, \"b\": loss1, }) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. This also works with Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedMetric instance \"\"\" return IndexedMetric ( self , kwargs ) def rename_arguments ( self , ** kwargs : str ) -> \"RenameArguments\" : \"\"\" Returns a metric that renames the keyword arguments expected by `.update()`. Example: ```python mean = jm.metrics.Mean().rename_arguments(values=\"loss\") ... loss = loss_fn(x, y) mean = mean.update(loss=loss) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A RenameArguments instance \"\"\" return RenameArguments ( self , kwargs )","title":"jax_metrics.metrics.Metric"},{"location":"api/metrics/Metric/#jax_metrics.metrics.metric.Metric.batch_updates","text":"Compute metric updates for a batch of data. Equivalent to .reset().update(**kwargs) . Parameters: Name Type Description Default kwargs tp . Any data to update the metric with {} Returns: Type Description M Metric with updated state Source code in jax_metrics/metrics/metric.py 103 104 105 106 107 108 109 110 111 112 113 114 def batch_updates ( self : M , ** kwargs : tp . Any ) -> M : \"\"\" Compute metric updates for a batch of data. Equivalent to `.reset().update(**kwargs)`. Arguments: kwargs: data to update the metric with Returns: Metric with updated state \"\"\" return self . reset () . update ( ** kwargs )","title":"batch_updates()"},{"location":"api/metrics/Metric/#jax_metrics.metrics.metric.Metric.compute","text":"Compute the current metric value. Source code in jax_metrics/metrics/metric.py 58 59 60 61 62 63 @abstractmethod def compute ( self ) -> tp . Any : \"\"\" Compute the current metric value. \"\"\" ...","title":"compute()"},{"location":"api/metrics/Metric/#jax_metrics.metrics.metric.Metric.index_into","text":"Returns a metric that \"indexes\" the specified keyword arguments expected by .update() . You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing ( __getitem__ ). Example: metrics = jm . Metrics ([ jm . metrics . Mean () . index_into ( values = [ \"a\" ]), jm . metrics . Mean () . index_into ( values = [ \"b\" ]), ]) . reset () metrics = metrics . update ( values = { \"a\" : loss0 , \"b\" : loss1 , }) Here values is set to a dict of arrays, but thanks to .index_into() each loss can index into its correspoding array. This also works with Parameters: Name Type Description Default **kwargs types . IndexLike keyword arguments to be indexed {} Returns: Type Description IndexedMetric A IndexedMetric instance Source code in jax_metrics/metrics/metric.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def index_into ( self , ** kwargs : types . IndexLike ) -> \"IndexedMetric\" : \"\"\" Returns a metric that \"indexes\" the specified keyword arguments expected by `.update()`. You can index into nested structures such as combinations of lists, tuples, dicts, or any other structure that supports indexing (`__getitem__`). Example: ```python metrics = jm.Metrics([ jm.metrics.Mean().index_into(values=[\"a\"]), jm.metrics.Mean().index_into(values=[\"b\"]), ]).reset() metrics = metrics.update(values={ \"a\": loss0, \"b\": loss1, }) ``` Here `values` is set to a dict of arrays, but thanks to `.index_into()` each loss can index into its correspoding array. This also works with Arguments: **kwargs: keyword arguments to be indexed Returns: A IndexedMetric instance \"\"\" return IndexedMetric ( self , kwargs )","title":"index_into()"},{"location":"api/metrics/Metric/#jax_metrics.metrics.metric.Metric.merge","text":"Merge the state of two metrics of the same type. Usually used to merge a metric with its batch_updates. Example: batch_updates = metric . batch_updates ( ** kwargs ) metric = metric . merge ( batch_updates ) Source code in jax_metrics/metrics/metric.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 @abstractmethod def merge ( self : M , other : M ) -> M : \"\"\" Merge the state of two metrics of the same type. Usually used to merge a metric with its batch_updates. Example: ```python batch_updates = metric.batch_updates(**kwargs) metric = metric.merge(batch_updates) ``` \"\"\" # return jax.tree_map(lambda x, y: x + y, self, other) ...","title":"merge()"},{"location":"api/metrics/Metric/#jax_metrics.metrics.metric.Metric.reduce","text":"Aggregate metric state. It assumes the metric's internal state has an additional 'device' dimension on the 0th axis. Example: batch_updates = metric . batch_updates ( ** kwargs ) batch_updates = jax . lax . all_gather ( batch_updates , axis_name = \"device\" ) batch_updates = batch_updates . reduce () metric = metric . merge ( batch_updates ) Returns: Type Description M Metric with aggregated state Source code in jax_metrics/metrics/metric.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 @abstractmethod def reduce ( self : M ) -> M : \"\"\" Aggregate metric state. It assumes the metric's internal state has an additional 'device' dimension on the 0th axis. Example: ```python batch_updates = metric.batch_updates(**kwargs) batch_updates = jax.lax.all_gather(batch_updates, axis_name=\"device\") batch_updates = batch_updates.reduce() metric = metric.merge(batch_updates) ``` Returns: Metric with aggregated state \"\"\" # return jax.tree_map(lambda x: jnp.sum(x, axis=0), self) ...","title":"reduce()"},{"location":"api/metrics/Metric/#jax_metrics.metrics.metric.Metric.rename_arguments","text":"Returns a metric that renames the keyword arguments expected by .update() . Example: mean = jm . metrics . Mean () . rename_arguments ( values = \"loss\" ) ... loss = loss_fn ( x , y ) mean = mean . update ( loss = loss ) Parameters: Name Type Description Default **kwargs str keyword arguments to be renamed {} Returns: Type Description RenameArguments A RenameArguments instance Source code in jax_metrics/metrics/metric.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def rename_arguments ( self , ** kwargs : str ) -> \"RenameArguments\" : \"\"\" Returns a metric that renames the keyword arguments expected by `.update()`. Example: ```python mean = jm.metrics.Mean().rename_arguments(values=\"loss\") ... loss = loss_fn(x, y) mean = mean.update(loss=loss) ``` Arguments: **kwargs: keyword arguments to be renamed Returns: A RenameArguments instance \"\"\" return RenameArguments ( self , kwargs )","title":"rename_arguments()"},{"location":"api/metrics/Metric/#jax_metrics.metrics.metric.Metric.reset","text":"Resets the metric state. Returns: Type Description M Metric with the initial state Source code in jax_metrics/metrics/metric.py 33 34 35 36 37 38 39 40 41 @abstractmethod def reset ( self : M ) -> M : \"\"\" Resets the metric state. Returns: Metric with the initial state \"\"\" ...","title":"reset()"},{"location":"api/metrics/Metric/#jax_metrics.metrics.metric.Metric.update","text":"Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining **kwargs . Parameters: Name Type Description Default **kwargs tp . Any data to update the metric with {} Returns: Type Description M Metric with updated state Source code in jax_metrics/metrics/metric.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 @abstractmethod def update ( self : M , ** kwargs : tp . Any ) -> M : \"\"\" Update the metric with the given data. Each metric accepts a different set of keyword arguments and must accept other keyword arguments, even if they not used by as remaining `**kwargs`. Arguments: **kwargs: data to update the metric with Returns: Metric with updated state \"\"\" ...","title":"update()"},{"location":"api/metrics/Metrics/","text":"jax_metrics.metrics.Metrics Bases: Metric Source code in jax_metrics/metrics/metrics.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 @dataclasses . dataclass class Metrics ( Metric ): metrics : tp . Dict [ str , Metric ] def reset ( self : M ) -> M : metrics = { name : metric . reset () for name , metric in self . metrics . items ()} return self . replace ( metrics = metrics ) def update ( self : M , ** kwargs ) -> M : \"\"\" Update all metrics with the given values. Each metric will receive the same keyword arguments but each can internally select the values to use. If a required value is not provided, the metric will raise a TypeError. Arguments: **kwargs: Keyword arguments to pass to each metric. Returns: Metrics instance with updated state. \"\"\" metrics = { name : metric . update ( ** kwargs ) for name , metric in self . metrics . items () } return self . replace ( metrics = metrics ) def compute ( self ) -> tp . Dict [ str , jax . Array ]: outputs = {} names = set () for name , metric in self . metrics . items (): value = metric . compute () for path , value , parent_iterable in utils . _flatten_names ( value ): name = utils . _unique_name ( names , name ) if path : if parent_iterable : name = f \" { path } / { name } \" else : name = path outputs [ name ] = value return outputs def slice ( self , ** kwargs : types . IndexLike ) -> \"Metrics\" : metrics = { name : metric . index_into ( ** kwargs ) for name , metric in self . metrics . items () } return self . replace ( metrics = metrics ) def merge ( self : M , other : M ) -> M : return type ( self )( metrics = { name : metric . merge ( other . metrics [ name ]) for name , metric in self . metrics . items () } ) def reduce ( self : M ) -> M : return type ( self )( metrics = { name : metric . reduce () for name , metric in self . metrics . items ()} ) update ( ** kwargs ) Update all metrics with the given values. Each metric will receive the same keyword arguments but each can internally select the values to use. If a required value is not provided, the metric will raise a TypeError. Parameters: Name Type Description Default **kwargs Keyword arguments to pass to each metric. {} Returns: Type Description M Metrics instance with updated state. Source code in jax_metrics/metrics/metrics.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def update ( self : M , ** kwargs ) -> M : \"\"\" Update all metrics with the given values. Each metric will receive the same keyword arguments but each can internally select the values to use. If a required value is not provided, the metric will raise a TypeError. Arguments: **kwargs: Keyword arguments to pass to each metric. Returns: Metrics instance with updated state. \"\"\" metrics = { name : metric . update ( ** kwargs ) for name , metric in self . metrics . items () } return self . replace ( metrics = metrics )","title":"Metrics"},{"location":"api/metrics/Metrics/#jax_metricsmetricsmetrics","text":"Bases: Metric Source code in jax_metrics/metrics/metrics.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 @dataclasses . dataclass class Metrics ( Metric ): metrics : tp . Dict [ str , Metric ] def reset ( self : M ) -> M : metrics = { name : metric . reset () for name , metric in self . metrics . items ()} return self . replace ( metrics = metrics ) def update ( self : M , ** kwargs ) -> M : \"\"\" Update all metrics with the given values. Each metric will receive the same keyword arguments but each can internally select the values to use. If a required value is not provided, the metric will raise a TypeError. Arguments: **kwargs: Keyword arguments to pass to each metric. Returns: Metrics instance with updated state. \"\"\" metrics = { name : metric . update ( ** kwargs ) for name , metric in self . metrics . items () } return self . replace ( metrics = metrics ) def compute ( self ) -> tp . Dict [ str , jax . Array ]: outputs = {} names = set () for name , metric in self . metrics . items (): value = metric . compute () for path , value , parent_iterable in utils . _flatten_names ( value ): name = utils . _unique_name ( names , name ) if path : if parent_iterable : name = f \" { path } / { name } \" else : name = path outputs [ name ] = value return outputs def slice ( self , ** kwargs : types . IndexLike ) -> \"Metrics\" : metrics = { name : metric . index_into ( ** kwargs ) for name , metric in self . metrics . items () } return self . replace ( metrics = metrics ) def merge ( self : M , other : M ) -> M : return type ( self )( metrics = { name : metric . merge ( other . metrics [ name ]) for name , metric in self . metrics . items () } ) def reduce ( self : M ) -> M : return type ( self )( metrics = { name : metric . reduce () for name , metric in self . metrics . items ()} )","title":"jax_metrics.metrics.Metrics"},{"location":"api/metrics/Metrics/#jax_metrics.metrics.metrics.Metrics.update","text":"Update all metrics with the given values. Each metric will receive the same keyword arguments but each can internally select the values to use. If a required value is not provided, the metric will raise a TypeError. Parameters: Name Type Description Default **kwargs Keyword arguments to pass to each metric. {} Returns: Type Description M Metrics instance with updated state. Source code in jax_metrics/metrics/metrics.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def update ( self : M , ** kwargs ) -> M : \"\"\" Update all metrics with the given values. Each metric will receive the same keyword arguments but each can internally select the values to use. If a required value is not provided, the metric will raise a TypeError. Arguments: **kwargs: Keyword arguments to pass to each metric. Returns: Metrics instance with updated state. \"\"\" metrics = { name : metric . update ( ** kwargs ) for name , metric in self . metrics . items () } return self . replace ( metrics = metrics )","title":"update()"},{"location":"api/metrics/Reduce/","text":"jax_metrics.metrics.Reduce Bases: SumMetric Encapsulates metrics that perform a reduce operation on the values. Source code in jax_metrics/metrics/reduce.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 class Reduce ( SumMetric ): \"\"\"Encapsulates metrics that perform a reduce operation on the values.\"\"\" total : jax . Array count : tp . Optional [ jax . Array ] reduction : Reduction = static_field () dtype : jnp . dtype = static_field () def __init__ ( self , reduction : tp . Union [ Reduction , str ], dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" Creates a `Reduce` instance. Arguments: reduction: (Optional) The reduction to apply to the metric values. Defaults to `SUM_OVER_BATCH_SIZE`. dtype: (Optional) data type of the metric result. Defaults to `float32`. \"\"\" self . reduction = ( reduction if isinstance ( reduction , Reduction ) else Reduction [ reduction ] ) self . dtype = dtype or jnp . float32 self . __dict__ . update ( self . _initial_values ()) def _initial_values ( self ) -> tp . Dict [ str , tp . Any ]: # initialize states total = jnp . array ( 0.0 , dtype = self . dtype ) if self . reduction in ( Reduction . sum_over_batch_size , Reduction . weighted_mean , ): count = jnp . array ( 0 , dtype = jnp . uint32 ) else : count = None return dict ( total = total , count = count ) def reset ( self : M ) -> M : \"\"\" Resets all of the metric state variables. Returns: An instance of `Reduce`. \"\"\" return self . replace ( ** self . _initial_values ()) def update ( self : M , values : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , ** _ , ) -> M : \"\"\" Accumulates statistics for computing the reduction metric. For example, if `values` is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of `result()` is 4. If the `sample_weight` is specified as [1, 1, 0, 0] then value of `result()` would be 2. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Defaults to 1. Returns: Array with the cumulative reduce. \"\"\" # perform update if sample_weight is not None : if sample_weight . ndim > values . ndim : raise Exception ( f \"sample_weight dimention is higher than values, when masking values sample_weight dimention needs to be equal or lower than values dimension, currently values have shape equal to { values . shape } \" ) try : # Broadcast weights if possible. sample_weight = jnp . broadcast_to ( sample_weight , values . shape ) except ValueError : # Reduce values to same ndim as weight array values_ndim , weight_ndim = values . ndim , sample_weight . ndim if self . reduction == Reduction . sum : values = jnp . sum ( values , axis = list ( range ( weight_ndim , values_ndim ))) else : values = jnp . mean ( values , axis = list ( range ( weight_ndim , values_ndim )) ) values = values * sample_weight value_sum = jnp . sum ( values ) total = ( self . total + value_sum ) . astype ( self . total . dtype ) # Exit early if the reduction doesn't have a denominator. if self . reduction == Reduction . sum : num_values = None # Update `count` for reductions that require a denominator. elif self . reduction == Reduction . sum_over_batch_size : num_values = np . prod ( values . shape ) else : if sample_weight is None : num_values = np . prod ( values . shape ) else : num_values = jnp . sum ( sample_weight ) if self . count is not None : assert num_values is not None count = ( self . count + num_values ) . astype ( self . count . dtype ) else : count = None return self . replace ( total = total , count = count ) def compute ( self ) -> jax . Array : if self . reduction == Reduction . sum : return self . total else : return self . total / self . count def from_argument ( self , argument : str ) -> RenameArguments : return self . rename_arguments ( values = argument ) __init__ ( reduction , dtype = None ) Creates a Reduce instance. Parameters: Name Type Description Default reduction tp . Union [ Reduction , str ] (Optional) The reduction to apply to the metric values. Defaults to SUM_OVER_BATCH_SIZE . required dtype tp . Optional [ jnp . dtype ] (Optional) data type of the metric result. Defaults to float32 . None Source code in jax_metrics/metrics/reduce.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , reduction : tp . Union [ Reduction , str ], dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" Creates a `Reduce` instance. Arguments: reduction: (Optional) The reduction to apply to the metric values. Defaults to `SUM_OVER_BATCH_SIZE`. dtype: (Optional) data type of the metric result. Defaults to `float32`. \"\"\" self . reduction = ( reduction if isinstance ( reduction , Reduction ) else Reduction [ reduction ] ) self . dtype = dtype or jnp . float32 self . __dict__ . update ( self . _initial_values ()) reset () Resets all of the metric state variables. Returns: Type Description M An instance of Reduce . Source code in jax_metrics/metrics/reduce.py 62 63 64 65 66 67 68 def reset ( self : M ) -> M : \"\"\" Resets all of the metric state variables. Returns: An instance of `Reduce`. \"\"\" return self . replace ( ** self . _initial_values ()) update ( values , sample_weight = None , ** _ ) Accumulates statistics for computing the reduction metric. For example, if values is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of result() is 4. If the sample_weight is specified as [1, 1, 0, 0] then value of result() would be 2. Parameters: Name Type Description Default values jax . Array Per-example value. required sample_weight tp . Optional [ jax . Array ] Optional weighting of each example. Defaults to 1. None Returns: Type Description M Array with the cumulative reduce. Source code in jax_metrics/metrics/reduce.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def update ( self : M , values : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , ** _ , ) -> M : \"\"\" Accumulates statistics for computing the reduction metric. For example, if `values` is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of `result()` is 4. If the `sample_weight` is specified as [1, 1, 0, 0] then value of `result()` would be 2. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Defaults to 1. Returns: Array with the cumulative reduce. \"\"\" # perform update if sample_weight is not None : if sample_weight . ndim > values . ndim : raise Exception ( f \"sample_weight dimention is higher than values, when masking values sample_weight dimention needs to be equal or lower than values dimension, currently values have shape equal to { values . shape } \" ) try : # Broadcast weights if possible. sample_weight = jnp . broadcast_to ( sample_weight , values . shape ) except ValueError : # Reduce values to same ndim as weight array values_ndim , weight_ndim = values . ndim , sample_weight . ndim if self . reduction == Reduction . sum : values = jnp . sum ( values , axis = list ( range ( weight_ndim , values_ndim ))) else : values = jnp . mean ( values , axis = list ( range ( weight_ndim , values_ndim )) ) values = values * sample_weight value_sum = jnp . sum ( values ) total = ( self . total + value_sum ) . astype ( self . total . dtype ) # Exit early if the reduction doesn't have a denominator. if self . reduction == Reduction . sum : num_values = None # Update `count` for reductions that require a denominator. elif self . reduction == Reduction . sum_over_batch_size : num_values = np . prod ( values . shape ) else : if sample_weight is None : num_values = np . prod ( values . shape ) else : num_values = jnp . sum ( sample_weight ) if self . count is not None : assert num_values is not None count = ( self . count + num_values ) . astype ( self . count . dtype ) else : count = None return self . replace ( total = total , count = count )","title":"Reduce"},{"location":"api/metrics/Reduce/#jax_metricsmetricsreduce","text":"Bases: SumMetric Encapsulates metrics that perform a reduce operation on the values. Source code in jax_metrics/metrics/reduce.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 class Reduce ( SumMetric ): \"\"\"Encapsulates metrics that perform a reduce operation on the values.\"\"\" total : jax . Array count : tp . Optional [ jax . Array ] reduction : Reduction = static_field () dtype : jnp . dtype = static_field () def __init__ ( self , reduction : tp . Union [ Reduction , str ], dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" Creates a `Reduce` instance. Arguments: reduction: (Optional) The reduction to apply to the metric values. Defaults to `SUM_OVER_BATCH_SIZE`. dtype: (Optional) data type of the metric result. Defaults to `float32`. \"\"\" self . reduction = ( reduction if isinstance ( reduction , Reduction ) else Reduction [ reduction ] ) self . dtype = dtype or jnp . float32 self . __dict__ . update ( self . _initial_values ()) def _initial_values ( self ) -> tp . Dict [ str , tp . Any ]: # initialize states total = jnp . array ( 0.0 , dtype = self . dtype ) if self . reduction in ( Reduction . sum_over_batch_size , Reduction . weighted_mean , ): count = jnp . array ( 0 , dtype = jnp . uint32 ) else : count = None return dict ( total = total , count = count ) def reset ( self : M ) -> M : \"\"\" Resets all of the metric state variables. Returns: An instance of `Reduce`. \"\"\" return self . replace ( ** self . _initial_values ()) def update ( self : M , values : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , ** _ , ) -> M : \"\"\" Accumulates statistics for computing the reduction metric. For example, if `values` is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of `result()` is 4. If the `sample_weight` is specified as [1, 1, 0, 0] then value of `result()` would be 2. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Defaults to 1. Returns: Array with the cumulative reduce. \"\"\" # perform update if sample_weight is not None : if sample_weight . ndim > values . ndim : raise Exception ( f \"sample_weight dimention is higher than values, when masking values sample_weight dimention needs to be equal or lower than values dimension, currently values have shape equal to { values . shape } \" ) try : # Broadcast weights if possible. sample_weight = jnp . broadcast_to ( sample_weight , values . shape ) except ValueError : # Reduce values to same ndim as weight array values_ndim , weight_ndim = values . ndim , sample_weight . ndim if self . reduction == Reduction . sum : values = jnp . sum ( values , axis = list ( range ( weight_ndim , values_ndim ))) else : values = jnp . mean ( values , axis = list ( range ( weight_ndim , values_ndim )) ) values = values * sample_weight value_sum = jnp . sum ( values ) total = ( self . total + value_sum ) . astype ( self . total . dtype ) # Exit early if the reduction doesn't have a denominator. if self . reduction == Reduction . sum : num_values = None # Update `count` for reductions that require a denominator. elif self . reduction == Reduction . sum_over_batch_size : num_values = np . prod ( values . shape ) else : if sample_weight is None : num_values = np . prod ( values . shape ) else : num_values = jnp . sum ( sample_weight ) if self . count is not None : assert num_values is not None count = ( self . count + num_values ) . astype ( self . count . dtype ) else : count = None return self . replace ( total = total , count = count ) def compute ( self ) -> jax . Array : if self . reduction == Reduction . sum : return self . total else : return self . total / self . count def from_argument ( self , argument : str ) -> RenameArguments : return self . rename_arguments ( values = argument )","title":"jax_metrics.metrics.Reduce"},{"location":"api/metrics/Reduce/#jax_metrics.metrics.reduce.Reduce.__init__","text":"Creates a Reduce instance. Parameters: Name Type Description Default reduction tp . Union [ Reduction , str ] (Optional) The reduction to apply to the metric values. Defaults to SUM_OVER_BATCH_SIZE . required dtype tp . Optional [ jnp . dtype ] (Optional) data type of the metric result. Defaults to float32 . None Source code in jax_metrics/metrics/reduce.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , reduction : tp . Union [ Reduction , str ], dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" Creates a `Reduce` instance. Arguments: reduction: (Optional) The reduction to apply to the metric values. Defaults to `SUM_OVER_BATCH_SIZE`. dtype: (Optional) data type of the metric result. Defaults to `float32`. \"\"\" self . reduction = ( reduction if isinstance ( reduction , Reduction ) else Reduction [ reduction ] ) self . dtype = dtype or jnp . float32 self . __dict__ . update ( self . _initial_values ())","title":"__init__()"},{"location":"api/metrics/Reduce/#jax_metrics.metrics.reduce.Reduce.reset","text":"Resets all of the metric state variables. Returns: Type Description M An instance of Reduce . Source code in jax_metrics/metrics/reduce.py 62 63 64 65 66 67 68 def reset ( self : M ) -> M : \"\"\" Resets all of the metric state variables. Returns: An instance of `Reduce`. \"\"\" return self . replace ( ** self . _initial_values ())","title":"reset()"},{"location":"api/metrics/Reduce/#jax_metrics.metrics.reduce.Reduce.update","text":"Accumulates statistics for computing the reduction metric. For example, if values is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of result() is 4. If the sample_weight is specified as [1, 1, 0, 0] then value of result() would be 2. Parameters: Name Type Description Default values jax . Array Per-example value. required sample_weight tp . Optional [ jax . Array ] Optional weighting of each example. Defaults to 1. None Returns: Type Description M Array with the cumulative reduce. Source code in jax_metrics/metrics/reduce.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def update ( self : M , values : jax . Array , sample_weight : tp . Optional [ jax . Array ] = None , ** _ , ) -> M : \"\"\" Accumulates statistics for computing the reduction metric. For example, if `values` is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of `result()` is 4. If the `sample_weight` is specified as [1, 1, 0, 0] then value of `result()` would be 2. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Defaults to 1. Returns: Array with the cumulative reduce. \"\"\" # perform update if sample_weight is not None : if sample_weight . ndim > values . ndim : raise Exception ( f \"sample_weight dimention is higher than values, when masking values sample_weight dimention needs to be equal or lower than values dimension, currently values have shape equal to { values . shape } \" ) try : # Broadcast weights if possible. sample_weight = jnp . broadcast_to ( sample_weight , values . shape ) except ValueError : # Reduce values to same ndim as weight array values_ndim , weight_ndim = values . ndim , sample_weight . ndim if self . reduction == Reduction . sum : values = jnp . sum ( values , axis = list ( range ( weight_ndim , values_ndim ))) else : values = jnp . mean ( values , axis = list ( range ( weight_ndim , values_ndim )) ) values = values * sample_weight value_sum = jnp . sum ( values ) total = ( self . total + value_sum ) . astype ( self . total . dtype ) # Exit early if the reduction doesn't have a denominator. if self . reduction == Reduction . sum : num_values = None # Update `count` for reductions that require a denominator. elif self . reduction == Reduction . sum_over_batch_size : num_values = np . prod ( values . shape ) else : if sample_weight is None : num_values = np . prod ( values . shape ) else : num_values = jnp . sum ( sample_weight ) if self . count is not None : assert num_values is not None count = ( self . count + num_values ) . astype ( self . count . dtype ) else : count = None return self . replace ( total = total , count = count )","title":"update()"},{"location":"api/metrics/Reduction/","text":"jax_metrics.metrics.Reduction Bases: enum . Enum Source code in jax_metrics/metrics/reduce.py 15 16 17 18 class Reduction ( enum . Enum ): sum = enum . auto () sum_over_batch_size = enum . auto () weighted_mean = enum . auto ()","title":"Reduction"},{"location":"api/metrics/Reduction/#jax_metricsmetricsreduction","text":"Bases: enum . Enum Source code in jax_metrics/metrics/reduce.py 15 16 17 18 class Reduction ( enum . Enum ): sum = enum . auto () sum_over_batch_size = enum . auto () weighted_mean = enum . auto ()","title":"jax_metrics.metrics.Reduction"},{"location":"api/metrics/SumMetric/","text":"jax_metrics.metrics.SumMetric Bases: Metric Source code in jax_metrics/metrics/metric.py 171 172 173 174 175 176 class SumMetric ( Metric ): def merge ( self : M , other : M ) -> M : return jax . tree_map ( lambda x , y : x + y , self , other ) def reduce ( self : M ) -> M : return jax . tree_map ( lambda x : jnp . sum ( x , axis = 0 ), self )","title":"SumMetric"},{"location":"api/metrics/SumMetric/#jax_metricsmetricssummetric","text":"Bases: Metric Source code in jax_metrics/metrics/metric.py 171 172 173 174 175 176 class SumMetric ( Metric ): def merge ( self : M , other : M ) -> M : return jax . tree_map ( lambda x , y : x + y , self , other ) def reduce ( self : M ) -> M : return jax . tree_map ( lambda x : jnp . sum ( x , axis = 0 ), self )","title":"jax_metrics.metrics.SumMetric"},{"location":"api/regularizers/L1/","text":"jax_metrics.regularizers.L1 Bases: L1L2 Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| Source code in jax_metrics/regularizers/l1l2.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 class L1 ( L1L2 ): r \"\"\" Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: $$\\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i|$$ \"\"\" def __init__ ( self , l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , name : tp . Optional [ str ] = None , ): super () . __init__ ( l1 = l , reduction = reduction , weight = weight , name = name )","title":"L1"},{"location":"api/regularizers/L1/#jax_metricsregularizersl1","text":"Bases: L1L2 Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| Source code in jax_metrics/regularizers/l1l2.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 class L1 ( L1L2 ): r \"\"\" Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: $$\\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i|$$ \"\"\" def __init__ ( self , l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , name : tp . Optional [ str ] = None , ): super () . __init__ ( l1 = l , reduction = reduction , weight = weight , name = name )","title":"jax_metrics.regularizers.L1"},{"location":"api/regularizers/L1L2/","text":"jax_metrics.regularizers.L1L2 Bases: Loss A regularizer that applies both L1 and L2 regularization penalties. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| The L2 regularization penalty is computed as \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 Source code in jax_metrics/regularizers/l1l2.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class L1L2 ( Loss ): r \"\"\" A regularizer that applies both L1 and L2 regularization penalties. The L1 regularization penalty is computed as: $$ \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| $$ The L2 regularization penalty is computed as $$\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2$$ \"\"\" def __init__ ( self , l1 = 0.0 , l2 = 0.0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , name : tp . Optional [ str ] = None , ): # pylint: disable=redefined-outer-name super () . __init__ ( reduction = reduction , weight = weight , name = name ) self . l1 = l1 self . l2 = l2 def call ( self , parameters : tp . Any , ** _ ) -> jax . Array : \"\"\" Computes the L1 and L2 regularization penalty simultaneously. Arguments: net_params: A structure with all the parameters of the model. \"\"\" regularization : jax . Array = jnp . array ( 0.0 ) if not self . l1 and not self . l2 : return regularization leaves = jax . tree_leaves ( parameters ) if self . l1 : regularization += self . l1 * sum ( jnp . sum ( jnp . abs ( p )) for p in leaves ) if self . l2 : regularization += self . l2 * sum ( jnp . sum ( jnp . square ( p )) for p in leaves ) return regularization call ( parameters , ** _ ) Computes the L1 and L2 regularization penalty simultaneously. Parameters: Name Type Description Default net_params A structure with all the parameters of the model. required Source code in jax_metrics/regularizers/l1l2.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def call ( self , parameters : tp . Any , ** _ ) -> jax . Array : \"\"\" Computes the L1 and L2 regularization penalty simultaneously. Arguments: net_params: A structure with all the parameters of the model. \"\"\" regularization : jax . Array = jnp . array ( 0.0 ) if not self . l1 and not self . l2 : return regularization leaves = jax . tree_leaves ( parameters ) if self . l1 : regularization += self . l1 * sum ( jnp . sum ( jnp . abs ( p )) for p in leaves ) if self . l2 : regularization += self . l2 * sum ( jnp . sum ( jnp . square ( p )) for p in leaves ) return regularization","title":"L1L2"},{"location":"api/regularizers/L1L2/#jax_metricsregularizersl1l2","text":"Bases: Loss A regularizer that applies both L1 and L2 regularization penalties. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| The L2 regularization penalty is computed as \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 Source code in jax_metrics/regularizers/l1l2.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class L1L2 ( Loss ): r \"\"\" A regularizer that applies both L1 and L2 regularization penalties. The L1 regularization penalty is computed as: $$ \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| $$ The L2 regularization penalty is computed as $$\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2$$ \"\"\" def __init__ ( self , l1 = 0.0 , l2 = 0.0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , name : tp . Optional [ str ] = None , ): # pylint: disable=redefined-outer-name super () . __init__ ( reduction = reduction , weight = weight , name = name ) self . l1 = l1 self . l2 = l2 def call ( self , parameters : tp . Any , ** _ ) -> jax . Array : \"\"\" Computes the L1 and L2 regularization penalty simultaneously. Arguments: net_params: A structure with all the parameters of the model. \"\"\" regularization : jax . Array = jnp . array ( 0.0 ) if not self . l1 and not self . l2 : return regularization leaves = jax . tree_leaves ( parameters ) if self . l1 : regularization += self . l1 * sum ( jnp . sum ( jnp . abs ( p )) for p in leaves ) if self . l2 : regularization += self . l2 * sum ( jnp . sum ( jnp . square ( p )) for p in leaves ) return regularization","title":"jax_metrics.regularizers.L1L2"},{"location":"api/regularizers/L1L2/#jax_metrics.regularizers.l1l2.L1L2.call","text":"Computes the L1 and L2 regularization penalty simultaneously. Parameters: Name Type Description Default net_params A structure with all the parameters of the model. required Source code in jax_metrics/regularizers/l1l2.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def call ( self , parameters : tp . Any , ** _ ) -> jax . Array : \"\"\" Computes the L1 and L2 regularization penalty simultaneously. Arguments: net_params: A structure with all the parameters of the model. \"\"\" regularization : jax . Array = jnp . array ( 0.0 ) if not self . l1 and not self . l2 : return regularization leaves = jax . tree_leaves ( parameters ) if self . l1 : regularization += self . l1 * sum ( jnp . sum ( jnp . abs ( p )) for p in leaves ) if self . l2 : regularization += self . l2 * sum ( jnp . sum ( jnp . square ( p )) for p in leaves ) return regularization","title":"call()"},{"location":"api/regularizers/L2/","text":"jax_metrics.regularizers.L2 Bases: L1L2 Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 <span class=\"arithmatex\"><span class=\"MathJax_Preview\">\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2</span><script type=\"math/tex\">\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 ``` Source code in jax_metrics/regularizers/l1l2.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 class L2 ( L1L2 ): r \"\"\" Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: $$\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2$$ ``` \"\"\" def __init__ ( self , l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , name : tp . Optional [ str ] = None , ): super () . __init__ ( l2 = l , reduction = reduction , weight = weight , name = name )","title":"L2"},{"location":"api/regularizers/L2/#jax_metricsregularizersl2","text":"Bases: L1L2 Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 <span class=\"arithmatex\"><span class=\"MathJax_Preview\">\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2</span><script type=\"math/tex\">\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 ``` Source code in jax_metrics/regularizers/l1l2.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 class L2 ( L1L2 ): r \"\"\" Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: $$\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2$$ ``` \"\"\" def __init__ ( self , l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , name : tp . Optional [ str ] = None , ): super () . __init__ ( l2 = l , reduction = reduction , weight = weight , name = name )","title":"jax_metrics.regularizers.L2"}]}